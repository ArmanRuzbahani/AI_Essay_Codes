{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0164b08",
   "metadata": {},
   "source": [
    "# 🧠 آشنایی با پردازش زبان طبیعی (Natural Language Processing - NLP)\n",
    "\n",
    "**پردازش زبان طبیعی (NLP)** یکی از شاخه‌های مهم **هوش مصنوعی 🤖** است که به تعامل و ارتباط کامپیوتر با زبان انسان‌ها 🗣️ می‌پردازد. در واقع NLP سعی دارد به کامپیوترها کمک کند تا زبان طبیعی انسان‌ها را بفهمند، تفسیر کنند و به آن پاسخ دهند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 اهداف اصلی NLP\n",
    "- 🧾 درک معنی و مفهوم متن\n",
    "- 📌 استخراج اطلاعات مهم از متن\n",
    "- 🌐 ترجمه زبان‌ها به یکدیگر\n",
    "- 😊 تشخیص و طبقه‌بندی احساسات\n",
    "- ✂️ خلاصه‌سازی متون طولانی\n",
    "- 📝 تولید متن و گفتار به زبان طبیعی\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 کاربردهای رایج NLP\n",
    "- 🌍 **مترجم‌های آنلاین:** مانند Google Translate\n",
    "- 🎙️ **دستیارهای صوتی هوشمند:** مانند Siri، Alexa و Google Assistant\n",
    "- 💬 **سیستم‌های تحلیل احساسات:** تحلیل نظرات کاربران در شبکه‌های اجتماعی\n",
    "- 🤖 **ربات‌های چت (Chatbots):** پاسخ‌گویی خودکار به کاربران\n",
    "- 🎯 **سیستم‌های پیشنهاددهنده:** توصیه فیلم، کتاب، موسیقی و...\n",
    "- ⚖️ **تحلیل متن‌های حقوقی و پزشکی:** استخراج اطلاعات کلیدی از اسناد تخصصی\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 تکنیک‌ها و ابزارهای معروف در NLP\n",
    "\n",
    "### 🧹 پیش‌پردازش متن (Text Preprocessing)\n",
    "- حذف نویز 🧽\n",
    "- ریشه‌یابی کلمات (Stemming) 🌱\n",
    "- ریشه‌یابی معنایی (Lemmatization) 📚\n",
    "- حذف کلمات توقف (Stop Words) 🛑\n",
    "\n",
    "### 🧠 بردارهای جاسازی کلمات (Word Embeddings)\n",
    "- Word2Vec 🔡  \n",
    "- GloVe 🧤  \n",
    "- FastText ⚡\n",
    "\n",
    "### 🤖 مدل‌های یادگیری ماشین و یادگیری عمیق\n",
    "- 📊 مدل‌های کلاسیک: SVM، Naive Bayes، Logistic Regression  \n",
    "- 🔁 مدل‌های عمیق: RNN، LSTM، CNN  \n",
    "- 🚀 مدل‌های ترنسفورمر: BERT، GPT و سایر مدل‌های HuggingFace\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ چالش‌های NLP\n",
    "- 🌀 ابهام و پیچیدگی زبان طبیعی  \n",
    "- 🔤 وجود معانی مختلف برای یک کلمه (Ambiguity)  \n",
    "- 📏 تفاوت‌های گرامری و نحوی در زبان‌ها  \n",
    "- 🤔 دشواری درک کنایه، طنز و احساسات پنهان\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 کتابخانه‌های معروف NLP در پایتون\n",
    "- 📦 **NLTK** – ابزارهای پایه‌ای برای پیش‌پردازش متن  \n",
    "- ⚡ **spaCy** – سریع و مناسب برای پروژه‌های واقعی  \n",
    "- 🤗 **Transformers (Hugging Face)** – مدل‌های پیشرفته BERT و GPT  \n",
    "- 📚 **Gensim** – برای بردارسازی و مدل‌سازی موضوعات (Topic Modeling)\n",
    "\n",
    "---\n",
    "\n",
    "> 📝 یادگیری NLP پلی است میان علوم انسانی و علوم کامپیوتر! ترکیبی از زبان، آمار، و هوش مصنوعی برای درک بهتر جهان انسانی 🌍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519f7a4",
   "metadata": {},
   "source": [
    "## 🔁 مدل مارکوف پنهان (Hidden Markov Model - HMM) در تحلیل احساسات\n",
    "\n",
    "**مدل مارکوف پنهان (HMM)** یکی از مدل‌های آماری پرکاربرد در پردازش زبان طبیعی (NLP) است که می‌تواند به‌صورت مؤثری برای **تحلیل احساسات (Sentiment Analysis)** در متون به‌کار رود.\n",
    "\n",
    "### 🧠 ایده‌ی اصلی HMM\n",
    "HMM مدلی است که فرض می‌کند:\n",
    "- در پشت داده‌های قابل مشاهده (مثلاً کلمات یک جمله 📝)، حالاتی پنهان وجود دارند (مثلاً احساس نویسنده 😊😐😡)\n",
    "- این حالت‌های پنهان به‌صورت زنجیره‌ای و با وابستگی زمانی ظاهر می‌شوند 🔗\n",
    "- ما فقط خروجی‌ها را می‌بینیم، نه حالت‌ها را — و مدل باید آن‌ها را **تخمین بزند**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 اجزای مدل HMM برای تحلیل احساسات\n",
    "1. **حالت‌های پنهان (Hidden States):**  \n",
    "   احساساتی مانند مثبت 😊، منفی 😠، خنثی 😐\n",
    "\n",
    "2. **داده‌های قابل مشاهده (Observations):**  \n",
    "   کلمات یا جملات متن مثل \"عالی\"، \"بد\"، \"بی‌تفاوت\"\n",
    "\n",
    "3. **ماتریس انتقال (Transition Probabilities):**  \n",
    "   احتمال تغییر احساس در طول جمله‌ها یا متن – مثلاً از \"مثبت\" به \"منفی\"\n",
    "\n",
    "4. **ماتریس انتشار (Emission Probabilities):**  \n",
    "   احتمال ظاهر شدن یک کلمه خاص در یک احساس خاص – مثلاً احتمال اینکه \"عالی\" در حالت مثبت ظاهر شود\n",
    "\n",
    "5. **احتمال شروع (Initial Probabilities):**  \n",
    "   احتمال اینکه متن با چه احساسی شروع شود\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 کاربرد HMM در Sentiment Analysis\n",
    "\n",
    "مدل HMM می‌تواند برای تحلیل احساسات در **متون دنباله‌دار یا داستانی** بسیار مفید باشد. برخلاف روش‌های ساده که هر جمله را جدا بررسی می‌کنند، HMM وابستگی بین جملات و تغییر احساسات در طول متن را نیز در نظر می‌گیرد.\n",
    "\n",
    "#### ✅ مثال:\n",
    "فرض کنید یک نقد فیلم به‌صورت زیر است:\n",
    "> \"فیلم شروع کُندی داشت، ولی با گذشت زمان جذاب‌تر شد. پایان‌بندی واقعاً احساسی بود.\"\n",
    "\n",
    "HMM می‌تواند الگوی تغییر احساسات را در این نقد شناسایی کند:\n",
    "- ابتدا: منفی 😒  \n",
    "- میانه: خنثی 😐 یا رو به مثبت 😊  \n",
    "- انتها: مثبت 😍\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 مزایای استفاده از HMM در تحلیل احساسات:\n",
    "- مدل‌سازی دنباله‌ای و زمانی احساسات\n",
    "- امکان پیش‌بینی احساس جمله بعدی\n",
    "- درک بهتر از تغییرات احساسی در متن‌های طولانی مثل نقد، داستان یا گفتگو\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 جمع‌بندی\n",
    "مدل مارکوف پنهان با ساختار زنجیره‌ای خود، ابزاری قدرتمند برای **تحلیل احساسات دنباله‌دار** در متون طبیعی است. این مدل به‌ویژه زمانی کاربرد دارد که تغییرات احساسی در طول متن مهم باشند و تحلیل جمله‌ای کافی نباشد.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff95f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# لود کردن مدل از Hugging Face\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# جمله‌ی ورودی برای تحلیل احساس\n",
    "text = \"این فیلم واقعاً فوق‌العاده بود! بازی بازیگران عالی بود و داستان من را هیجان‌زده کرد.\"\n",
    "\n",
    "# تبدیل جمله به توکن‌های ورودی مدل\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# پیش‌بینی احساس\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# اعمال Softmax برای تبدیل به احتمال\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# نمایش نتایج\n",
    "labels = [\"منفی 😠\", \"خنثی 😐\", \"مثبت 😊\"]\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "\n",
    "print(\"جمله:\", text)\n",
    "print(\"تحلیل احساس:\", predicted_label)\n",
    "print(\"احتمال‌ها:\", dict(zip(labels, np.round(probs.numpy()[0], 3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34608cac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
