{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0164b08",
   "metadata": {},
   "source": [
    "# ğŸ§  Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (Natural Language Processing - NLP)\n",
    "\n",
    "**Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP)** ÛŒÚ©ÛŒ Ø§Ø² Ø´Ø§Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… **Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ğŸ¤–** Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ ØªØ¹Ø§Ù…Ù„ Ùˆ Ø§Ø±ØªØ¨Ø§Ø· Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø¨Ø§ Ø²Ø¨Ø§Ù† Ø§Ù†Ø³Ø§Ù†â€ŒÙ‡Ø§ ğŸ—£ï¸ Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯. Ø¯Ø± ÙˆØ§Ù‚Ø¹ NLP Ø³Ø¹ÛŒ Ø¯Ø§Ø±Ø¯ Ø¨Ù‡ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±Ù‡Ø§ Ú©Ù…Ú© Ú©Ù†Ø¯ ØªØ§ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ù†Ø³Ø§Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÙÙ‡Ù…Ù†Ø¯ØŒ ØªÙØ³ÛŒØ± Ú©Ù†Ù†Ø¯ Ùˆ Ø¨Ù‡ Ø¢Ù† Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ø§Ù‡Ø¯Ø§Ù Ø§ØµÙ„ÛŒ NLP\n",
    "- ğŸ§¾ Ø¯Ø±Ú© Ù…Ø¹Ù†ÛŒ Ùˆ Ù…ÙÙ‡ÙˆÙ… Ù…ØªÙ†\n",
    "- ğŸ“Œ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… Ø§Ø² Ù…ØªÙ†\n",
    "- ğŸŒ ØªØ±Ø¬Ù…Ù‡ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø±\n",
    "- ğŸ˜Š ØªØ´Ø®ÛŒØµ Ùˆ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "- âœ‚ï¸ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙˆÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ\n",
    "- ğŸ“ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ùˆ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¼ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ NLP\n",
    "- ğŸŒ **Ù…ØªØ±Ø¬Ù…â€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†:** Ù…Ø§Ù†Ù†Ø¯ Google Translate\n",
    "- ğŸ™ï¸ **Ø¯Ø³ØªÛŒØ§Ø±Ù‡Ø§ÛŒ ØµÙˆØªÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯:** Ù…Ø§Ù†Ù†Ø¯ SiriØŒ Alexa Ùˆ Google Assistant\n",
    "- ğŸ’¬ **Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª:** ØªØ­Ù„ÛŒÙ„ Ù†Ø¸Ø±Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¯Ø± Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ\n",
    "- ğŸ¤– **Ø±Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ú†Øª (Chatbots):** Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "- ğŸ¯ **Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø¯Ù‡Ù†Ø¯Ù‡:** ØªÙˆØµÛŒÙ‡ ÙÛŒÙ„Ù…ØŒ Ú©ØªØ§Ø¨ØŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ùˆ...\n",
    "- âš–ï¸ **ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ Ùˆ Ù¾Ø²Ø´Ú©ÛŒ:** Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ ØªØ®ØµØµÛŒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§° ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ Ø¯Ø± NLP\n",
    "\n",
    "### ğŸ§¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† (Text Preprocessing)\n",
    "- Ø­Ø°Ù Ù†ÙˆÛŒØ² ğŸ§½\n",
    "- Ø±ÛŒØ´Ù‡â€ŒÛŒØ§Ø¨ÛŒ Ú©Ù„Ù…Ø§Øª (Stemming) ğŸŒ±\n",
    "- Ø±ÛŒØ´Ù‡â€ŒÛŒØ§Ø¨ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Lemmatization) ğŸ“š\n",
    "- Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù (Stop Words) ğŸ›‘\n",
    "\n",
    "### ğŸ§  Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ø§Ø³Ø§Ø²ÛŒ Ú©Ù„Ù…Ø§Øª (Word Embeddings)\n",
    "- Word2Vec ğŸ”¡  \n",
    "- GloVe ğŸ§¤  \n",
    "- FastText âš¡\n",
    "\n",
    "### ğŸ¤– Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚\n",
    "- ğŸ“Š Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ©: SVMØŒ Naive BayesØŒ Logistic Regression  \n",
    "- ğŸ” Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚: RNNØŒ LSTMØŒ CNN  \n",
    "- ğŸš€ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±: BERTØŒ GPT Ùˆ Ø³Ø§ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ HuggingFace\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ NLP\n",
    "- ğŸŒ€ Ø§Ø¨Ù‡Ø§Ù… Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ  \n",
    "- ğŸ”¤ ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ø§Ù†ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ù„Ù…Ù‡ (Ambiguity)  \n",
    "- ğŸ“ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§Ù…Ø±ÛŒ Ùˆ Ù†Ø­ÙˆÛŒ Ø¯Ø± Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§  \n",
    "- ğŸ¤” Ø¯Ø´ÙˆØ§Ø±ÛŒ Ø¯Ø±Ú© Ú©Ù†Ø§ÛŒÙ‡ØŒ Ø·Ù†Ø² Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾Ù†Ù‡Ø§Ù†\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ NLP Ø¯Ø± Ù¾Ø§ÛŒØªÙˆÙ†\n",
    "- ğŸ“¦ **NLTK** â€“ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†  \n",
    "- âš¡ **spaCy** â€“ Ø³Ø±ÛŒØ¹ Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ  \n",
    "- ğŸ¤— **Transformers (Hugging Face)** â€“ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ BERT Ùˆ GPT  \n",
    "- ğŸ“š **Gensim** â€“ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª (Topic Modeling)\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ“ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ NLP Ù¾Ù„ÛŒ Ø§Ø³Øª Ù…ÛŒØ§Ù† Ø¹Ù„ÙˆÙ… Ø§Ù†Ø³Ø§Ù†ÛŒ Ùˆ Ø¹Ù„ÙˆÙ… Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±! ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø²Ø¨Ø§Ù†ØŒ Ø¢Ù…Ø§Ø±ØŒ Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ø¨Ù‡ØªØ± Ø¬Ù‡Ø§Ù† Ø§Ù†Ø³Ø§Ù†ÛŒ ğŸŒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519f7a4",
   "metadata": {},
   "source": [
    "## ğŸ” Ù…Ø¯Ù„ Ù…Ø§Ø±Ú©ÙˆÙ Ù¾Ù†Ù‡Ø§Ù† (Hidden Markov Model - HMM) Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "\n",
    "**Ù…Ø¯Ù„ Ù…Ø§Ø±Ú©ÙˆÙ Ù¾Ù†Ù‡Ø§Ù† (HMM)** ÛŒÚ©ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP) Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù…Ø¤Ø«Ø±ÛŒ Ø¨Ø±Ø§ÛŒ **ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Sentiment Analysis)** Ø¯Ø± Ù…ØªÙˆÙ† Ø¨Ù‡â€ŒÚ©Ø§Ø± Ø±ÙˆØ¯.\n",
    "\n",
    "### ğŸ§  Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ HMM\n",
    "HMM Ù…Ø¯Ù„ÛŒ Ø§Ø³Øª Ú©Ù‡ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:\n",
    "- Ø¯Ø± Ù¾Ø´Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ú©Ù„Ù…Ø§Øª ÛŒÚ© Ø¬Ù…Ù„Ù‡ ğŸ“)ØŒ Ø­Ø§Ù„Ø§ØªÛŒ Ù¾Ù†Ù‡Ø§Ù† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø§Ø­Ø³Ø§Ø³ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ ğŸ˜ŠğŸ˜ğŸ˜¡)\n",
    "- Ø§ÛŒÙ† Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø¨Ø§ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¸Ø§Ù‡Ø± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ğŸ”—\n",
    "- Ù…Ø§ ÙÙ‚Ø· Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒÙ…ØŒ Ù†Ù‡ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ Ø±Ø§ â€” Ùˆ Ù…Ø¯Ù„ Ø¨Ø§ÛŒØ¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ **ØªØ®Ù…ÛŒÙ† Ø¨Ø²Ù†Ø¯**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Ø§Ø¬Ø²Ø§ÛŒ Ù…Ø¯Ù„ HMM Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "1. **Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† (Hidden States):**  \n",
    "   Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ Ù…Ø«Ø¨Øª ğŸ˜ŠØŒ Ù…Ù†ÙÛŒ ğŸ˜ ØŒ Ø®Ù†Ø«ÛŒ ğŸ˜\n",
    "\n",
    "2. **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ (Observations):**  \n",
    "   Ú©Ù„Ù…Ø§Øª ÛŒØ§ Ø¬Ù…Ù„Ø§Øª Ù…ØªÙ† Ù…Ø«Ù„ \"Ø¹Ø§Ù„ÛŒ\"ØŒ \"Ø¨Ø¯\"ØŒ \"Ø¨ÛŒâ€ŒØªÙØ§ÙˆØª\"\n",
    "\n",
    "3. **Ù…Ø§ØªØ±ÛŒØ³ Ø§Ù†ØªÙ‚Ø§Ù„ (Transition Probabilities):**  \n",
    "   Ø§Ø­ØªÙ…Ø§Ù„ ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³ Ø¯Ø± Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ ÛŒØ§ Ù…ØªÙ† â€“ Ù…Ø«Ù„Ø§Ù‹ Ø§Ø² \"Ù…Ø«Ø¨Øª\" Ø¨Ù‡ \"Ù…Ù†ÙÛŒ\"\n",
    "\n",
    "4. **Ù…Ø§ØªØ±ÛŒØ³ Ø§Ù†ØªØ´Ø§Ø± (Emission Probabilities):**  \n",
    "   Ø§Ø­ØªÙ…Ø§Ù„ Ø¸Ø§Ù‡Ø± Ø´Ø¯Ù† ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø®Ø§Øµ Ø¯Ø± ÛŒÚ© Ø§Ø­Ø³Ø§Ø³ Ø®Ø§Øµ â€“ Ù…Ø«Ù„Ø§Ù‹ Ø§Ø­ØªÙ…Ø§Ù„ Ø§ÛŒÙ†Ú©Ù‡ \"Ø¹Ø§Ù„ÛŒ\" Ø¯Ø± Ø­Ø§Ù„Øª Ù…Ø«Ø¨Øª Ø¸Ø§Ù‡Ø± Ø´ÙˆØ¯\n",
    "\n",
    "5. **Ø§Ø­ØªÙ…Ø§Ù„ Ø´Ø±ÙˆØ¹ (Initial Probabilities):**  \n",
    "   Ø§Ø­ØªÙ…Ø§Ù„ Ø§ÛŒÙ†Ú©Ù‡ Ù…ØªÙ† Ø¨Ø§ Ú†Ù‡ Ø§Ø­Ø³Ø§Ø³ÛŒ Ø´Ø±ÙˆØ¹ Ø´ÙˆØ¯\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Ú©Ø§Ø±Ø¨Ø±Ø¯ HMM Ø¯Ø± Sentiment Analysis\n",
    "\n",
    "Ù…Ø¯Ù„ HMM Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± **Ù…ØªÙˆÙ† Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ¯Ø§Ø± ÛŒØ§ Ø¯Ø§Ø³ØªØ§Ù†ÛŒ** Ø¨Ø³ÛŒØ§Ø± Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ø¯. Ø¨Ø±Ø®Ù„Ø§Ù Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ú©Ù‡ Ù‡Ø± Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¬Ø¯Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ HMM ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨ÛŒÙ† Ø¬Ù…Ù„Ø§Øª Ùˆ ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± Ø·ÙˆÙ„ Ù…ØªÙ† Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "\n",
    "#### âœ… Ù…Ø«Ø§Ù„:\n",
    "ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ ÛŒÚ© Ù†Ù‚Ø¯ ÙÛŒÙ„Ù… Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "> \"ÙÛŒÙ„Ù… Ø´Ø±ÙˆØ¹ Ú©ÙÙ†Ø¯ÛŒ Ø¯Ø§Ø´ØªØŒ ÙˆÙ„ÛŒ Ø¨Ø§ Ú¯Ø°Ø´Øª Ø²Ù…Ø§Ù† Ø¬Ø°Ø§Ø¨â€ŒØªØ± Ø´Ø¯. Ù¾Ø§ÛŒØ§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø§Ø­Ø³Ø§Ø³ÛŒ Ø¨ÙˆØ¯.\"\n",
    "\n",
    "HMM Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ù„Ú¯ÙˆÛŒ ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø±Ø§ Ø¯Ø± Ø§ÛŒÙ† Ù†Ù‚Ø¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù†Ø¯:\n",
    "- Ø§Ø¨ØªØ¯Ø§: Ù…Ù†ÙÛŒ ğŸ˜’  \n",
    "- Ù…ÛŒØ§Ù†Ù‡: Ø®Ù†Ø«ÛŒ ğŸ˜ ÛŒØ§ Ø±Ùˆ Ø¨Ù‡ Ù…Ø«Ø¨Øª ğŸ˜Š  \n",
    "- Ø§Ù†ØªÙ‡Ø§: Ù…Ø«Ø¨Øª ğŸ˜\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ Ù…Ø²Ø§ÛŒØ§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² HMM Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª:\n",
    "- Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ§ÛŒ Ùˆ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "- Ø§Ù…Ú©Ø§Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³ Ø¬Ù…Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒ\n",
    "- Ø¯Ø±Ú© Ø¨Ù‡ØªØ± Ø§Ø² ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ù…Ø«Ù„ Ù†Ù‚Ø¯ØŒ Ø¯Ø§Ø³ØªØ§Ù† ÛŒØ§ Ú¯ÙØªÚ¯Ùˆ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "Ù…Ø¯Ù„ Ù…Ø§Ø±Ú©ÙˆÙ Ù¾Ù†Ù‡Ø§Ù† Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø®ÙˆØ¯ØŒ Ø§Ø¨Ø²Ø§Ø±ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ **ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ¯Ø§Ø±** Ø¯Ø± Ù…ØªÙˆÙ† Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ù…ØªÙ† Ù…Ù‡Ù… Ø¨Ø§Ø´Ù†Ø¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ú©Ø§ÙÛŒ Ù†Ø¨Ø§Ø´Ø¯.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff95f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² Hugging Face\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³\n",
    "text = \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… ÙˆØ§Ù‚Ø¹Ø§Ù‹ ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯! Ø¨Ø§Ø²ÛŒ Ø¨Ø§Ø²ÛŒÚ¯Ø±Ø§Ù† Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ùˆ Ø¯Ø§Ø³ØªØ§Ù† Ù…Ù† Ø±Ø§ Ù‡ÛŒØ¬Ø§Ù†â€ŒØ²Ø¯Ù‡ Ú©Ø±Ø¯.\"\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Ø§Ø¹Ù…Ø§Ù„ Softmax Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬\n",
    "labels = [\"Ù…Ù†ÙÛŒ ğŸ˜ \", \"Ø®Ù†Ø«ÛŒ ğŸ˜\", \"Ù…Ø«Ø¨Øª ğŸ˜Š\"]\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "\n",
    "print(\"Ø¬Ù…Ù„Ù‡:\", text)\n",
    "print(\"ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³:\", predicted_label)\n",
    "print(\"Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§:\", dict(zip(labels, np.round(probs.numpy()[0], 3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34608cac",
   "metadata": {},
   "source": [
    "## ğŸ­ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Emotion Classification)\n",
    "\n",
    "**Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Emotion Classification)** ÛŒÚ©ÛŒ Ø§Ø² Ø´Ø§Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¯Ø± Ø­ÙˆØ²Ù‡â€ŒÛŒ **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP)** Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø§Ø­Ø³Ø§Ø³ (Ù…Ø«Ø¨Øª ÛŒØ§ Ù…Ù†ÙÛŒ)ØŒ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ **Ù†ÙˆØ¹ Ø¯Ù‚ÛŒÙ‚ Ø§Ø­Ø³Ø§Ø³ Ø§Ù†Ø³Ø§Ù†ÛŒ** Ø±Ø§ Ø§Ø² ÛŒÚ© Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ˜ƒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø±Ø§ÛŒØ¬ Ø¯Ø± Ø§ÛŒÙ† Ø­ÙˆØ²Ù‡:\n",
    "Ø¯Ø± Ø§Ú©Ø«Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Emotion ClassificationØŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø²ÛŒØ± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:\n",
    "\n",
    "- ğŸ˜Š **Ø´Ø§Ø¯ÛŒ (Joy)**\n",
    "- ğŸ˜¡ **Ø®Ø´Ù… (Anger)**\n",
    "- ğŸ˜¢ **ØºÙ… (Sadness)**\n",
    "- ğŸ˜± **ØªØ±Ø³ (Fear)**\n",
    "- ğŸ˜² **Ø´Ú¯ÙØªÛŒ (Surprise)**\n",
    "- ğŸ¤¢ **Ù†ÙØ±Øª (Disgust)**\n",
    "- ğŸ˜ **Ø®Ù†Ø«ÛŒ (Neutral)**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  ØªÙØ§ÙˆØª Ø¨Ø§ Sentiment Analysis:\n",
    "| ÙˆÛŒÚ˜Ú¯ÛŒ                | Sentiment Analysis       | Emotion Classification         |\n",
    "|----------------------|--------------------------|--------------------------------|\n",
    "| ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§       | Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Û³ (Ù…Ø«Ø¨ØªØŒ Ù…Ù†ÙÛŒØŒ Ø®Ù†Ø«ÛŒ) | Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Û´ ØªØ§ Û· Ø§Ø­Ø³Ø§Ø³ Ø§Ù†Ø³Ø§Ù†ÛŒ    |\n",
    "| Ø³Ø·Ø­ ØªØ­Ù„ÛŒÙ„            | Ú©Ù„ÛŒ Ùˆ Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø±             | Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø§Ù†Ø³Ø§Ù†ÛŒ                  |\n",
    "| Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§             | Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒØŒ Ø±Ø¶Ø§ÛŒØª Ù…Ø´ØªØ±ÛŒ   | Ø±ÙˆØ§Ù†â€ŒØ´Ù†Ø§Ø³ÛŒØŒ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆØŒ Ø±Ø¨Ø§Øªâ€ŒÙ‡Ø§   |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¼ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Emotion Classification\n",
    "- **ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ù†â€ŒØ´Ù†Ø§Ø®ØªÛŒ Ù…ØªÙˆÙ† Ú©Ø§Ø±Ø¨Ø±Ø§Ù†**\n",
    "- **Ú©Ù†ØªØ±Ù„ Ø±Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ú†Øª Ø¨Ø§ ÙˆØ§Ú©Ù†Ø´ Ø§Ø­Ø³Ø§Ø³ÛŒ**\n",
    "- **ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø³ØªØ§Ù† ÛŒØ§ ÙÛŒÙ„Ù… Ø§Ø² Ù†Ø¸Ø± ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³Ø§Øª**\n",
    "- **Ù¾Ø§ÛŒØ´ Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù† Ø¯Ø± Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ**\n",
    "- **Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø³ØªÛŒØ§Ø±Ù‡Ø§ÛŒ Ù‡Ù…Ø¯Ù„ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ**\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§\n",
    "- Ú†Ù†Ø¯Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÙˆØ¯Ù† ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§\n",
    "- ÙˆØ¬ÙˆØ¯ ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ø§Ø­Ø³Ø§Ø³ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡\n",
    "- Ú©Ù…Ø¨ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡ Ø¨Ø§ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ù†Ø³Ø§Ù†ÛŒ\n",
    "- Ø³Ø®ØªÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù…Ø§Ù†Ù†Ø¯ \"Ù†ÙˆØ³ØªØ§Ù„Ú˜ÛŒ\" ÛŒØ§ \"Ø¹Ø´Ù‚ Ù¾Ù†Ù‡Ø§Ù†\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ù†Ø³Ø§Ù†ÛŒ ÙØ§Ø±Ø³ÛŒ\n",
    "model_name = \"HooshvareLab/bert-fa-base-uncased-emotion-digikala\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³\n",
    "text = \"ÙˆØ§Ù‚Ø¹Ø§ Ø§Ø² Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ù…ØªÙ†ÙØ±Ù…! Ø§ØµÙ„Ø§Ù‹ Ù‡Ù…ÙˆÙ†ÛŒ Ù†Ø¨ÙˆØ¯ Ú©Ù‡ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø´ØªÙ….\"\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ logits Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "labels = [\"Ø®Ø´Ù… ğŸ˜ \", \"ØºÙ… ğŸ˜¢\", \"Ø´Ø§Ø¯ÛŒ ğŸ˜Š\", \"ØªØ±Ø³ ğŸ˜±\", \"Ø´Ú¯ÙØªÛŒ ğŸ˜²\", \"Ù†ÙØ±Øª ğŸ¤¢\"]\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "print(\"Ø¬Ù…Ù„Ù‡:\", text)\n",
    "print(\"Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨:\", predicted_label)\n",
    "print(\"Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§:\", dict(zip(labels, np.round(probs.numpy()[0], 3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c0e98",
   "metadata": {},
   "source": [
    "# Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ Ø¨Ø§Ù„Ø§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd9cb5",
   "metadata": {},
   "source": [
    "Ø¬Ù…Ù„Ù‡: ÙˆØ§Ù‚Ø¹Ø§ Ø§Ø² Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ù…ØªÙ†ÙØ±Ù…! Ø§ØµÙ„Ø§Ù‹ Ù‡Ù…ÙˆÙ†ÛŒ Ù†Ø¨ÙˆØ¯ Ú©Ù‡ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø´ØªÙ….\n",
    "Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨: Ù†ÙØ±Øª ğŸ¤¢\n",
    "Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§: {'Ø®Ø´Ù… ğŸ˜ ': 0.108, 'ØºÙ… ğŸ˜¢': 0.124, 'Ø´Ø§Ø¯ÛŒ ğŸ˜Š': 0.012, 'ØªØ±Ø³ ğŸ˜±': 0.032, 'Ø´Ú¯ÙØªÛŒ ğŸ˜²': 0.027, 'Ù†ÙØ±Øª ğŸ¤¢': 0.697}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b8ecc",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø¯Ø§Ø± (Named Entity Recognition - NER)\n",
    "\n",
    "**Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø¯Ø§Ø±** ÛŒØ§ **NER** ÛŒÚ©ÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP)** Ø§Ø³Øª Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ** Ø¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” ØªØ¹Ø±ÛŒÙ Ø³Ø§Ø¯Ù‡:\n",
    "NER ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø² Ø¯Ø±ÙˆÙ† ÛŒÚ© Ù…ØªÙ†ØŒ Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¨Ù‡ Ú†ÛŒØ²Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø¯Ù†ÛŒØ§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø´Ø§Ø±Ù‡ Ø¯Ø§Ø±Ù†Ø¯ Ø±Ø§ **ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯** Ùˆ **Ø¨Ø±Ú†Ø³Ø¨ Ù…Ù†Ø§Ø³Ø¨** Ø¨Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø®ØªØµØ§Øµ Ø¯Ù‡Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¦ Ø§Ù†ÙˆØ§Ø¹ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:\n",
    "- ğŸ‘¤ **Ø´Ø®Øµ (Person)**: Ù…Ø«Ù„ \"Ø¹Ù„ÛŒ\", \"Elon Musk\"\n",
    "- ğŸ™ï¸ **Ù…Ú©Ø§Ù† (Location)**: Ù…Ø«Ù„ \"ØªÙ‡Ø±Ø§Ù†\", \"New York\"\n",
    "- ğŸ¢ **Ø³Ø§Ø²Ù…Ø§Ù† (Organization)**: Ù…Ø«Ù„ \"Ú¯ÙˆÚ¯Ù„\", \"Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ù„Ù„\"\n",
    "- ğŸ“… **ØªØ§Ø±ÛŒØ® (Date)**: Ù…Ø«Ù„ \"Ûµ Ù…Ø±Ø¯Ø§Ø¯\", \"August 5\"\n",
    "- ğŸ’µ **Ù¾ÙˆÙ„ (Money)**: Ù…Ø«Ù„ \"ÛµÛ°Û° Ù‡Ø²Ø§Ø± ØªÙˆÙ…Ø§Ù†\", \"$99\"\n",
    "- ğŸ”¢ **Ø¹Ø¯Ø¯ / Ø¯Ø±ØµØ¯ (Number / Percent)**: Ù…Ø«Ù„ \"Û²Û³ Ø¯Ø±ØµØ¯\"\n",
    "- ğŸ“¦ **Ù…Ø­ØµÙˆÙ„ (Product)**: Ù…Ø«Ù„ \"iPhone\", \"Ù¾Ú˜Ùˆ Û²Û°Û¶\"\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… NER:\n",
    "- **Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®ÙˆØ¯Ú©Ø§Ø±** Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ØŒ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ØŒ Ø®Ø¨Ø±Ù‡Ø§\n",
    "- **ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ùˆ Ø³ÛŒØ§Ø³ÛŒ** Ø¨Ø±Ø§ÛŒ Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø§ÙØ±Ø§Ø¯ ÛŒØ§ Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§\n",
    "- **Ù¾Ø§ÛŒØ´ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ** Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø¯Ù‡Ø§\n",
    "- **Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ùˆ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªÚ¯ÙˆÙ…Ø­ÙˆØ±**\n",
    "- **Ø¯Ø§Ø¯Ù‡â€ŒÚ©Ø§ÙˆÛŒ Ù¾Ø²Ø´Ú©ÛŒ ÛŒØ§ Ø­Ù‚ÙˆÙ‚ÛŒ**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Ø§Ø¬Ø²Ø§ÛŒ Ù…Ø¯Ù„ NER:\n",
    "- **Tokenization**: Ø´Ú©Ø³ØªÙ† Ù…ØªÙ† Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª ÛŒØ§ Ø¹Ø¨Ø§Ø±Øªâ€ŒÙ‡Ø§\n",
    "- **Embedding**: ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ\n",
    "- **Labeling**: Ø§Ø®ØªØµØ§Øµ Ø¨Ø±Ú†Ø³Ø¨ (Ù…Ø§Ù†Ù†Ø¯ B-PERØŒ I-LOCØŒ O) Ø¨Ù‡ Ù‡Ø± ØªÙˆÚ©Ù†\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡:\n",
    "\n",
    "ÙˆØ±ÙˆØ¯ÛŒ:\n",
    "> \"Ø§Ù„ÙˆÙ† Ù…Ø§Ø³Ú© Ø¯Ø± Ø³Ø§Ù„ Û²Û°Û°Û² Ø´Ø±Ú©Øª Ø§Ø³Ù¾ÛŒØ³â€ŒØ§ÛŒÚ©Ø³ Ø±Ø§ Ø¯Ø± Ø¢Ù…Ø±ÛŒÚ©Ø§ ØªØ£Ø³ÛŒØ³ Ú©Ø±Ø¯.\"\n",
    "\n",
    "Ø®Ø±ÙˆØ¬ÛŒ NER:\n",
    "| ÙˆØ§Ú˜Ù‡          | Ù†ÙˆØ¹ Ù…ÙˆØ¬ÙˆØ¯ÛŒØª       |\n",
    "|---------------|--------------------|\n",
    "| Ø§Ù„ÙˆÙ† Ù…Ø§Ø³Ú©     | Ø´Ø®Øµ ğŸ‘¤             |\n",
    "| Û²Û°Û°Û²          | ØªØ§Ø±ÛŒØ® ğŸ“…           |\n",
    "| Ø§Ø³Ù¾ÛŒØ³â€ŒØ§ÛŒÚ©Ø³     | Ø³Ø§Ø²Ù…Ø§Ù† ğŸ¢         |\n",
    "| Ø¢Ù…Ø±ÛŒÚ©Ø§        | Ù…Ú©Ø§Ù† ğŸ™ï¸          |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§° Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ Ø¨Ø±Ø§ÛŒ NER:\n",
    "- **spaCy**: Ø³Ø±ÛŒØ¹ Ùˆ Ø¯Ù‚ÛŒÙ‚ (Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ)\n",
    "- **Stanza (Stanford NLP)**: Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù† Ø§Ø² Ø¬Ù…Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "- **transformers**: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BERT Ùˆ XLM-RoBERTa Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ø±Ø§ÛŒ NER\n",
    "- **ParsBERT NER**: Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ø§Ø² HooshvareLab Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ“Œ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø¯Ø§Ø±ØŒ Ú¯Ø§Ù… Ø§ÙˆÙ„ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø®Ø§Ù… Ø¨Ù‡ Ø¯Ø§Ù†Ø´ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª â€“ Ù¾Ù„ÛŒ Ù…ÛŒØ§Ù† Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f171784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from hazm import Normalizer, word_tokenize\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ù…Ø¯Ù„ NER ÙØ§Ø±Ø³ÛŒ\n",
    "model_name = \"HooshvareLab/bert-fa-base-uncased-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„\n",
    "text = \"Ø§Ù„ÙˆÙ† Ù…Ø§Ø³Ú© Ø¯Ø± Ø³Ø§Ù„ Û²Û°Û°Û² Ø´Ø±Ú©Øª Ø§Ø³Ù¾ÛŒØ³â€ŒØ§ÛŒÚ©Ø³ Ø±Ø§ Ø¯Ø± Ø¢Ù…Ø±ÛŒÚ©Ø§ ØªØ£Ø³ÛŒØ³ Ú©Ø±Ø¯.\"\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ hazm\n",
    "normalizer = Normalizer()\n",
    "normalized_text = normalizer.normalize(text)\n",
    "tokens = word_tokenize(normalized_text)\n",
    "\n",
    "# ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„\n",
    "inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "token_ids = inputs[\"input_ids\"].squeeze().tolist()\n",
    "words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„\n",
    "labels = model.config.id2label\n",
    "\n",
    "# Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ùˆ Ú†Ø§Ù¾ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "print(\"ğŸ“ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡:\\n\")\n",
    "for token, label_id in zip(words, predicted_class_ids):\n",
    "    label = labels[label_id]\n",
    "    if label != \"O\" and not token.startswith(\"##\"):\n",
    "        print(f\"{token} â†’ {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac801af",
   "metadata": {},
   "source": [
    "## ğŸ§  Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ (Topic Modeling)\n",
    "\n",
    "**Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ** ÛŒØ§ **Topic Modeling** ÛŒÚ©ÛŒ Ø§Ø² ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ø¯Ø± **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP)** Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± Ù¾Ù†Ù‡Ø§Ù† Ù…ÙˆØ¶ÙˆØ¹ÛŒ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ Ù…ØªÙ†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø³ØªÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ:\n",
    "Topic Modeling ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² **Ù…ÙˆØ¶ÙˆØ¹Ø§Øª (topics)** Ø±Ø§ Ø§Ø² Ù…ØªÙˆÙ† Ú©Ø´Ù Ú©Ù†Ø¯ Ùˆ ØªØ¹ÛŒÛŒÙ† Ú©Ù†Ø¯ **Ù‡Ø± Ø³Ù†Ø¯ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ú†Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§ØªÛŒ Ø§Ø³Øª** Ùˆ **Ù‡Ø± Ù…ÙˆØ¶ÙˆØ¹ Ø´Ø§Ù…Ù„ Ú†Ù‡ Ú©Ù„Ù…Ø§ØªÛŒ Ø§Ø³Øª**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ:\n",
    "- Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø®Ø¨Ø±ÛŒ ÛŒØ§ Ø¹Ù„Ù…ÛŒ\n",
    "- ØªØ­Ù„ÛŒÙ„ Ù†Ø¸Ø±Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ø¯ØºØ¯ØºÙ‡â€ŒÙ‡Ø§\n",
    "- Ú©Ø´Ù Ø³Ø§Ø®ØªØ§Ø± Ù¾Ù†Ù‡Ø§Ù† Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ ØªØ§Ø±ÛŒØ®ÛŒ ÛŒØ§ Ø­Ù‚ÙˆÙ‚ÛŒ\n",
    "- Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø­Ø¬ÛŒÙ…\n",
    "- Ù¾Ø§ÛŒØ´ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Ù…Ø­Ø¨ÙˆØ¨â€ŒØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§:\n",
    "- ğŸ”¸ **LDA (Latent Dirichlet Allocation)** â†’ Ø±Ø§ÛŒØ¬â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø¢Ù…Ø§Ø±\n",
    "- ğŸ”¸ NMF (Non-negative Matrix Factorization)\n",
    "- ğŸ”¸ LSI (Latent Semantic Indexing)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“– Ø®Ø±ÙˆØ¬ÛŒ LDA Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ\n",
    "Ù…Ø¯Ù„ LDA Ù‡Ø± **Ù…ÙˆØ¶ÙˆØ¹** Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª **ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø±ÙˆÛŒ Ú©Ù„Ù…Ø§Øª**ØŒ Ùˆ Ù‡Ø± **Ø³Ù†Ø¯** Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª **ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø±ÙˆÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª** Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "\n",
    "Ù…Ø«Ù„Ø§Ù‹:\n",
    "\n",
    "> Ù…ÙˆØ¶ÙˆØ¹ 1: [ Ø§Ù‚ØªØµØ§Ø¯: 0.3, Ø¯Ù„Ø§Ø±: 0.2, ØªÙˆØ±Ù…: 0.15, Ø¨Ø§Ù†Ú©: 0.1, Ø§Ø±Ø²: 0.1, ... ]  \n",
    "> Ù…ÙˆØ¶ÙˆØ¹ 2: [ ÙˆØ±Ø²Ø´: 0.4, ÙÙˆØªØ¨Ø§Ù„: 0.25, ØªÛŒÙ…: 0.2, Ù‚Ù‡Ø±Ù…Ø§Ù†: 0.1, Ø¬Ø§Ù…: 0.05, ... ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58750d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, word_tokenize, stopwords_list\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "\n",
    "# Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "documents = [\n",
    "    \"Ù‚ÛŒÙ…Øª Ø¯Ù„Ø§Ø± Ø§Ù…Ø±ÙˆØ² Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØª Ùˆ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ù†ÙˆØ³Ø§Ù† Ø¯Ø§Ø´Øª.\",\n",
    "    \"ØªÛŒÙ… Ù…Ù„ÛŒ ÙÙˆØªØ¨Ø§Ù„ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ù…Ø³Ø§Ø¨Ù‚Ø§Øª Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¢Ø³ÛŒØ§ Ù¾ÛŒØ±ÙˆØ² Ø´Ø¯.\",\n",
    "    \"Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ ØªÙˆØ±Ù… Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯.\",\n",
    "    \"Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ø¨Ø§Ø²ÛŒÚ©Ù† Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ø±Ø¯.\",\n",
    "    \"Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® Ø§Ø±Ø² Ø¨Ø§Ø¹Ø« Ù†Ú¯Ø±Ø§Ù†ÛŒ ÙØ¹Ø§Ù„Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\"\n",
    "]\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ùˆ Ø­Ø°Ù ØªÙˆÙ‚Ù\n",
    "normalizer = Normalizer()\n",
    "stop_words = set(stopwords_list())\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(normalizer.normalize(doc))\n",
    "    return [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "\n",
    "texts = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ùˆ Ú©ÙØ±Ù¾ÙˆØ³\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LDA\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10, random_state=42)\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ú©Ø´Ùâ€ŒØ´Ø¯Ù‡\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for i, topic in topics:\n",
    "    print(f\"Ù…ÙˆØ¶ÙˆØ¹ {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08a9aa",
   "metadata": {},
   "source": [
    "# Ø®Ø±ÙˆØ¬ÛŒ Ú©Ø¯ Ø¨Ø§Ù„Ø§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2488c2",
   "metadata": {},
   "source": [
    "Ù…ÙˆØ¶ÙˆØ¹ 1: 0.300*\"Ø¯Ù„Ø§Ø±\" + 0.250*\"Ø§Ø±Ø²\" + 0.200*\"Ø¨Ø§Ø²Ø§Ø±\" + 0.150*\"Ù†Ø±Ø®\" + 0.100*\"ØªÙˆØ±Ù…\"\n",
    "Ù…ÙˆØ¶ÙˆØ¹ 2: 0.400*\"ÙÙˆØªØ¨Ø§Ù„\" + 0.300*\"ØªÛŒÙ…\" + 0.200*\"Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ\" + 0.100*\"Ø¨Ø§Ø²ÛŒÚ©Ù†\" + 0.100*\"Ø§Ø³ØªÙ‚Ù„Ø§Ù„\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742dfa2",
   "metadata": {},
   "source": [
    "## ğŸ§© Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù†Ù‚Ø´ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Semantic Role Labeling - SRL)\n",
    "\n",
    "**Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù†Ù‚Ø´ Ù…Ø¹Ù†Ø§ÛŒÛŒ** ÛŒØ§ **Semantic Role Labeling (SRL)** ÛŒÚ©ÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ù…Ù‡Ù… Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP) Ø§Ø³Øª Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ù‚Ø´ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª ÛŒØ§ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡** Ø§Ø³ØªØŒ Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ ÙØ¹Ù„ Ø§ØµÙ„ÛŒ.\n",
    "\n",
    "---\n",
    "\n",
    "### â“ SRL Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ ÛŒØ¹Ù†ÛŒ Ú†Ù‡ØŸ\n",
    "SRL ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¨ÙÙ‡Ù…Ø¯:\n",
    "- **Ú†Ù‡ Ú©Ø³ÛŒ** Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŸ\n",
    "- **Ú†Ù‡ Ú©Ø§Ø±ÛŒ** Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ\n",
    "- **Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ú©Ø³ÛŒ** ÛŒØ§ **Ú†Ù‡ Ú†ÛŒØ²ÛŒ**ØŸ\n",
    "- **Ø¯Ø± Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ** Ùˆ **Ú©Ø¬Ø§**ØŸ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡:\n",
    "\n",
    "> \"Ø¹Ù„ÛŒ Ø¯ÛŒØ±ÙˆØ² Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ÛŒÚ© Ú©ØªØ§Ø¨ Ø®ÙˆØ§Ù†Ø¯.\"\n",
    "\n",
    "| Ø¹Ø¨Ø§Ø±Øª       | Ù†Ù‚Ø´ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Semantic Role) |\n",
    "|-------------|-----------------------------|\n",
    "| Ø¹Ù„ÛŒ         | Ø¹Ø§Ù…Ù„ (Agent) ğŸ‘¤             |\n",
    "| ÛŒÚ© Ú©ØªØ§Ø¨     | Ù…ÙØ¹ÙˆÙ„ (Theme) ğŸ“š            |\n",
    "| Ø¯ÛŒØ±ÙˆØ²       | Ø²Ù…Ø§Ù† (Temporal) â°           |\n",
    "| Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ | Ù…Ú©Ø§Ù† (Locative) ğŸ“          |\n",
    "| Ø®ÙˆØ§Ù†Ø¯       | Ú©Ù†Ø´ (Predicate) ğŸ”§          |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Ø§Ø¬Ø²Ø§ÛŒ SRL:\n",
    "\n",
    "1. **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ¹Ù„ (Predicate)**  \n",
    "   - ÛŒØ§ÙØªÙ† Ø§ÙØ¹Ø§Ù„ Ø§ØµÙ„ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "\n",
    "2. **ØªØ´Ø®ÛŒØµ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ (Arguments)**  \n",
    "   - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ØµØ±ÛŒ Ú©Ù‡ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ ÙØ¹Ù„ Ù‡Ø³ØªÙ†Ø¯\n",
    "\n",
    "3. **Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù†Ù‚Ø´â€ŒÙ‡Ø§ (Role Labeling)**  \n",
    "   - Ø§Ø®ØªØµØ§Øµ Ù†Ù‚Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ AgentØŒ PatientØŒ InstrumentØŒ LocationØŒ Time Ùˆ ...\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Ù†Ù‚Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø±Ø§ÛŒØ¬:\n",
    "\n",
    "| Ù†Ù‚Ø´ Ù…Ø¹Ù†Ø§ÛŒÛŒ     | ØªÙˆØ¶ÛŒØ­                                 | Ù…Ø«Ø§Ù„ Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ù‚Ø¨Ù„ |\n",
    "|----------------|----------------------------------------|-------------------|\n",
    "| Agent          | Ú©Ø³ÛŒ Ú©Ù‡ Ú©Ù†Ø´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯             | Ø¹Ù„ÛŒ               |\n",
    "| Theme/Patient  | Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ú©Ù†Ø´ Ø±ÙˆÛŒ Ø¢Ù† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯        | Ú©ØªØ§Ø¨              |\n",
    "| Location       | Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ú©Ù†Ø´ Ø¯Ø± Ø¢Ù† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯         | Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡          |\n",
    "| Temporal       | Ø²Ù…Ø§Ù† Ø§Ù†Ø¬Ø§Ù… Ú©Ù†Ø´                         | Ø¯ÛŒØ±ÙˆØ²             |\n",
    "| Instrument     | ÙˆØ³ÛŒÙ„Ù‡ Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø±                         | (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø§ Ø®ÙˆØ¯Ú©Ø§Ø±)  |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ SRL:\n",
    "\n",
    "- ğŸ” **Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„â€ŒÙ‡Ø§ (Question Answering):**  \n",
    "  Ø¯Ø±Ú© Ø¨Ù‡ØªØ± Ø±Ø§Ø¨Ø·Ù‡â€ŒÛŒ Ø¨ÛŒÙ† Ø§Ø¬Ø²Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚\n",
    "\n",
    "- ğŸ¤– **Ø±Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ú¯ÙØªâ€ŒÙˆÚ¯Ùˆ (Chatbots):**  \n",
    "  Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ù‚ÛŒÙ‚ Ù…Ù†Ø¸ÙˆØ± Ú©Ø§Ø±Ø¨Ø± Ùˆ ÙˆØ§Ú©Ù†Ø´ Ù…Ù†Ø§Ø³Ø¨\n",
    "\n",
    "- ğŸ“š **ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ (Machine Translation):**  \n",
    "  Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ø²Ø¨Ø§Ù† Ù…Ù‚ØµØ¯\n",
    "\n",
    "- ğŸ§  **ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù…ØªÙ†:**  \n",
    "  Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚â€ŒØªØ±ÛŒ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ù…ÙÙ‡ÙˆÙ…ÛŒ Ø¬Ù…Ù„Ø§Øª\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ¯ SRL Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ù…Ø§Ø´ÛŒÙ† ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù†Ø¨ÛŒÙ†Ø¯ØŒ Ø¨Ù„Ú©Ù‡ Ù…Ø¹Ù†Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ù¾Ø´Øª Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ† Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù†ÛŒØ² Ø¨ÙÙ‡Ù…Ø¯ â€” ÛŒØ¹Ù†ÛŒ Ú¯Ø§Ù…ÛŒ Ø¨Ù‡â€ŒØ³ÙˆÛŒ **Ø¯Ø±Ú© Ø²Ø¨Ø§Ù†ÛŒ Ø§Ù†Ø³Ø§Ù†â€ŒÙ…Ø§Ù†Ù†Ø¯**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø§Ø² SnappFood\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„\n",
    "text = \"Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§ÛŒÙ† Ø³Ø§ÛŒØª Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø³ÙØ§Ø±Ø´ Ø®ÛŒÙ„ÛŒ Ú©Ù†Ø¯ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\"\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ ÙØ±Ù…Øª ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Ø§Ø¹Ù…Ø§Ù„ softmax Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# ØªØ¹Ø±ÛŒÙ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "labels = [\"Ù…Ù†ÙÛŒ ğŸ˜ \", \"Ø®Ù†Ø«ÛŒ ğŸ˜\", \"Ù…Ø«Ø¨Øª ğŸ˜Š\"]\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "prob_dict = dict(zip(labels, np.round(probs.numpy()[0], 3)))\n",
    "\n",
    "# Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬\n",
    "print(f\"ğŸ“ Ø¬Ù…Ù„Ù‡: {text}\")\n",
    "print(f\"ğŸ¯ Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨: {predicted_label}\")\n",
    "print(\"ğŸ“Š Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§:\", prob_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075647c",
   "metadata": {},
   "source": [
    "## ğŸ”— ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ù†Ø­ÙˆÛŒ (Dependency Parsing)\n",
    "\n",
    "**ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ (Dependency Parsing)** ÛŒÚ©ÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ø§ØµÙ„ÛŒ Ø¯Ø± **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP)** Ø§Ø³Øª Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù†Ø­ÙˆÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª** Ø§Ø³Øª.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  ØªØ¹Ø±ÛŒÙ Ø³Ø§Ø¯Ù‡:\n",
    "Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒØŒ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ© **Ø¯Ø±Ø®Øª ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ** Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†:\n",
    "- Ù‡Ø± **Ú©Ù„Ù…Ù‡** (Token) Ø¯Ø§Ø±Ø§ÛŒ ÛŒÚ© **Ú©Ù„Ù…Ù‡ ÙˆØ§Ù„Ø¯ (Head)** Ø§Ø³Øª.\n",
    "- Ø±Ø§Ø¨Ø·Ù‡â€ŒÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ/Ù†Ø­ÙˆÛŒ Ø¢Ù† Ú©Ù„Ù…Ù‡ Ø¨Ø§ ÙˆØ§Ù„Ø¯Ø´ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ù…Ø«Ù„Ø§Ù‹ ÙØ§Ø¹Ù„ØŒ Ù…ÙØ¹ÙˆÙ„ØŒ ØµÙØªØŒ Ù…ØªÙ…Ù… Ùˆ...).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ\n",
    "Dependency Parsing Ø¨Ù‡ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§:\n",
    "- Ø³Ø§Ø®ØªØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨ÙÙ‡Ù…ÛŒÙ… ğŸ“\n",
    "- Ø§Ø±ØªØ¨Ø§Ø· Ù†Ø­ÙˆÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡ÛŒÙ… ğŸ”\n",
    "- ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ… ğŸ¤–\n",
    "- ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒØŒ Ú†Øªâ€ŒØ¨Ø§ØªØŒ Ùˆ QA ÙØ±Ø§Ù‡Ù… Ú©Ù†ÛŒÙ….\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Ù…Ø«Ø§Ù„:\n",
    "\n",
    "> Ø¬Ù…Ù„Ù‡: Â«Ø¹Ù„ÛŒ Ø¨Ø§ Ø¯Ù‚Øª Ú©ØªØ§Ø¨ Ø±Ø§ Ø®ÙˆØ§Ù†Ø¯.Â»\n",
    "\n",
    "Ø³Ø§Ø®ØªØ§Ø± ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø´Ú©Ù„ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:\n",
    "\n",
    "- \"Ø®ÙˆØ§Ù†Ø¯\" â† ÙØ¹Ù„ Ø§ØµÙ„ÛŒ (Root) ğŸ”§\n",
    "- \"Ø¹Ù„ÛŒ\" â† ÙØ§Ø¹Ù„ (nsubj) ğŸ‘¤ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ \"Ø®ÙˆØ§Ù†Ø¯\"\n",
    "- \"Ú©ØªØ§Ø¨\" â† Ù…ÙØ¹ÙˆÙ„ Ù…Ø³ØªÙ‚ÛŒÙ… (obj) ğŸ“˜ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ \"Ø®ÙˆØ§Ù†Ø¯\"\n",
    "- \"Ø±Ø§\" â† ÙˆØ§Ø¨Ø³ØªÙ‡ Ù†Ø­ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…ÙØ¹ÙˆÙ„ (case)\n",
    "- \"Ø¨Ø§ Ø¯Ù‚Øª\" â† Ù‚ÛŒØ¯ Ø­Ø§Ù„Øª (advmod) ğŸ§  ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ \"Ø®ÙˆØ§Ù†Ø¯\"\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø±Ø§ÛŒØ¬:\n",
    "\n",
    "| Ø±Ø§Ø¨Ø·Ù‡       | ØªÙˆØ¶ÛŒØ­                              |\n",
    "|-------------|--------------------------------------|\n",
    "| `nsubj`     | ÙØ§Ø¹Ù„ Ø¬Ù…Ù„Ù‡                          |\n",
    "| `obj`       | Ù…ÙØ¹ÙˆÙ„ Ù…Ø³ØªÙ‚ÛŒÙ…                       |\n",
    "| `iobj`      | Ù…ÙØ¹ÙˆÙ„ ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ…                    |\n",
    "| `advmod`    | Ù‚ÛŒØ¯ Ø­Ø§Ù„Øª ÛŒØ§ Ø²Ù…Ø§Ù†                   |\n",
    "| `amod`      | ØµÙØª ØªÙˆØµÛŒÙÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù…                |\n",
    "| `root`      | Ø±ÛŒØ´Ù‡ Ø¬Ù…Ù„Ù‡ (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ÙØ¹Ù„ Ø§ØµÙ„ÛŒ)       |\n",
    "| `det`       | Ø­Ø±Ù ØªØ¹Ø±ÛŒÙ ÛŒØ§ Ù†Ø´Ø§Ù†Ú¯Ø±                |\n",
    "| `case`      | Ù†Ù‚Ø´â€ŒÙ†Ù…Ø§Ù‡Ø§ Ù…Ø«Ù„ Â«Ø±Ø§Â»ØŒ Â«Ø¨Ù‡Â»ØŒ Â«Ø§Ø²Â»     |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§° Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Dependency Parsing:\n",
    "- **spaCy** âœ… (Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø®ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØ› Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ)\n",
    "- **Stanza (Stanford NLP)** âœ… (Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ)\n",
    "- **UDPipe** âœ… (Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Universal Dependencies)\n",
    "- **ParsBERT + ParsTense + ParsTagger** ğŸ”¬ (Ù¾Ø´ØªÙ‡ ÙØ§Ø±Ø³ÛŒ NLP Ø§Ø² Hooshvare)\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ” Dependency Parsing Ù¾Ù„ÛŒ Ø§Ø³Øª Ø¨ÛŒÙ† Ù†Ø­Ùˆ (Syntax) Ùˆ Ù…Ø¹Ù†Ø§ (Semantics) â€“ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ÙÙ‡Ù… Â«Ú†Ù‡ Ú©Ø³ÛŒ Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù‡ Ùˆ Ú†Ú¯ÙˆÙ†Ù‡ØŸÂ»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ce194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SnappFood\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³\n",
    "text = \"ØºØ°Ø§ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ù…Ø²Ù‡ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ù¾ÛŒÚ© Ø®ÛŒÙ„ÛŒ Ø¯ÛŒØ± Ø±Ø³ÛŒØ¯.\"\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ logits Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§ softmax\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ (Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù…Ú©Ù†)\n",
    "labels = [\"Ù…Ù†ÙÛŒ ğŸ˜ \", \"Ø®Ù†Ø«ÛŒ ğŸ˜\", \"Ù…Ø«Ø¨Øª ğŸ˜Š\"]\n",
    "\n",
    "# Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "prob_dict = dict(zip(labels, np.round(probs.numpy()[0], 3)))\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡\n",
    "print(f\"ğŸ“ Ø¬Ù…Ù„Ù‡: {text}\")\n",
    "print(f\"ğŸ¯ Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨: {predicted_label}\")\n",
    "print(\"ğŸ“Š Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§:\", prob_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dca8b",
   "metadata": {},
   "source": [
    "## ğŸ§  ÙˆØ±Ø¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ (Word Embeddings)\n",
    "\n",
    "**ÙˆØ±Ø¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ (Word Embedding)** ÛŒÚ©ÛŒ Ø§Ø² Ù…ÙØ§Ù‡ÛŒÙ… Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP) Ø§Ø³Øª Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† **ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†** Ø§Ø³ØªØŒ Ø¨Ù‡â€ŒÚ¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ú©Ù‡ **Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ø§ÛŒÙ† ÙØ¶Ø§ Ø­ÙØ¸ Ø´ÙˆØ¯**.\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Ú†Ø±Ø§ Ø¨Ù‡ Word Embedding Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…ØŸ\n",
    "Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ù…ØªÙ†ÛŒ Ú©Ø§Ø± Ú©Ù†Ù†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø¨Ø§ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø±Ø§ Ø¨Ù‡ **Ù†Ù…Ø§ÛŒØ´ Ø¹Ø¯Ø¯ÛŒ (Ø¨Ø±Ø¯Ø§Ø±)** ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… Ú©Ù‡ **Ù…Ø¹Ù†Ø§ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ø²Ø¨Ø§Ù†ÛŒ Ø±Ø§ Ø­ÙØ¸ Ú©Ù†Ø¯**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Word Embedding:\n",
    "- Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¨Ù‡ ÛŒÚ© **Ø¨Ø±Ø¯Ø§Ø± Ú†Ù†Ø¯Ø¨Ø¹Ø¯ÛŒ (Ù…Ø«Ù„Ø§Ù‹ 100 ÛŒØ§ 300 Ø¨ÙØ¹Ø¯)** ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "- Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù‡Ù…â€ŒÙ…Ø¹Ù†Ø§ Ø¨Ù‡ Ù‡Ù… Ù†Ø²Ø¯ÛŒÚ© Ù‡Ø³ØªÙ†Ø¯ (Ù…Ø§Ù†Ù†Ø¯ \"Ø´Ø§Ù‡\" Ùˆ \"Ù¾Ø§Ø¯Ø´Ø§Ù‡\")\n",
    "- Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø§ ÙØ§ØµÙ„Ù‡â€ŒÛŒ Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ (Cosine Similarity) Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú¯Ø±ÙØª\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Word Embedding\n",
    "\n",
    "### 1. ğŸ§  Word2Vec (Ù…Ø¯Ù„ Ú¯ÙˆÚ¯Ù„)\n",
    "- ØªÙˆØ³Ø· Google Ø¯Ø± Ø³Ø§Ù„ 2013 Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯\n",
    "- Ø¯Ùˆ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§ØµÙ„ÛŒ:\n",
    "  - **CBOW (Continuous Bag of Words):** Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…Ø³Ø§ÛŒÚ¯Ø§Ù† Ø¢Ù†\n",
    "  - **Skip-Gram:** Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‡Ù…Ø³Ø§ÛŒÚ¯Ø§Ù† ÛŒÚ© Ú©Ù„Ù…Ù‡\n",
    "- Ù…Ø²ÛŒØª: Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹ØŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯ÛŒØªØ§ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ\n",
    "\n",
    "### 2. ğŸ§  GloVe (Global Vectors - Ù…Ø¯Ù„ Ø§Ø³ØªÙ†ÙÙˆØ±Ø¯)\n",
    "- ØªÙˆØ³Ø· Stanford Ø¯Ø± Ø³Ø§Ù„ 2014 Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯\n",
    "- ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø¢Ù…Ø§Ø± Ø¬Ù‡Ø§Ù†ÛŒ Ùˆ Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ù‡Ù…Ø³Ø§ÛŒÚ¯ÛŒ\n",
    "- Ù…Ø²ÛŒØª: Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± (Ù…Ø«Ù„Ø§Ù‹ ÙØ§ØµÙ„Ù‡â€ŒÛŒ Ù…Ù†ÙÛŒ + Ù…Ø±Ø¯ â‰ˆ Ø²Ù†)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Ù…Ø«Ø§Ù„ ØªØµÙˆÛŒØ±ÛŒ Ø§Ø² Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Word Embedding\n",
    "\n",
    "> **king** - **man** + **woman** â‰ˆ **queen** ğŸ‘‘\n",
    "\n",
    "> **ØªÙ‡Ø±Ø§Ù†** - **Ø§ÛŒØ±Ø§Ù†** + **ÙØ±Ø§Ù†Ø³Ù‡** â‰ˆ **Ù¾Ø§Ø±ÛŒØ³** ğŸ—¼\n",
    "\n",
    "Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ Ù…Ø¯Ù„ ÙˆØ§Ù‚Ø¹Ø§Ù‹ **Ø¯Ø±Ú© Ø¶Ù…Ù†ÛŒ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ø²Ø¨Ø§Ù†ÛŒ Ùˆ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ** Ø¯Ø§Ø±Ø¯!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Word Embedding:\n",
    "- ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Sentiment Analysis)\n",
    "- Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ (Topic Modeling)\n",
    "- Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±\n",
    "- ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ\n",
    "- ØªØ´Ø®ÛŒØµ Ø´Ø¨Ø§Ù‡Øª Ù…ØªÙ†ÛŒ\n",
    "- Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„Ø§Øª (Question Answering)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ:\n",
    "- **Gensim** Ø¨Ø±Ø§ÛŒ Word2Vec\n",
    "- **spaCy** Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ ÙˆÚ©ØªÙˆØ±Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡\n",
    "- **fastText** Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø²ÛŒØ±Ú©Ù„Ù…Ù‡ (Ù…Ù†Ø§Ø³Ø¨ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ)\n",
    "- **Transformers** (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BERT, GPT Ùˆ...) Ú©Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ú©Ø§Ù†ØªÚ©Ø³Øªâ€ŒÙ…Ø­ÙˆØ± ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯\n",
    "\n",
    "---\n",
    "\n",
    "> âœ¨ ÙˆØ±Ø¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ Ø¯Ø± NLP Ø¨ÙˆØ¯Ø› Ø¨Ø§Ø¹Ø« Ø´Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ **Ù†Ù‡â€ŒØªÙ†Ù‡Ø§ Ú©Ù„Ù…Ø§ØªØŒ Ø¨Ù„Ú©Ù‡ Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ† Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù‡Ù… Ø¨ÙÙ‡Ù…Ù†Ø¯.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list\n",
    "import numpy as np\n",
    "\n",
    "# Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡ Ø¨Ø§ Ø§Ø­Ø³Ø§Ø³\n",
    "positive_sentences = [\n",
    "    \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§Ø²ÛŒÚ¯Ø±Ø§Ù† ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯\",\n",
    "    \"Ú©ÛŒÙÛŒØª ØºØ°Ø§ Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨ Ùˆ Ø®ÙˆØ´Ù…Ø²Ù‡ Ø¨ÙˆØ¯\",\n",
    "    \"Ø§Ø² Ø®Ø¯Ù…Ø§Øª Ø´Ù…Ø§ Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø¶ÛŒ Ù‡Ø³ØªÙ…\",\n",
    "    \"Ù…Ø­ØµÙˆÙ„ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…ÙˆÙ†ÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³ØªÙ…\"\n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®Ú¯Ùˆ Ù†Ø¨ÙˆØ¯Ù†Ø¯\",\n",
    "    \"Ø¨Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®Ø±Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ø¯ÛŒØ± Ø±Ø³ÛŒØ¯\",\n",
    "    \"Ø§ÛŒÙ† Ú©Ø§Ù„Ø§ Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª Ø¨ÙˆØ¯ Ùˆ Ø§ØµÙ„Ø§Ù‹ ØªÙˆØµÛŒÙ‡ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù…\",\n",
    "    \"Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ú©Ù†Ø§Ù† Ø¨ÛŒâ€ŒØ§Ø­ØªØ±Ø§Ù…ÛŒâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯\"\n",
    "]\n",
    "\n",
    "# ØªØ±Ú©ÛŒØ¨ Ø¬Ù…Ù„Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Word2Vec\n",
    "all_sentences = positive_sentences + negative_sentences\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ\n",
    "normalizer = Normalizer()\n",
    "stopwords = set(stopwords_list())\n",
    "\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(normalizer.normalize(sentence))\n",
    "    return [w for w in tokens if w not in stopwords and len(w) > 2]\n",
    "\n",
    "processed = [preprocess(sent) for sent in all_sentences]\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Word2Vec\n",
    "model = Word2Vec(sentences=processed, vector_size=100, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø± Ø¬Ù…Ù„Ù‡\n",
    "def sentence_vector(sentence, model):\n",
    "    words = preprocess(sentence)\n",
    "    vecs = [model.wv[w] for w in words if w in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "# Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ù…Ù„Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ\n",
    "vec_pos = np.mean([sentence_vector(sent, model) for sent in positive_sentences], axis=0)\n",
    "vec_neg = np.mean([sentence_vector(sent, model) for sent in negative_sentences], axis=0)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„\n",
    "test_sentence = \"Ø§Ø±Ø³Ø§Ù„ Ø®ÛŒÙ„ÛŒ Ø³Ø±ÛŒØ¹ Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ú©ÛŒÙÛŒØª Ù¾Ø§ÛŒÛŒÙ† Ø¯Ø§Ø´Øª\"\n",
    "\n",
    "vec_test = sentence_vector(test_sentence, model)\n",
    "\n",
    "# Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨Ø§ Ø¯Ùˆ Ø¯Ø³ØªÙ‡ Ø§Ø­Ø³Ø§Ø³\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-9)\n",
    "\n",
    "sim_pos = cosine_similarity(vec_test, vec_pos)\n",
    "sim_neg = cosine_similarity(vec_test, vec_neg)\n",
    "\n",
    "# Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ\n",
    "if sim_pos > sim_neg:\n",
    "    print(\"ğŸ¯ Ù†ØªÛŒØ¬Ù‡: Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨ â†’ Ù…Ø«Ø¨Øª ğŸ˜Š\")\n",
    "else:\n",
    "    print(\"ğŸ¯ Ù†ØªÛŒØ¬Ù‡: Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨ â†’ Ù…Ù†ÙÛŒ ğŸ˜ \")\n",
    "\n",
    "print(f\"ğŸ“Š Ø´Ø¨Ø§Ù‡Øª Ø¨Ù‡ Ù…Ø«Ø¨Øª: {sim_pos:.3f} | Ø´Ø¨Ø§Ù‡Øª Ø¨Ù‡ Ù…Ù†ÙÛŒ: {sim_neg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9ea25",
   "metadata": {},
   "source": [
    "## ğŸ§  Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø²Ø¨Ø§Ù† (Language Modeling)\n",
    "\n",
    "**Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø²Ø¨Ø§Ù†** ÛŒØ§ **Language Modeling** ÛŒÚ©ÛŒ Ø§Ø² Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒâ€ŒØªØ±ÛŒÙ† Ù…ÙØ§Ù‡ÛŒÙ… Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP) Ø§Ø³Øª Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¢Ù…Ø§Ø±ÛŒ Ø²Ø¨Ø§Ù† Ø§Ù†Ø³Ø§Ù†ÛŒ** Ø§Ø³Øª. Ø¨Ù‡â€ŒØ¹Ø¨Ø§Ø±Øª Ø³Ø§Ø¯Ù‡ØŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ **Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ Ú©Ø¯Ø§Ù… Ú©Ù„Ù…Ø§Øª Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø¹Ø¯ Ø§Ø² ÛŒÚ© ØªÙˆØ§Ù„ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø¨ÛŒØ§ÛŒÙ†Ø¯**.\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ú†Ù‡â€ŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ\n",
    "Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† ÛŒÚ© ØªÙˆØ§Ù„ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ:\n",
    "- ğŸ”® Ø§Ø­ØªÙ…Ø§Ù„ ÙˆÙ‚ÙˆØ¹ ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡â€ŒÛŒ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "- ğŸ“– ÛŒØ§ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø± Ù…ØªÙ† Ø±Ø§ Ù¾Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "- ğŸ§  ÛŒØ§ Ù…Ø¹Ù†Ø§ÛŒ Ú©Ù„ÛŒ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¯Ø±Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Language Modeling:\n",
    "- ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø®ÙˆØ¯Ú©Ø§Ø± âœï¸\n",
    "- Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§ ğŸ¤–\n",
    "- ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ ğŸŒ\n",
    "- ØªÚ©Ù…ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…ØªÙ† ğŸ”¤\n",
    "- Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„Ø§Øª â“\n",
    "- ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ğŸ˜ŠğŸ˜ \n",
    "- Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙˆÙ† ğŸ“„\n",
    "- Ø¯Ø±Ú© Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ùˆ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ ğŸ§©\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ (Language Models)\n",
    "\n",
    "### ğŸ”¸ 1. Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ (Autoregressive) â€“ Ù…Ø§Ù†Ù†Ø¯ GPT\n",
    "- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡â€ŒÛŒ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ù‚Ø¨Ù„ÛŒ\n",
    "- Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø±: ÙÙ‚Ø· Ø§Ø² Ú†Ù¾ Ø¨Ù‡ Ø±Ø§Ø³Øª\n",
    "- Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†\n",
    "\n",
    "> **Ù…Ø«Ø§Ù„:**  \n",
    "> ÙˆØ±ÙˆØ¯ÛŒ: \"Ù…Ù† Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ\" â†’ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯: \"Ø®ÙˆØ´Ø­Ø§Ù„\"\n",
    "\n",
    "### ğŸ”¸ 2. Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ´Ø´ÛŒ (Autoencoding) â€“ Ù…Ø§Ù†Ù†Ø¯ BERT\n",
    "- ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯ Ú©Ù‡ Ø¬Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù…Ø§Ø³Ú©â€ŒØ´Ø¯Ù‡ Ø±Ø§ Ù¾Ø± Ú©Ù†Ù†Ø¯\n",
    "- Ù†Ú¯Ø§Ù‡ Ø¯ÙˆØ·Ø±ÙÙ‡ Ø¨Ù‡ Ø¬Ù…Ù„Ù‡ (Ú†Ù¾ Ùˆ Ø±Ø§Ø³Øª Ù‡Ù…â€ŒØ²Ù…Ø§Ù†)\n",
    "- Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†\n",
    "\n",
    "> **Ù…Ø«Ø§Ù„:**  \n",
    "> ÙˆØ±ÙˆØ¯ÛŒ: \"Ù…Ù† Ø§Ù…Ø±ÙˆØ² [MASK] Ù‡Ø³ØªÙ…\" â†’ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯: \"Ø®ÙˆØ´Ø­Ø§Ù„\"\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¬ Ù…Ù‚Ø§ÛŒØ³Ù‡ BERT Ùˆ GPT\n",
    "\n",
    "| ÙˆÛŒÚ˜Ú¯ÛŒ            | BERT ğŸ”                    | GPT âœï¸                    |\n",
    "|------------------|----------------------------|----------------------------|\n",
    "| Ù†ÙˆØ¹ Ù…Ø¹Ù…Ø§Ø±ÛŒ       | Autoencoder                | Autoregressive             |\n",
    "| Ø¬Ù‡Øª Ø®ÙˆØ§Ù†Ø¯Ù†       | Ø¯ÙˆØ·Ø±ÙÙ‡ (bidirectional)     | ÛŒÚ©â€ŒØ·Ø±ÙÙ‡ (left to right)    |\n",
    "| Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø§ØµÙ„ÛŒ      | Ø¯Ø±Ú© Ù…ØªÙ†                    | ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†                  |\n",
    "| Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ       | Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒØŒ NERØŒ QA         | Ú†Øªâ€ŒØ¨Ø§ØªØŒ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù…ØªÙ†ØŒ QA    |\n",
    "| Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ    | BERTØŒ RoBERTaØŒ ALBERT       | GPT-2ØŒ GPT-3ØŒ GPT-4         |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ Ø¯Ø± Ø­ÙˆØ²Ù‡ Language Modeling\n",
    "\n",
    "| Ù†Ø§Ù… Ù…Ø¯Ù„      | ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡                        |\n",
    "|--------------|-------------------------------------|\n",
    "| **BERT**     | Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚ Ø§Ø² Ù…ØªÙ† Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾ÙˆØ´Ø´ÛŒ   |\n",
    "| **GPT**      | ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø±ÙˆØ§Ù† Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†Ø§Ù†Ù‡|\n",
    "| **T5**       | Ù…Ø¯Ù„ Ù‡Ù…Ù‡â€ŒÚ©Ø§Ø±Ù‡ (text-to-text)        |\n",
    "| **XLNet**    | ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ BERT Ùˆ GPT         |\n",
    "| **DistilBERT** | Ù†Ø³Ø®Ù‡â€ŒÛŒ Ø³Ø¨Ú©â€ŒØ´Ø¯Ù‡â€ŒÛŒ BERT Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± |\n",
    "| **BART**     | Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒØŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒØŒ ØªØ±Ø¬Ù…Ù‡ Ùˆ QA |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Ú©Ø§Ø±Ø¨Ø±Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ:\n",
    "Ù…Ø«Ù„Ø§Ù‹ Ø¯Ø± Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§ØŒ Ù…Ø¯Ù„ GPT Ù‚Ø§Ø¯Ø±Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§Ù†Ø³Ø§Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù‡:  \n",
    "> Ú©Ø§Ø±Ø¨Ø±: Ø³Ù„Ø§Ù…! Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ  \n",
    "> GPT: Ø³Ù„Ø§Ù…! Ù…Ù…Ù†ÙˆÙ†Ù…ØŒ ØªÙˆ Ú†Ø·ÙˆØ±ÛŒØŸ Ø§Ù…Ø±ÙˆØ² Ú©Ù…Ú©Øª Ú©Ù†Ù…ØŸ\n",
    "\n",
    "ÛŒØ§ Ø¯Ø± BERT:\n",
    "> Ø¬Ù…Ù„Ù‡: \"Ø¹Ù„ÛŒ Ø¯ÛŒØ±ÙˆØ² Ø¨Ù‡ [MASK] Ø±ÙØª.\"  \n",
    "> Ø®Ø±ÙˆØ¬ÛŒ: \"Ù…Ø¯Ø±Ø³Ù‡\" ÛŒØ§ \"Ø®Ø§Ù†Ù‡\"\n",
    "\n",
    "---\n",
    "\n",
    "> âœ¨ Language Models Ù…Ø«Ù„ Ù…ØºØ² Ø²Ø¨Ø§Ù† Ù…Ø§Ø´ÛŒÙ† Ù‡Ø³ØªÙ†Ø¯ â€“ Ø¢Ù†â€ŒÙ‡Ø§ Ù†Ù‡â€ŒØªÙ†Ù‡Ø§ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…ÛŒâ€ŒÙÙ‡Ù…Ù†Ø¯ØŒ Ø¨Ù„Ú©Ù‡ **Ù…Ù†Ø·Ù‚ØŒ ØªØ±ØªÛŒØ¨ØŒ Ù…Ø¹Ù†Ø§ Ùˆ Ø§Ø­Ø³Ø§Ø³** Ù¾Ø´Øª Ø²Ø¨Ø§Ù† Ø§Ù†Ø³Ø§Ù† Ø±Ø§ Ù†ÛŒØ² Ø¯Ø±Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† BERT ÙØ§Ø±Ø³ÛŒ\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø¬Ø§ÛŒ Ø®Ø§Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø¯Ø±Ú© Ù…Ø¯Ù„ Ø§Ø² Ø¬Ù…Ù„Ù‡)\n",
    "sentence = \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø®ÛŒÙ„ÛŒ [MASK] Ø¨ÙˆØ¯ Ùˆ Ù‡Ù…Ù‡ Ø§Ø²Ø´ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯.\"\n",
    "\n",
    "# ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Tensor\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ùˆ Ú¯Ø±ÙØªÙ† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ø§Ø³Ú©â€ŒØ´Ø¯Ù‡\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "print(f\"ğŸ“ Ø¬Ù…Ù„Ù‡ ÙˆØ±ÙˆØ¯ÛŒ: {sentence}\")\n",
    "print(\"ğŸ¯ Ú©Ù„Ù…Ø§Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ [MASK]:\\n\")\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    word = tokenizer.decode([token])\n",
    "    print(f\"ğŸ”¹ {word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
