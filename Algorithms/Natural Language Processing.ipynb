{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0164b08",
   "metadata": {},
   "source": [
    "# 🧠 آشنایی با پردازش زبان طبیعی (Natural Language Processing - NLP)\n",
    "\n",
    "**پردازش زبان طبیعی (NLP)** یکی از شاخه‌های مهم **هوش مصنوعی 🤖** است که به تعامل و ارتباط کامپیوتر با زبان انسان‌ها 🗣️ می‌پردازد. در واقع NLP سعی دارد به کامپیوترها کمک کند تا زبان طبیعی انسان‌ها را بفهمند، تفسیر کنند و به آن پاسخ دهند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 اهداف اصلی NLP\n",
    "- 🧾 درک معنی و مفهوم متن\n",
    "- 📌 استخراج اطلاعات مهم از متن\n",
    "- 🌐 ترجمه زبان‌ها به یکدیگر\n",
    "- 😊 تشخیص و طبقه‌بندی احساسات\n",
    "- ✂️ خلاصه‌سازی متون طولانی\n",
    "- 📝 تولید متن و گفتار به زبان طبیعی\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 کاربردهای رایج NLP\n",
    "- 🌍 **مترجم‌های آنلاین:** مانند Google Translate\n",
    "- 🎙️ **دستیارهای صوتی هوشمند:** مانند Siri، Alexa و Google Assistant\n",
    "- 💬 **سیستم‌های تحلیل احساسات:** تحلیل نظرات کاربران در شبکه‌های اجتماعی\n",
    "- 🤖 **ربات‌های چت (Chatbots):** پاسخ‌گویی خودکار به کاربران\n",
    "- 🎯 **سیستم‌های پیشنهاددهنده:** توصیه فیلم، کتاب، موسیقی و...\n",
    "- ⚖️ **تحلیل متن‌های حقوقی و پزشکی:** استخراج اطلاعات کلیدی از اسناد تخصصی\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 تکنیک‌ها و ابزارهای معروف در NLP\n",
    "\n",
    "### 🧹 پیش‌پردازش متن (Text Preprocessing)\n",
    "- حذف نویز 🧽\n",
    "- ریشه‌یابی کلمات (Stemming) 🌱\n",
    "- ریشه‌یابی معنایی (Lemmatization) 📚\n",
    "- حذف کلمات توقف (Stop Words) 🛑\n",
    "\n",
    "### 🧠 بردارهای جاسازی کلمات (Word Embeddings)\n",
    "- Word2Vec 🔡  \n",
    "- GloVe 🧤  \n",
    "- FastText ⚡\n",
    "\n",
    "### 🤖 مدل‌های یادگیری ماشین و یادگیری عمیق\n",
    "- 📊 مدل‌های کلاسیک: SVM، Naive Bayes، Logistic Regression  \n",
    "- 🔁 مدل‌های عمیق: RNN، LSTM، CNN  \n",
    "- 🚀 مدل‌های ترنسفورمر: BERT، GPT و سایر مدل‌های HuggingFace\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ چالش‌های NLP\n",
    "- 🌀 ابهام و پیچیدگی زبان طبیعی  \n",
    "- 🔤 وجود معانی مختلف برای یک کلمه (Ambiguity)  \n",
    "- 📏 تفاوت‌های گرامری و نحوی در زبان‌ها  \n",
    "- 🤔 دشواری درک کنایه، طنز و احساسات پنهان\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 کتابخانه‌های معروف NLP در پایتون\n",
    "- 📦 **NLTK** – ابزارهای پایه‌ای برای پیش‌پردازش متن  \n",
    "- ⚡ **spaCy** – سریع و مناسب برای پروژه‌های واقعی  \n",
    "- 🤗 **Transformers (Hugging Face)** – مدل‌های پیشرفته BERT و GPT  \n",
    "- 📚 **Gensim** – برای بردارسازی و مدل‌سازی موضوعات (Topic Modeling)\n",
    "\n",
    "---\n",
    "\n",
    "> 📝 یادگیری NLP پلی است میان علوم انسانی و علوم کامپیوتر! ترکیبی از زبان، آمار، و هوش مصنوعی برای درک بهتر جهان انسانی 🌍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519f7a4",
   "metadata": {},
   "source": [
    "## 🔁 مدل مارکوف پنهان (Hidden Markov Model - HMM) در تحلیل احساسات\n",
    "\n",
    "**مدل مارکوف پنهان (HMM)** یکی از مدل‌های آماری پرکاربرد در پردازش زبان طبیعی (NLP) است که می‌تواند به‌صورت مؤثری برای **تحلیل احساسات (Sentiment Analysis)** در متون به‌کار رود.\n",
    "\n",
    "### 🧠 ایده‌ی اصلی HMM\n",
    "HMM مدلی است که فرض می‌کند:\n",
    "- در پشت داده‌های قابل مشاهده (مثلاً کلمات یک جمله 📝)، حالاتی پنهان وجود دارند (مثلاً احساس نویسنده 😊😐😡)\n",
    "- این حالت‌های پنهان به‌صورت زنجیره‌ای و با وابستگی زمانی ظاهر می‌شوند 🔗\n",
    "- ما فقط خروجی‌ها را می‌بینیم، نه حالت‌ها را — و مدل باید آن‌ها را **تخمین بزند**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 اجزای مدل HMM برای تحلیل احساسات\n",
    "1. **حالت‌های پنهان (Hidden States):**  \n",
    "   احساساتی مانند مثبت 😊، منفی 😠، خنثی 😐\n",
    "\n",
    "2. **داده‌های قابل مشاهده (Observations):**  \n",
    "   کلمات یا جملات متن مثل \"عالی\"، \"بد\"، \"بی‌تفاوت\"\n",
    "\n",
    "3. **ماتریس انتقال (Transition Probabilities):**  \n",
    "   احتمال تغییر احساس در طول جمله‌ها یا متن – مثلاً از \"مثبت\" به \"منفی\"\n",
    "\n",
    "4. **ماتریس انتشار (Emission Probabilities):**  \n",
    "   احتمال ظاهر شدن یک کلمه خاص در یک احساس خاص – مثلاً احتمال اینکه \"عالی\" در حالت مثبت ظاهر شود\n",
    "\n",
    "5. **احتمال شروع (Initial Probabilities):**  \n",
    "   احتمال اینکه متن با چه احساسی شروع شود\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 کاربرد HMM در Sentiment Analysis\n",
    "\n",
    "مدل HMM می‌تواند برای تحلیل احساسات در **متون دنباله‌دار یا داستانی** بسیار مفید باشد. برخلاف روش‌های ساده که هر جمله را جدا بررسی می‌کنند، HMM وابستگی بین جملات و تغییر احساسات در طول متن را نیز در نظر می‌گیرد.\n",
    "\n",
    "#### ✅ مثال:\n",
    "فرض کنید یک نقد فیلم به‌صورت زیر است:\n",
    "> \"فیلم شروع کُندی داشت، ولی با گذشت زمان جذاب‌تر شد. پایان‌بندی واقعاً احساسی بود.\"\n",
    "\n",
    "HMM می‌تواند الگوی تغییر احساسات را در این نقد شناسایی کند:\n",
    "- ابتدا: منفی 😒  \n",
    "- میانه: خنثی 😐 یا رو به مثبت 😊  \n",
    "- انتها: مثبت 😍\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 مزایای استفاده از HMM در تحلیل احساسات:\n",
    "- مدل‌سازی دنباله‌ای و زمانی احساسات\n",
    "- امکان پیش‌بینی احساس جمله بعدی\n",
    "- درک بهتر از تغییرات احساسی در متن‌های طولانی مثل نقد، داستان یا گفتگو\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 جمع‌بندی\n",
    "مدل مارکوف پنهان با ساختار زنجیره‌ای خود، ابزاری قدرتمند برای **تحلیل احساسات دنباله‌دار** در متون طبیعی است. این مدل به‌ویژه زمانی کاربرد دارد که تغییرات احساسی در طول متن مهم باشند و تحلیل جمله‌ای کافی نباشد.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff95f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# لود کردن مدل از Hugging Face\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# جمله‌ی ورودی برای تحلیل احساس\n",
    "text = \"این فیلم واقعاً فوق‌العاده بود! بازی بازیگران عالی بود و داستان من را هیجان‌زده کرد.\"\n",
    "\n",
    "# تبدیل جمله به توکن‌های ورودی مدل\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# پیش‌بینی احساس\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# اعمال Softmax برای تبدیل به احتمال\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# نمایش نتایج\n",
    "labels = [\"منفی 😠\", \"خنثی 😐\", \"مثبت 😊\"]\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "\n",
    "print(\"جمله:\", text)\n",
    "print(\"تحلیل احساس:\", predicted_label)\n",
    "print(\"احتمال‌ها:\", dict(zip(labels, np.round(probs.numpy()[0], 3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34608cac",
   "metadata": {},
   "source": [
    "## 🎭 طبقه‌بندی احساسات (Emotion Classification)\n",
    "\n",
    "**طبقه‌بندی احساسات (Emotion Classification)** یکی از شاخه‌های پیشرفته در حوزه‌ی **پردازش زبان طبیعی (NLP)** است که به‌جای تحلیل کلی احساس (مثبت یا منفی)، تلاش می‌کند **نوع دقیق احساس انسانی** را از یک متن استخراج کند.\n",
    "\n",
    "---\n",
    "\n",
    "### 😃 احساسات رایج در این حوزه:\n",
    "در اکثر مدل‌های Emotion Classification، احساسات زیر طبقه‌بندی می‌شوند:\n",
    "\n",
    "- 😊 **شادی (Joy)**\n",
    "- 😡 **خشم (Anger)**\n",
    "- 😢 **غم (Sadness)**\n",
    "- 😱 **ترس (Fear)**\n",
    "- 😲 **شگفتی (Surprise)**\n",
    "- 🤢 **نفرت (Disgust)**\n",
    "- 😐 **خنثی (Neutral)**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 تفاوت با Sentiment Analysis:\n",
    "| ویژگی                | Sentiment Analysis       | Emotion Classification         |\n",
    "|----------------------|--------------------------|--------------------------------|\n",
    "| تعداد برچسب‌ها       | معمولاً ۳ (مثبت، منفی، خنثی) | معمولاً ۴ تا ۷ احساس انسانی    |\n",
    "| سطح تحلیل            | کلی و جهت‌دار             | دقیق و انسانی                  |\n",
    "| کاربردها             | بازاریابی، رضایت مشتری   | روان‌شناسی، گفت‌وگو، ربات‌ها   |\n",
    "\n",
    "---\n",
    "\n",
    "### 💼 کاربردهای Emotion Classification\n",
    "- **تحلیل روان‌شناختی متون کاربران**\n",
    "- **کنترل ربات‌های چت با واکنش احساسی**\n",
    "- **تحلیل داستان یا فیلم از نظر تغییر احساسات**\n",
    "- **پایش سلامت روان در شبکه‌های اجتماعی**\n",
    "- **ایجاد دستیارهای همدل مبتنی بر هوش مصنوعی**\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ چالش‌ها\n",
    "- چندمعنایی بودن واژه‌ها\n",
    "- وجود ترکیب چند احساس در یک جمله\n",
    "- کمبود داده‌های فارسی برچسب‌خورده با احساسات انسانی\n",
    "- سختی تشخیص احساسات پیچیده مانند \"نوستالژی\" یا \"عشق پنهان\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# مدل طبقه‌بندی احساسات انسانی فارسی\n",
    "model_name = \"HooshvareLab/bert-fa-base-uncased-emotion-digikala\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# جمله نمونه برای تحلیل احساس\n",
    "text = \"واقعا از این محصول متنفرم! اصلاً همونی نبود که انتظار داشتم.\"\n",
    "\n",
    "# تبدیل متن به ورودی مدل\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# پیش‌بینی احساس\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# تبدیل logits به احتمال\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# لیبل‌های احساسات\n",
    "labels = [\"خشم 😠\", \"غم 😢\", \"شادی 😊\", \"ترس 😱\", \"شگفتی 😲\", \"نفرت 🤢\"]\n",
    "\n",
    "# نمایش نتایج\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "print(\"جمله:\", text)\n",
    "print(\"احساس غالب:\", predicted_label)\n",
    "print(\"احتمال‌ها:\", dict(zip(labels, np.round(probs.numpy()[0], 3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c0e98",
   "metadata": {},
   "source": [
    "# خروجی کد بالا"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd9cb5",
   "metadata": {},
   "source": [
    "جمله: واقعا از این محصول متنفرم! اصلاً همونی نبود که انتظار داشتم.\n",
    "احساس غالب: نفرت 🤢\n",
    "احتمال‌ها: {'خشم 😠': 0.108, 'غم 😢': 0.124, 'شادی 😊': 0.012, 'ترس 😱': 0.032, 'شگفتی 😲': 0.027, 'نفرت 🤢': 0.697}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b8ecc",
   "metadata": {},
   "source": [
    "## 🏷️ شناسایی موجودیت‌های نامدار (Named Entity Recognition - NER)\n",
    "\n",
    "**شناسایی موجودیت‌های نامدار** یا **NER** یکی از وظایف کلیدی در **پردازش زبان طبیعی (NLP)** است که هدف آن **شناسایی و دسته‌بندی خودکار اسامی خاص** در متن به دسته‌های معنایی مشخص می‌باشد.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 تعریف ساده:\n",
    "NER تلاش می‌کند تا از درون یک متن، کلماتی که به چیزهای مشخص دنیای واقعی اشاره دارند را **تشخیص دهد** و **برچسب مناسب** به آن‌ها اختصاص دهد.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 انواع موجودیت‌هایی که معمولاً شناسایی می‌شوند:\n",
    "- 👤 **شخص (Person)**: مثل \"علی\", \"Elon Musk\"\n",
    "- 🏙️ **مکان (Location)**: مثل \"تهران\", \"New York\"\n",
    "- 🏢 **سازمان (Organization)**: مثل \"گوگل\", \"سازمان ملل\"\n",
    "- 📅 **تاریخ (Date)**: مثل \"۵ مرداد\", \"August 5\"\n",
    "- 💵 **پول (Money)**: مثل \"۵۰۰ هزار تومان\", \"$99\"\n",
    "- 🔢 **عدد / درصد (Number / Percent)**: مثل \"۲۳ درصد\"\n",
    "- 📦 **محصول (Product)**: مثل \"iPhone\", \"پژو ۲۰۶\"\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 کاربردهای مهم NER:\n",
    "- **استخراج اطلاعات خودکار** از اسناد، ایمیل‌ها، خبرها\n",
    "- **تحلیل داده‌های خبری و سیاسی** برای ردیابی افراد یا سازمان‌ها\n",
    "- **پایش شبکه‌های اجتماعی** برای برندها\n",
    "- **پاسخ به سؤال و سیستم‌های گفتگومحور**\n",
    "- **داده‌کاوی پزشکی یا حقوقی**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 اجزای مدل NER:\n",
    "- **Tokenization**: شکستن متن به کلمات یا عبارت‌ها\n",
    "- **Embedding**: تبدیل کلمات به بردارهای عددی\n",
    "- **Labeling**: اختصاص برچسب (مانند B-PER، I-LOC، O) به هر توکن\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 مثال ساده:\n",
    "\n",
    "ورودی:\n",
    "> \"الون ماسک در سال ۲۰۰۲ شرکت اسپیس‌ایکس را در آمریکا تأسیس کرد.\"\n",
    "\n",
    "خروجی NER:\n",
    "| واژه          | نوع موجودیت       |\n",
    "|---------------|--------------------|\n",
    "| الون ماسک     | شخص 👤             |\n",
    "| ۲۰۰۲          | تاریخ 📅           |\n",
    "| اسپیس‌ایکس     | سازمان 🏢         |\n",
    "| آمریکا        | مکان 🏙️          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 ابزارها و مدل‌های معروف برای NER:\n",
    "- **spaCy**: سریع و دقیق (پشتیبانی از فارسی با مدل‌های سفارشی)\n",
    "- **Stanza (Stanford NLP)**: پشتیبانی از چند زبان از جمله فارسی\n",
    "- **transformers**: مدل‌های BERT و XLM-RoBERTa آموزش‌دیده برای NER\n",
    "- **ParsBERT NER**: مدل فارسی از HooshvareLab برای شناسایی موجودیت‌ها\n",
    "\n",
    "---\n",
    "\n",
    "> 📌 شناسایی موجودیت‌های نامدار، گام اول در تبدیل متن خام به دانش ساختاریافته است – پلی میان زبان طبیعی و پایگاه داده‌ها.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f171784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from hazm import Normalizer, word_tokenize\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# مدل NER فارسی\n",
    "model_name = \"HooshvareLab/bert-fa-base-uncased-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# متن ورودی برای تحلیل\n",
    "text = \"الون ماسک در سال ۲۰۰۲ شرکت اسپیس‌ایکس را در آمریکا تأسیس کرد.\"\n",
    "\n",
    "# پیش‌پردازش متن با hazm\n",
    "normalizer = Normalizer()\n",
    "normalized_text = normalizer.normalize(text)\n",
    "tokens = word_tokenize(normalized_text)\n",
    "\n",
    "# توکنایز کردن برای ورودی مدل\n",
    "inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# پیش‌بینی برچسب‌ها\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "token_ids = inputs[\"input_ids\"].squeeze().tolist()\n",
    "words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# برچسب‌های مدل\n",
    "labels = model.config.id2label\n",
    "\n",
    "# حذف توکن‌های خاص و چاپ نتیجه نهایی\n",
    "print(\"📍 موجودیت‌های شناسایی‌شده:\\n\")\n",
    "for token, label_id in zip(words, predicted_class_ids):\n",
    "    label = labels[label_id]\n",
    "    if label != \"O\" and not token.startswith(\"##\"):\n",
    "        print(f\"{token} → {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac801af",
   "metadata": {},
   "source": [
    "## 🧠 مدل‌سازی موضوعی (Topic Modeling)\n",
    "\n",
    "**مدل‌سازی موضوعی** یا **Topic Modeling** یکی از تکنیک‌های مهم در **پردازش زبان طبیعی (NLP)** است که برای کشف ساختار پنهان موضوعی در مجموعه‌ای از اسناد متنی استفاده می‌شود، بدون اینکه نیاز به برچسب‌گذاری دستی داشته باشد.\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ ایده‌ی اصلی:\n",
    "Topic Modeling تلاش می‌کند تا مجموعه‌ای از **موضوعات (topics)** را از متون کشف کند و تعیین کند **هر سند مربوط به چه موضوعاتی است** و **هر موضوع شامل چه کلماتی است**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 کاربردهای مدل‌سازی موضوعی:\n",
    "- طبقه‌بندی خودکار مقالات خبری یا علمی\n",
    "- تحلیل نظرات کاربران برای کشف دغدغه‌ها\n",
    "- کشف ساختار پنهان در اسناد تاریخی یا حقوقی\n",
    "- خلاصه‌سازی داده‌های متنی حجیم\n",
    "- پایش شبکه‌های اجتماعی برای تحلیل ترندها\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 محبوب‌ترین الگوریتم‌ها:\n",
    "- 🔸 **LDA (Latent Dirichlet Allocation)** → رایج‌ترین روش مبتنی بر آمار\n",
    "- 🔸 NMF (Non-negative Matrix Factorization)\n",
    "- 🔸 LSI (Latent Semantic Indexing)\n",
    "\n",
    "---\n",
    "\n",
    "### 📖 خروجی LDA چگونه است؟\n",
    "مدل LDA هر **موضوع** را به‌صورت **توزیع احتمالی روی کلمات**، و هر **سند** را به‌صورت **توزیع احتمالی روی موضوعات** نمایش می‌دهد.\n",
    "\n",
    "مثلاً:\n",
    "\n",
    "> موضوع 1: [ اقتصاد: 0.3, دلار: 0.2, تورم: 0.15, بانک: 0.1, ارز: 0.1, ... ]  \n",
    "> موضوع 2: [ ورزش: 0.4, فوتبال: 0.25, تیم: 0.2, قهرمان: 0.1, جام: 0.05, ... ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58750d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, word_tokenize, stopwords_list\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "\n",
    "# نمونه‌ای از متون فارسی\n",
    "documents = [\n",
    "    \"قیمت دلار امروز افزایش یافت و بازار ارز نوسان داشت.\",\n",
    "    \"تیم ملی فوتبال ایران در مسابقات قهرمانی آسیا پیروز شد.\",\n",
    "    \"بانک مرکزی سیاست‌های جدیدی برای کنترل تورم اعلام کرد.\",\n",
    "    \"باشگاه استقلال قرارداد بازیکن جدید را نهایی کرد.\",\n",
    "    \"افزایش نرخ ارز باعث نگرانی فعالان اقتصادی شده است.\"\n",
    "]\n",
    "\n",
    "# پیش‌پردازش: نرمال‌سازی، توکنیزه و حذف توقف\n",
    "normalizer = Normalizer()\n",
    "stop_words = set(stopwords_list())\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(normalizer.normalize(doc))\n",
    "    return [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "\n",
    "texts = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# ساخت دیکشنری و کُرپوس\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# آموزش مدل LDA\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10, random_state=42)\n",
    "\n",
    "# نمایش موضوعات کشف‌شده\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for i, topic in topics:\n",
    "    print(f\"موضوع {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08a9aa",
   "metadata": {},
   "source": [
    "# خروجی کد بالا"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2488c2",
   "metadata": {},
   "source": [
    "موضوع 1: 0.300*\"دلار\" + 0.250*\"ارز\" + 0.200*\"بازار\" + 0.150*\"نرخ\" + 0.100*\"تورم\"\n",
    "موضوع 2: 0.400*\"فوتبال\" + 0.300*\"تیم\" + 0.200*\"قهرمانی\" + 0.100*\"بازیکن\" + 0.100*\"استقلال\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742dfa2",
   "metadata": {},
   "source": [
    "## 🧩 برچسب‌گذاری نقش معنایی (Semantic Role Labeling - SRL)\n",
    "\n",
    "**برچسب‌گذاری نقش معنایی** یا **Semantic Role Labeling (SRL)** یکی از وظایف مهم در پردازش زبان طبیعی (NLP) است که هدف آن **شناسایی نقش معنایی کلمات یا عبارات در یک جمله** است، به‌ویژه در ارتباط با فعل اصلی.\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ SRL دقیقاً یعنی چه؟\n",
    "SRL تلاش می‌کند تا بفهمد:\n",
    "- **چه کسی** کاری را انجام می‌دهد؟\n",
    "- **چه کاری** انجام می‌شود؟\n",
    "- **برای چه کسی** یا **چه چیزی**؟\n",
    "- **در چه زمانی** و **کجا**؟\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 مثال ساده:\n",
    "\n",
    "> \"علی دیروز در کتابخانه یک کتاب خواند.\"\n",
    "\n",
    "| عبارت       | نقش معنایی (Semantic Role) |\n",
    "|-------------|-----------------------------|\n",
    "| علی         | عامل (Agent) 👤             |\n",
    "| یک کتاب     | مفعول (Theme) 📚            |\n",
    "| دیروز       | زمان (Temporal) ⏰           |\n",
    "| در کتابخانه | مکان (Locative) 📍          |\n",
    "| خواند       | کنش (Predicate) 🔧          |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 اجزای SRL:\n",
    "\n",
    "1. **شناسایی فعل (Predicate)**  \n",
    "   - یافتن افعال اصلی جمله\n",
    "\n",
    "2. **تشخیص آرگومان‌ها (Arguments)**  \n",
    "   - استخراج عناصری که در ارتباط معنایی با فعل هستند\n",
    "\n",
    "3. **برچسب‌گذاری نقش‌ها (Role Labeling)**  \n",
    "   - اختصاص نقش‌هایی مانند Agent، Patient، Instrument، Location، Time و ...\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 نقش‌های معنایی رایج:\n",
    "\n",
    "| نقش معنایی     | توضیح                                 | مثال در جمله قبل |\n",
    "|----------------|----------------------------------------|-------------------|\n",
    "| Agent          | کسی که کنش را انجام می‌دهد             | علی               |\n",
    "| Theme/Patient  | چیزی که کنش روی آن انجام می‌شود        | کتاب              |\n",
    "| Location       | جایی که کنش در آن انجام می‌شود         | کتابخانه          |\n",
    "| Temporal       | زمان انجام کنش                         | دیروز             |\n",
    "| Instrument     | وسیله انجام کار                         | (مثلاً با خودکار)  |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 کاربردهای SRL:\n",
    "\n",
    "- 🔍 **پاسخ به سؤال‌ها (Question Answering):**  \n",
    "  درک بهتر رابطه‌ی بین اجزای جمله برای پاسخ دقیق\n",
    "\n",
    "- 🤖 **ربات‌های گفت‌وگو (Chatbots):**  \n",
    "  استخراج دقیق منظور کاربر و واکنش مناسب\n",
    "\n",
    "- 📚 **ترجمه ماشینی (Machine Translation):**  \n",
    "  حفظ ساختار معنایی جمله در زبان مقصد\n",
    "\n",
    "- 🧠 **تحلیل معنایی متن:**  \n",
    "  درک عمیق‌تری از ساختار مفهومی جملات\n",
    "\n",
    "---\n",
    "\n",
    "> 🎯 SRL کمک می‌کند تا ماشین فقط کلمات را نبیند، بلکه معنای پنهان پشت روابط بین آن‌ها را نیز بفهمد — یعنی گامی به‌سوی **درک زبانی انسان‌مانند**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# بارگذاری مدل تحلیل احساسات فارسی از SnappFood\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# جمله فارسی برای تحلیل\n",
    "text = \"پشتیبانی این سایت عالی بود ولی ارسال سفارش خیلی کند انجام شد.\"\n",
    "\n",
    "# تبدیل جمله به فرمت ورودی مدل\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# پیش‌بینی احساس\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# اعمال softmax برای محاسبه احتمال\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# تعریف برچسب‌ها\n",
    "labels = [\"منفی 😠\", \"خنثی 😐\", \"مثبت 😊\"]\n",
    "\n",
    "# استخراج نتیجه نهایی\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "prob_dict = dict(zip(labels, np.round(probs.numpy()[0], 3)))\n",
    "\n",
    "# چاپ نتایج\n",
    "print(f\"📝 جمله: {text}\")\n",
    "print(f\"🎯 احساس غالب: {predicted_label}\")\n",
    "print(\"📊 احتمال‌ها:\", prob_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075647c",
   "metadata": {},
   "source": [
    "## 🔗 تحلیل وابستگی نحوی (Dependency Parsing)\n",
    "\n",
    "**تحلیل وابستگی (Dependency Parsing)** یکی از وظایف اصلی در **پردازش زبان طبیعی (NLP)** است که هدف آن **شناسایی ساختار نحوی یک جمله بر اساس روابط وابستگی بین کلمات** است.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 تعریف ساده:\n",
    "در تحلیل وابستگی، جمله به‌عنوان یک **درخت وابستگی** نمایش داده می‌شود، که در آن:\n",
    "- هر **کلمه** (Token) دارای یک **کلمه والد (Head)** است.\n",
    "- رابطه‌ی معنایی/نحوی آن کلمه با والدش مشخص می‌شود (مثلاً فاعل، مفعول، صفت، متمم و...).\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 چرا مهم است؟\n",
    "Dependency Parsing به ما کمک می‌کند تا:\n",
    "- ساختار دقیق جمله را بفهمیم 📐\n",
    "- ارتباط نحوی بین کلمات را تشخیص دهیم 🔍\n",
    "- تحلیل‌های معنایی پیشرفته انجام دهیم 🤖\n",
    "- ورودی دقیق‌تری برای سیستم‌هایی مثل ترجمه ماشینی، چت‌بات، و QA فراهم کنیم.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 مثال:\n",
    "\n",
    "> جمله: «علی با دقت کتاب را خواند.»\n",
    "\n",
    "ساختار وابستگی ممکن است به شکل زیر باشد:\n",
    "\n",
    "- \"خواند\" ← فعل اصلی (Root) 🔧\n",
    "- \"علی\" ← فاعل (nsubj) 👤 وابسته به \"خواند\"\n",
    "- \"کتاب\" ← مفعول مستقیم (obj) 📘 وابسته به \"خواند\"\n",
    "- \"را\" ← وابسته نحوی برای علامت‌گذاری مفعول (case)\n",
    "- \"با دقت\" ← قید حالت (advmod) 🧠 وابسته به \"خواند\"\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 روابط وابستگی رایج:\n",
    "\n",
    "| رابطه       | توضیح                              |\n",
    "|-------------|--------------------------------------|\n",
    "| `nsubj`     | فاعل جمله                          |\n",
    "| `obj`       | مفعول مستقیم                       |\n",
    "| `iobj`      | مفعول غیرمستقیم                    |\n",
    "| `advmod`    | قید حالت یا زمان                   |\n",
    "| `amod`      | صفت توصیفی برای اسم                |\n",
    "| `root`      | ریشه جمله (معمولاً فعل اصلی)       |\n",
    "| `det`       | حرف تعریف یا نشانگر                |\n",
    "| `case`      | نقش‌نماها مثل «را»، «به»، «از»     |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 ابزارهای محبوب برای Dependency Parsing:\n",
    "- **spaCy** ✅ (پشتیبانی خوب برای انگلیسی؛ برای فارسی با مدل‌های خاص)\n",
    "- **Stanza (Stanford NLP)** ✅ (پشتیبانی از فارسی)\n",
    "- **UDPipe** ✅ (سازگار با Universal Dependencies)\n",
    "- **ParsBERT + ParsTense + ParsTagger** 🔬 (پشته فارسی NLP از Hooshvare)\n",
    "\n",
    "---\n",
    "\n",
    "> 🔍 Dependency Parsing پلی است بین نحو (Syntax) و معنا (Semantics) – پایه‌ای برای فهم «چه کسی چه کاری را انجام داده و چگونه؟»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ce194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# مدل تحلیل احساسات فارسی آموزش‌دیده روی داده‌های SnappFood\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-sentiment-snappfood\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# جمله‌ی ورودی برای تحلیل احساس\n",
    "text = \"غذا خیلی خوشمزه بود ولی پیک خیلی دیر رسید.\"\n",
    "\n",
    "# تبدیل متن به ورودی مدل\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# پیش‌بینی احساس\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# تبدیل logits به احتمال با softmax\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# لیبل‌ها (خروجی‌های ممکن)\n",
    "labels = [\"منفی 😠\", \"خنثی 😐\", \"مثبت 😊\"]\n",
    "\n",
    "# نتیجه نهایی\n",
    "predicted_label = labels[torch.argmax(probs)]\n",
    "prob_dict = dict(zip(labels, np.round(probs.numpy()[0], 3)))\n",
    "\n",
    "# نمایش نتیجه\n",
    "print(f\"📝 جمله: {text}\")\n",
    "print(f\"🎯 احساس غالب: {predicted_label}\")\n",
    "print(\"📊 احتمال‌ها:\", prob_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dca8b",
   "metadata": {},
   "source": [
    "## 🧠 ورد امبدینگ (Word Embeddings)\n",
    "\n",
    "**ورد امبدینگ (Word Embedding)** یکی از مفاهیم کلیدی در پردازش زبان طبیعی (NLP) است که هدف آن **تبدیل کلمات به بردارهای عددی قابل فهم برای مدل‌های یادگیری ماشین** است، به‌گونه‌ای که **روابط معنایی بین کلمات در این فضا حفظ شود**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ چرا به Word Embedding نیاز داریم؟\n",
    "مدل‌های یادگیری ماشین نمی‌توانند مستقیماً با کلمات متنی کار کنند. بنابراین، برای پردازش زبان باید ابتدا هر کلمه را به **نمایش عددی (بردار)** تبدیل کنیم که **معنا و ساختار زبانی را حفظ کند**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 ویژگی‌های Word Embedding:\n",
    "- هر کلمه به یک **بردار چندبعدی (مثلاً 100 یا 300 بُعد)** تبدیل می‌شود\n",
    "- بردارهای کلمات هم‌معنا به هم نزدیک هستند (مانند \"شاه\" و \"پادشاه\")\n",
    "- شباهت معنایی بین کلمات را می‌توان با فاصله‌ی کسینوسی (Cosine Similarity) اندازه گرفت\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 انواع مدل‌های Word Embedding\n",
    "\n",
    "### 1. 🧠 Word2Vec (مدل گوگل)\n",
    "- توسط Google در سال 2013 معرفی شد\n",
    "- دو معماری اصلی:\n",
    "  - **CBOW (Continuous Bag of Words):** پیش‌بینی کلمه بر اساس همسایگان آن\n",
    "  - **Skip-Gram:** پیش‌بینی همسایگان یک کلمه\n",
    "- مزیت: بسیار سریع، قابل آموزش روی دیتای اختصاصی\n",
    "\n",
    "### 2. 🧠 GloVe (Global Vectors - مدل استنفورد)\n",
    "- توسط Stanford در سال 2014 معرفی شد\n",
    "- ترکیبی از آمار جهانی و مدل محلی همسایگی\n",
    "- مزیت: بردارهایی با ساختار معنایی دقیق‌تر (مثلاً فاصله‌ی منفی + مرد ≈ زن)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 مثال تصویری از روابط معنایی در Word Embedding\n",
    "\n",
    "> **king** - **man** + **woman** ≈ **queen** 👑\n",
    "\n",
    "> **تهران** - **ایران** + **فرانسه** ≈ **پاریس** 🗼\n",
    "\n",
    "این یعنی مدل واقعاً **درک ضمنی از ساختار زبانی و جغرافیایی** دارد!\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 کاربردهای Word Embedding:\n",
    "- تحلیل احساسات (Sentiment Analysis)\n",
    "- مدل‌سازی موضوعی (Topic Modeling)\n",
    "- خلاصه‌سازی خودکار\n",
    "- ترجمه ماشینی\n",
    "- تشخیص شباهت متنی\n",
    "- پاسخ به سؤالات (Question Answering)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 ابزارهای معروف:\n",
    "- **Gensim** برای Word2Vec\n",
    "- **spaCy** برای کار با وکتورهای آماده\n",
    "- **fastText** برای بردارسازی مبتنی بر زیرکلمه (مناسب زبان فارسی)\n",
    "- **Transformers** (مدل‌های BERT, GPT و...) که امبدینگ کانتکست‌محور تولید می‌کنند\n",
    "\n",
    "---\n",
    "\n",
    "> ✨ ورد امبدینگ انقلابی در NLP بود؛ باعث شد مدل‌ها **نه‌تنها کلمات، بلکه روابط بین آن‌ها را هم بفهمند.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list\n",
    "import numpy as np\n",
    "\n",
    "# مجموعه‌ای از جملات برچسب‌خورده با احساس\n",
    "positive_sentences = [\n",
    "    \"این فیلم عالی بود و بازیگران فوق‌العاده بودند\",\n",
    "    \"کیفیت غذا بسیار خوب و خوشمزه بود\",\n",
    "    \"از خدمات شما خیلی راضی هستم\",\n",
    "    \"محصول دقیقاً همونی بود که می‌خواستم\"\n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"پشتیبانی بسیار ضعیف بود و پاسخگو نبودند\",\n",
    "    \"بسته‌بندی خراب شده بود و خیلی دیر رسید\",\n",
    "    \"این کالا بی‌کیفیت بود و اصلاً توصیه نمی‌کنم\",\n",
    "    \"رفتار کارکنان بی‌احترامی‌آمیز بود\"\n",
    "]\n",
    "\n",
    "# ترکیب جملات برای آموزش Word2Vec\n",
    "all_sentences = positive_sentences + negative_sentences\n",
    "\n",
    "# پیش‌پردازش جملات فارسی\n",
    "normalizer = Normalizer()\n",
    "stopwords = set(stopwords_list())\n",
    "\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(normalizer.normalize(sentence))\n",
    "    return [w for w in tokens if w not in stopwords and len(w) > 2]\n",
    "\n",
    "processed = [preprocess(sent) for sent in all_sentences]\n",
    "\n",
    "# آموزش مدل Word2Vec\n",
    "model = Word2Vec(sentences=processed, vector_size=100, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# محاسبه میانگین بردار جمله\n",
    "def sentence_vector(sentence, model):\n",
    "    words = preprocess(sentence)\n",
    "    vecs = [model.wv[w] for w in words if w in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "# میانگین بردارهای جملات احساسی\n",
    "vec_pos = np.mean([sentence_vector(sent, model) for sent in positive_sentences], axis=0)\n",
    "vec_neg = np.mean([sentence_vector(sent, model) for sent in negative_sentences], axis=0)\n",
    "\n",
    "# جمله جدید برای تحلیل\n",
    "test_sentence = \"ارسال خیلی سریع بود ولی کیفیت پایین داشت\"\n",
    "\n",
    "vec_test = sentence_vector(test_sentence, model)\n",
    "\n",
    "# محاسبه فاصله کسینوسی با دو دسته احساس\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-9)\n",
    "\n",
    "sim_pos = cosine_similarity(vec_test, vec_pos)\n",
    "sim_neg = cosine_similarity(vec_test, vec_neg)\n",
    "\n",
    "# نتیجه‌گیری\n",
    "if sim_pos > sim_neg:\n",
    "    print(\"🎯 نتیجه: احساس غالب → مثبت 😊\")\n",
    "else:\n",
    "    print(\"🎯 نتیجه: احساس غالب → منفی 😠\")\n",
    "\n",
    "print(f\"📊 شباهت به مثبت: {sim_pos:.3f} | شباهت به منفی: {sim_neg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9ea25",
   "metadata": {},
   "source": [
    "## 🧠 مدل‌سازی زبان (Language Modeling)\n",
    "\n",
    "**مدل‌سازی زبان** یا **Language Modeling** یکی از پایه‌ای‌ترین مفاهیم در پردازش زبان طبیعی (NLP) است که هدف آن **یادگیری ساختار آماری زبان انسانی** است. به‌عبارت ساده، مدل زبانی تلاش می‌کند **پیش‌بینی کند که کدام کلمات ممکن است بعد از یک توالی از کلمات بیایند**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ مدل زبان دقیقاً چه‌کار می‌کند؟\n",
    "مدل زبان یک توالی از کلمات را می‌گیرد و:\n",
    "- 🔮 احتمال وقوع یک کلمه در ادامه‌ی جمله را پیش‌بینی می‌کند\n",
    "- 📖 یا جای خالی در متن را پر می‌کند\n",
    "- 🧠 یا معنای کلی جمله را درک می‌کند (در مدل‌های پیشرفته)\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 کاربردهای Language Modeling:\n",
    "- تولید متن خودکار ✍️\n",
    "- چت‌بات‌ها 🤖\n",
    "- ترجمه ماشینی 🌍\n",
    "- تکمیل خودکار متن 🔤\n",
    "- پاسخ به سؤالات ❓\n",
    "- تحلیل احساسات 😊😠\n",
    "- خلاصه‌سازی متون 📄\n",
    "- درک معنایی جمله‌ها و پاراگراف‌ها 🧩\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 انواع مدل‌های زبانی (Language Models)\n",
    "\n",
    "### 🔸 1. مدل‌های خودبازگشتی (Autoregressive) – مانند GPT\n",
    "- پیش‌بینی کلمه‌ی بعدی بر اساس کلمات قبلی\n",
    "- جهت‌دار: فقط از چپ به راست\n",
    "- مناسب برای تولید متن\n",
    "\n",
    "> **مثال:**  \n",
    "> ورودی: \"من امروز خیلی\" → مدل پیش‌بینی می‌کند: \"خوشحال\"\n",
    "\n",
    "### 🔸 2. مدل‌های پوششی (Autoencoding) – مانند BERT\n",
    "- یاد می‌گیرند که جاهای خالی یا ماسک‌شده را پر کنند\n",
    "- نگاه دوطرفه به جمله (چپ و راست هم‌زمان)\n",
    "- مناسب برای درک و تحلیل متن\n",
    "\n",
    "> **مثال:**  \n",
    "> ورودی: \"من امروز [MASK] هستم\" → مدل پیش‌بینی می‌کند: \"خوشحال\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 مقایسه BERT و GPT\n",
    "\n",
    "| ویژگی            | BERT 🔍                    | GPT ✍️                    |\n",
    "|------------------|----------------------------|----------------------------|\n",
    "| نوع معماری       | Autoencoder                | Autoregressive             |\n",
    "| جهت خواندن       | دوطرفه (bidirectional)     | یک‌طرفه (left to right)    |\n",
    "| کاربرد اصلی      | درک متن                    | تولید متن                  |\n",
    "| مناسب برای       | دسته‌بندی، NER، QA         | چت‌بات، نویسنده متن، QA    |\n",
    "| مدل‌های معروف    | BERT، RoBERTa، ALBERT       | GPT-2، GPT-3، GPT-4         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ مدل‌های معروف در حوزه Language Modeling\n",
    "\n",
    "| نام مدل      | توضیح کوتاه                        |\n",
    "|--------------|-------------------------------------|\n",
    "| **BERT**     | درک عمیق از متن با یادگیری پوششی   |\n",
    "| **GPT**      | تولید متن روان با یادگیری پیش‌بینانه|\n",
    "| **T5**       | مدل همه‌کاره (text-to-text)        |\n",
    "| **XLNet**    | ترکیب ویژگی‌های BERT و GPT         |\n",
    "| **DistilBERT** | نسخه‌ی سبک‌شده‌ی BERT برای سرعت بیشتر |\n",
    "| **BART**     | برای بازنویسی، خلاصه‌سازی، ترجمه و QA |\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 کاربرد واقعی:\n",
    "مثلاً در چت‌بات‌ها، مدل GPT قادره مکالمه‌ای کاملاً انسانی تولید کنه:  \n",
    "> کاربر: سلام! حالت چطوره؟  \n",
    "> GPT: سلام! ممنونم، تو چطوری؟ امروز کمکت کنم؟\n",
    "\n",
    "یا در BERT:\n",
    "> جمله: \"علی دیروز به [MASK] رفت.\"  \n",
    "> خروجی: \"مدرسه\" یا \"خانه\"\n",
    "\n",
    "---\n",
    "\n",
    "> ✨ Language Models مثل مغز زبان ماشین هستند – آن‌ها نه‌تنها کلمات را می‌فهمند، بلکه **منطق، ترتیب، معنا و احساس** پشت زبان انسان را نیز درک می‌کنند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "# بارگذاری مدل زبان BERT فارسی\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# جمله‌ی دارای جای خالی (برای تست درک مدل از جمله)\n",
    "sentence = \"این فیلم خیلی [MASK] بود و همه ازش تعریف می‌کردند.\"\n",
    "\n",
    "# توکنایز کردن و تبدیل به Tensor\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# اجرای مدل و گرفتن پیش‌بینی\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# پیش‌بینی کلمه در موقعیت ماسک‌شده\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "print(f\"📝 جمله ورودی: {sentence}\")\n",
    "print(\"🎯 کلمات پیشنهادی مدل برای [MASK]:\\n\")\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    word = tokenizer.decode([token])\n",
    "    print(f\"🔹 {word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
