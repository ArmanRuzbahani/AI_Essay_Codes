{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77945b74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ุณูุฑ ุงุฏฺฏุฑ: ฺฉุดู ูุนูุงุฑโูุง ุงุฏฺฏุฑ ุนูู ๐๐ง\n",
    "\n",
    "**ุจุงุฏ ุจุง ูู ุณู ูุนูุงุฑ ุงุณุงุณ ุงุฏฺฏุฑ ุนูู ุฑุง ฺฉุดู ฺฉูู:**\n",
    "\n",
    "## 1. <span style=\"color: #4d90fe;\">CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู)</span> ๐ผ๏ธ\n",
    "- **ูุบุฒ ุจูุง ูุงุดู** ๐๏ธ\n",
    "- ุงุฒ ููุชุฑูุง ฺฉุงููููุดู ุจุฑุง ุชุดุฎุต ุงูฺฏููุง ูุญู ุงุณุชูุงุฏู ูโฺฉูุฏ\n",
    "- ุงุฏูโุขู ุจุฑุง ูพุฑุฏุงุฒุด ุชุตุงูุฑ ู ูุฏู\n",
    "- ูุซุงู: ุชุดุฎุต ุงุดุง ุฏุฑ ุนฺฉุณโูุง\n",
    "\n",
    "## 2. <span style=\"color: #34a853;\">RNN (ุดุจฺฉู ุนุตุจ ุจุงุฒฺฏุดุช)</span> ๐\n",
    "- **ุญุงูุธู ฺฉูุชุงูโูุฏุช ุจุฑุง ุฏุงุฏูโูุง ูุชูุงู** ๐\n",
    "- ูโุชูุงูุฏ ุงุทูุงุนุงุช ุฑุง ุงุฒ ูุฑุงุญู ูุจู ุจู ุฎุงุทุฑ ุจุณูพุงุฑุฏ\n",
    "- ููุงุณุจ ุจุฑุง ูุชูุ ฺฏูุชุงุฑ ู ุฏุงุฏูโูุง ุฒูุงู\n",
    "- ูุซุงู: ูพุดโุจู ฺฉููู ุจุนุฏ ุฏุฑ ุฌููู\n",
    "\n",
    "## 3. <span style=\"color: #ea4335;\">Transformer</span> โก\n",
    "- **ุงูููุงุจ ุฏุฑ ูพุฑุฏุงุฒุด ุฒุจุงู** ๐ฌ\n",
    "- ุงุฒ ูฺฉุงูุฒู ุชูุฌู (Attention) ุงุณุชูุงุฏู ูโฺฉูุฏ\n",
    "- ูโุชูุงูุฏ ุฑูุงุจุท ุจููุฏูุฏุช ุฑุง ุฏุฑ ุฏุงุฏูโูุง ุชุดุฎุต ุฏูุฏ\n",
    "- ูุซุงู: ุชุฑุฌูู ูุงุดูุ ูุฏูโูุง ฺฏูุชฺฏู\n",
    "\n",
    "๐ **ูฺฉุชู ุทูุง:**  \n",
    "ุงู ูุนูุงุฑโูุง ุงุบูุจ ุจุง ูู ุชุฑฺฉุจ ูโุดููุฏ ุชุง ุณุณุชูโูุง ููุดููุฏ ูุฏุฑุชููุฏ ุจุณุงุฒูุฏ!\n",
    "\n",
    "๐ฏ **ูุฏู ูุง ุฏุฑ ุงู ูุณุฑ:**  \n",
    "ุงุฏฺฏุฑ ุงุตูู ูุฑ ูุนูุงุฑ + ุฏุฑฺฉ ฺฉุงุฑุจุฑุฏูุง ุนูู + ูพุงุฏูโุณุงุฒ ุณุงุฏู\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664615",
   "metadata": {},
   "source": [
    "# ุงุฏฺฏุฑ ุนูู: ููุชูุฑ ูุญุฑฺฉ ููุด ูุตููุน ูุฏุฑู ๐๐ง\n",
    "\n",
    "**ุงุฏฺฏุฑ ุนูู**ุ ูพุดุฑูุชูโุชุฑู ุดุงุฎู ุงุฏฺฏุฑ ูุงุดู ุงุณุช ฺฉู ุจุง ุงููุงู ุงุฒ ุณุงุฎุชุงุฑ ูุบุฒ ุงูุณุงูุ ุงูููุงุจ ุฏุฑ ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุงุฌุงุฏ ฺฉุฑุฏู ุงุณุช. ุงู ููุงูุฑ ุจุง ูุนูุงุฑโูุง ููุดููุฏุงูู ุฎูุฏ ูุงุฏุฑ ุจู ุงุฏฺฏุฑ ุณูุณููโูุฑุงุชุจ ูฺฺฏโูุง ุงุฒ ุฏุงุฏูโูุง ุฎุงู ุงุณุช:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa30d",
   "metadata": {},
   "source": [
    "# ๐งฉ CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู)\n",
    "Convolutional Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## โ๏ธ ฺฺฏููู ฺฉุงุฑ ูโฺฉูุฏุ\n",
    "\n",
    "**ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู (CNN)** ฺฉ ูุฏู ุจุณุงุฑ ูุฏุฑุชููุฏ ุจุฑุง ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุชุตูุฑ ู ูุถุง ุงุณุช.\n",
    "\n",
    "- ูุณุชู ุงุตู CNNุ **ูุงูโูุง ฺฉุงููููุดู (Convolutional Layers)** ูุณุชูุฏ ฺฉู ุจุง ููุชุฑูุง ฺฉูฺฺฉ (Kernel) ุงุฒ ุฑู ุชุตูุฑ ุนุจูุฑ ูโฺฉููุฏ ู ูฺฺฏโูุง ููู ุฑุง ุดูุงุณุง ูโฺฉููุฏ (ูุซู ูุจูโูุงุ ุจุงูุชโูุง ู ุงุดุงุก).\n",
    "- ุงู ูฺฺฏโูุง ูุฑุญููโุจูโูุฑุญูู ุชุฑฺฉุจ ู ุงูุชุฒุงุน ูโุดููุฏ ุชุง ูุฏู ุจุชูุงูุฏ ุญุช ุงูฺฏููุง ูพฺุฏู ุฑุง ุดูุงุณุง ฺฉูุฏ.\n",
    "- **ูุงูโูุง Pooling** ุจุฑุง ฺฉุงูุด ุญุฌู ุฏุงุฏู ู ุฌููฺฏุฑ ุงุฒ ุงุฏฺฏุฑ ุจุดโุงุฒุญุฏ (Overfitting) ุจู ฺฉุงุฑ ูโุฑููุฏ.\n",
    "- ุฏุฑ ุงูุชูุงุ **ูุงูโูุง Fully Connected** ุชุตูู ููุง ุฑุง ุฏุฑุจุงุฑู ุฏุณุชูโุจูุฏ ุง ุชุดุฎุต ูโฺฏุฑูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ฺุฑุง CNNุ\n",
    "\n",
    "- **ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ูฺฺฏโูุง** ุจุฏูู ูุงุฒ ุจู ุงุณุชุฎุฑุงุฌ ุฏุณุช\n",
    "- **ููุงุณโูพุฐุฑ ู ุฏูุช ุจุงูุง** ุจุฑุง ุฏุงุฏูโูุง ุชุตูุฑ ู ูุถุง\n",
    "- **ููุงููุช ุจู ููุฒ** ู ุชุบุฑุงุช ุฌุฒุฆ ุฏุฑ ูุฑูุฏ (ูุงููุฏ ุฌุงุจุฌุง ุง ฺุฑุฎุด)\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ ฺฉุงุฑุจุฑุฏูุง ููู CNN\n",
    "\n",
    "- **ุดูุงุณุง ู ุฏุณุชูโุจูุฏ ุชุตุงูุฑ** (ูุงููุฏ ุชุดุฎุต ฺฏุฑุจู/ุณฺฏุ ฺูุฑู ู ...)\n",
    "- **ุชุดุฎุต ุงุดุงุก ุฏุฑ ุชุตุงูุฑ ู ูุฏููุง**\n",
    "- **ุชุญูู ุชุตุงูุฑ ูพุฒุดฺฉ** (ูุงููุฏ ุชุดุฎุต ุจูุงุฑ ุงุฒ ุฑุงุฏูููฺ ุง MRI)\n",
    "- **ูพุฑุฏุงุฒุด ุชุตูุฑ ู ุจูุง ูุงุดู** (Computer Vision)\n",
    "- **ุชุญูู ุฏุณุชโุฎุท ู ุงุนุฏุงุฏ ููุดุชูโุดุฏู**\n",
    "- **ุฎูุฏุฑููุง ุฎูุฏุฑุงูุ ุฑุจุงุชฺฉ ู ุณุณุชูโูุง ุงููุช**\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฌ ุฎูุงุตู ุฏุฑ ฺฉ ุฌููู:\n",
    "\n",
    "> **CNNูุง ุจุง ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ูฺฺฏโูุง ุชุตูุฑ ุงุฒ ุฏุงุฏู ุฎุงูุ ฺฉ ุงุฒ ูุฏุฑุชููุฏุชุฑู ุงูฺฏูุฑุชูโูุง ุจุฑุง ุชุญูู ุชุตุงูุฑ ู ุณฺฏูุงูโูุง ูุถุง ูุณุชูุฏ.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ฺฉุชุงุจุฎุงููโูุง ููุฑุฏ ูุงุฒ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ูุฑุญูู 1: ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ูุชู\n",
    "# ูุฏู: ุชุจุฏู ูุชู ุจู ุจุฑุฏุงุฑูุง ุนุฏุฏ ู ุขูุงุฏูโุณุงุฒ ุจุฑุง ูุฑูุฏ ุดุจฺฉู\n",
    "def prepare_text_data(texts, labels, max_words=10000, max_len=100):\n",
    "    # ุชูฺฉูโุณุงุฒ: ุชุจุฏู ฺฉููุงุช ุจู ุชูฺฉูโูุง ุนุฏุฏ\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # ุชุจุฏู ูุชู ุจู ุฏูุจุงููโูุง ุนุฏุฏ\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # ูพุฏ ฺฉุฑุฏู ุฏูุจุงููโูุง ุจุฑุง ฺฉุณุงูโุณุงุฒ ุทูู\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    # ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ุขุฑุงู\n",
    "    labels = np.array(labels)\n",
    "    return data, labels, tokenizer\n",
    "\n",
    "# ูุฑุญูู 2: ุชุนุฑู ูุนูุงุฑ ุดุจฺฉู ฺฉุงููููุดู\n",
    "# ูุฏู: ุงุฌุงุฏ ฺฉ ูุฏู CNN ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุชู ู ุทุจููโุจูุฏ ุงุญุณุงุณุงุช\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = models.Sequential([\n",
    "        # ูุงู ุชุนุจู: ุชุจุฏู ุชูฺฉูโูุง ุจู ุจุฑุฏุงุฑูุง ูุชุฑุงฺฉู\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        # ูุงู ฺฉุงููููุดู ุงูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุญู ุงุฒ ูุชู\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        # ูุงู ูพูููฺฏ: ฺฉุงูุด ุงุจุนุงุฏ ู ุญูุธ ูฺฺฏโูุง ููู\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # ูุงู ฺฉุงููููุดู ุฏูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูพฺุฏูโุชุฑ\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # ูุณุทุญโุณุงุฒ: ุขูุงุฏูโุณุงุฒ ุจุฑุง ูุงูโูุง ูุชุฑุงฺฉู\n",
    "        layers.Flatten(),\n",
    "        # ูุงู ฺฉุงููุงู ูุชุตู: ุชุฑฺฉุจ ูฺฺฏโูุง\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # ูุงู ุฎุฑูุฌ: ุทุจููโุจูุฏ ุฏูุฏู (ูุซุจุช/ููู)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ูุฑุญูู 3: ุขููุฒุด ู ุงุฑุฒุงุจ ูุฏู\n",
    "# ูุฏู: ุขููุฒุด ูุฏู ู ุงุฑุฒุงุจ ุนููฺฉุฑุฏ ุขู ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # ฺฉุงููพุงู ูุฏู ุจุง ุจูููโุณุงุฒ ู ุชุงุจุน ูุฒูู\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # ุขููุฒุด ูุฏู\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "    # ุงุฑุฒุงุจ ูุฏู\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'ุฏูุช ุชุณุช: {test_acc:.4f}')\n",
    "    return test_acc, test_loss\n",
    "\n",
    "# ุงูฺฏูุฑุชู ุงุตู\n",
    "def main():\n",
    "    # ููููู ุฏุงุฏูโูุง\n",
    "    # ูุฑุถ\n",
    "    texts = [\"ููู ุนุงู ุจูุฏ ู ุฎู ูุฐุช ุจุฑุฏู\", \"ุงู ุจุฏุชุฑู ุชุฌุฑุจู ูู ุจูุฏ\", ...]  # ุฏุงุฏูโูุง ูุชู\n",
    "    labels = [1, 0, ...]  # ุจุฑฺุณุจโูุง\n",
    "    max_words = 10000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช ุฏุฑ ูุงฺฺฏุงู\n",
    "    max_len = 100  # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "\n",
    "    # ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง\n",
    "    x_data, y_data, tokenizer = prepare_text_data(texts, labels, max_words, max_len)\n",
    "    # ุชูุณู ุฏุงุฏูโูุง ุจู ุขููุฒุด ู ุชุณุช (ูุฑุถ: 80% ุขููุฒุดุ 20% ุชุณุช)\n",
    "    train_size = int(0.8 * len(x_data))\n",
    "    x_train, y_train = x_data[:train_size], y_data[:train_size]\n",
    "    x_test, y_test = x_data[train_size:], y_data[train_size:]\n",
    "\n",
    "    # ุชุนุฑู ูุฏู\n",
    "    model = build_cnn_model(max_words, max_len)\n",
    "    # ููุงุด ุฎูุงุตู ูุฏู\n",
    "    model.summary()\n",
    "    # ุขููุฒุด ู ุงุฑุฒุงุจ\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# ุงุฌุฑุง ุจุฑูุงูู\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cf4ac",
   "metadata": {},
   "source": [
    "# ๐ ุดุจฺฉูโูุง ุนุตุจ ุจุงุฒฺฏุดุช (RNN)\n",
    "Recurrent Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## โ๏ธ ฺฺฏููู ฺฉุงุฑ ูโฺฉูุฏุ\n",
    "\n",
    "**ุดุจฺฉูโูุง ุนุตุจ ุจุงุฒฺฏุดุช (RNN)** ููุน ุดุจฺฉู ุนุตุจ ูุณุชูุฏ ฺฉู ุจุฑุง ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุชุฑุชุจ (Sequence Data) ูุซู ูุชูุ ฺฏูุชุงุฑ ุง ุณุฑ ุฒูุงู ุทุฑุงุญ ุดุฏูโุงูุฏ.\n",
    "\n",
    "- ุฏุฑ RNNุ ุฎุฑูุฌ ูุฑ ฺฏุงู ุจู ุนููุงู ูุฑูุฏ ุจู ฺฏุงู ุจุนุฏ ุฏุงุฏู ูโุดูุฏุ  \n",
    "  ุนู ุญุงูุธูโุง ุงุฒ ูุถุนุชโูุง ูุจู ุฏุฑ ูุฏู ูุฌูุฏ ุฏุงุฑุฏ.\n",
    "- ุงู ุณุงุฎุชุงุฑ ุงุฌุงุฒู ูโุฏูุฏ ุชุง ูุฏู ุจุชูุงูุฏ **ูุงุจุณุชฺฏโูุง ุฒูุงู** ู **ุงุฑุชุจุงุท ุจู ุฏุงุฏูโูุง** ุฑุง ุฏุฑ ุฏูุจุงูู ุญูุธ ฺฉูุฏ.\n",
    "\n",
    "> RNNูุง ุจุฑุฎูุงู ุดุจฺฉูโูุง ุนุตุจ ูุนููู (Feedforward)ุ ูโุชูุงููุฏ ุงุทูุงุนุงุช ฺฏุฐุดุชู ุฑุง ุฏุฑ ุฎูุฏ ูฺฏู ุฏุงุฑูุฏ ู ุจุง ุชูุฌู ุจู ุขููุง ุชุตูู ุจฺฏุฑูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง ฺุฑุง RNNุ\n",
    "\n",
    "- **ุฏุฑฺฉ ุชุฑุชุจ ู ุฒููู:**  \n",
    "  ุจุฑุง ุชุญูู ูุชูุ ุตุฏุงุ ููุณูุ ุฏุงุฏูโูุง ุณูุณูุฑ ุง ูุฑ ฺุฒ ฺฉู ุชุฑุชุจ ููู ุจุงุดุฏ.\n",
    "- **ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุจุง ุทูู ูุชุบุฑ:**  \n",
    "  ูุงููุฏ ุฌููุงุช ฺฉูุชุงู ู ุจููุฏุ ุง ุฒูุงูโูุง ูุฎุชูู ุฏุฑ ุณุฑโูุง ุฒูุงู.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ฺฉุงุฑุจุฑุฏูุง ููู\n",
    "\n",
    "- **ูุฏูโุณุงุฒ ุฒุจุงู ุทุจุน (NLP):**  \n",
    "  ุชููุฏ ู ุชุฑุฌูู ูุชูุ ุฎูุงุตูโุณุงุฒ ุฎูุฏฺฉุงุฑ ู ฺุชโุจุงุชโูุง\n",
    "- **ุชุญูู ุงุญุณุงุณุงุช ู ูุญู ุฏุฑ ุฌููุงุช:**  \n",
    "  ุฏุฑฺฉ ูุนูุง ูพููุงู ูพุดุช ูุชู ู ูพุดโุจู ุงุญุณุงุณ ุง ูุตุฏ ฺฏููุฏู\n",
    "- **ุชุดุฎุต ฺฏูุชุงุฑ ู ุชุจุฏู ุตูุช ุจู ูุชู**\n",
    "- **ูพุดโุจู ุณุฑโูุง ุฒูุงู:**  \n",
    "  ูุงููุฏ ููุช ุณูุงูุ ุขุจโูููุงุ ุง ุฏุงุฏูโูุง ูพุฒุดฺฉ\n",
    "- **ุชุดุฎุต ูุนุงูุช ู ุฑูุชุงุฑ ฺฉุงุฑุจุฑ ุงุฒ ุฏุงุฏูโูุง ูุชูุงู**\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฌ ุฎูุงุตู ุฏุฑ ฺฉ ุฌููู:\n",
    "\n",
    "> **RNNูุง ุจุง ุฏุงุดุชู ุญุงูุธู ุฏุงุฎูุ ูโุชูุงููุฏ ูุงุจุณุชฺฏโูุง ุฒูุงู ู ูุนูุง ุฑุง ุฏุฑ ุฏุงุฏูโูุง ุฏูุจุงููโุฏุงุฑ ุฏุฑฺฉ ู ูุฏูโุณุงุฒ ฺฉููุฏ.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f57a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# ุฏุงุฏู\n",
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is bad\",\n",
    "    \"I hate this\",\n",
    "    \"This is great\",\n",
    "    \"I like it\",\n",
    "    \"It is terrible\"\n",
    "]\n",
    "labels = [1, 0, 0, 1, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# ุชูฺฉูุงุฒุฑ ุจุฑุง ุชุจุฏู ฺฉููุงุช ุจู ุงุนุฏุงุฏ\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# ุชุจุฏู ุฌููุงุช ุจู ุฏูุจุงูู ุงุนุฏุงุฏ\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# ุณุงุฎุช ูุฏู \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=padded.shape[1]),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.fit(padded, labels, epochs=10)\n",
    "\n",
    "# ุฌููู ุฌุฏุฏ ฺฉุงุฑุจุฑ\n",
    "new_sentence = [\"I love this product! It works really well\"]\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ุฌููู ุฌุฏุฏ\n",
    "seq = tokenizer.texts_to_sequences(new_sentence)\n",
    "padded_seq = pad_sequences(seq, maxlen=padded.shape[1], padding='post')\n",
    "\n",
    "# ูพุดโุจู ุงุญุณุงุณ ุฌููู\n",
    "prediction = model.predict(padded_seq)[0][0]\n",
    "\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {prediction:.3f}\")\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"Sentiment: Positive ๐\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative ๐\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8625a8",
   "metadata": {},
   "source": [
    "# ุดุจฺฉูโูุง LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72f35",
   "metadata": {},
   "source": [
    "# ๐ LSTM (ุญุงูุธู ุจููุฏูุฏุช ฺฉูุชุงู - Long Short-Term Memory)\n",
    "ฺฉุงุฑุจุฑุฏูุง LSTM ุฏุฑ ุชุนุงูู ุงุญุณุงุณ\n",
    "\n",
    "---\n",
    "\n",
    "## โ๏ธ ฺฺฏููู ฺฉุงุฑ ูโฺฉูุฏุ\n",
    "\n",
    "**LSTM** ููุน ุดุจฺฉู ุนุตุจ ุจุงุฒฺฏุดุช (RNN) ูพุดุฑูุชู ุงุณุช ฺฉู ุจุฑุง ุงุฏฺฏุฑ ูุงุจุณุชฺฏโูุง ุทููุงูโูุฏุช ุฏุฑ ุฏุงุฏูโูุง ุชุฑุชุจ (Sequence Data) ุทุฑุงุญ ุดุฏู ู ูุดฺฉู ูุฑุงููุด ุญุงูุธู ฺฉูุชุงูโูุฏุช ุฑุง ุฏุฑ RNNูุง ูุนููู ุจุฑุทุฑู ูโฺฉูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ ฺฉุงุฑุจุฑุฏูุง ููู LSTM ุฏุฑ ุชุนุงูู ุงุญุณุงุณ\n",
    "\n",
    "- **ุชุญูู ุงุญุณุงุณุงุช ูุชู:**  \n",
    "  ุดูุงุณุง ู ุฏุณุชูโุจูุฏ ุงุญุณุงุณุงุช (ุดุงุฏุ ุบูุ ุฎุดู ู...) ุฏุฑ ูพุงูโูุงุ ุดุจฺฉูโูุง ุงุฌุชูุงุน ุง ูุธุฑุงุช ฺฉุงุฑุจุฑุงู.\n",
    "- **ุชุดุฎุต ฺฏูุชุงุฑ ุงุญุณุงุณ:**  \n",
    "  ุชุญูู ูฺฺฏโูุง ูุซู ุฒุฑูุจูุ ุณุฑุนุช ู ุฑุชู ฺฏูุชุงุฑ ุจุฑุง ุชุดุฎุต ุญุงูุงุช ุงุญุณุงุณ ฺฏููุฏู.\n",
    "- **ูุฏูโุณุงุฒ ูฺฉุงููุงุช ุงุญุณุงุณ:**  \n",
    "  ุญูุธ ุชุงุฑุฎฺู ู ุฒููู ูฺฉุงููุงุช ุจุฑุง ุงุฑุงุฆู ูพุงุณุฎโูุง ููุดููุฏ ู ููุฏูุงูู ุฏุฑ ฺุชโุจุงุชโูุง ู ุฏุณุชุงุฑูุง ุตูุช.\n",
    "- **ูพุดโุจู ุฑูุชุงุฑ ุง ุชุบุฑุงุช ุงุญุณุงุณ:**  \n",
    "  ุชุญูู ุฏุงุฏูโูุง ุณุฑ ุฒูุงู (ูุงููุฏ ูุนุงูุช ฺฉุงุฑุจุฑุงู ุง ุญุงูุงุช ุฑูุฒุงูู) ุจุฑุง ูพุดโุจู ููุณุงูุงุช ุง ุชุบุฑ ุงุญุณุงุณุงุช.\n",
    "- **ุฏุฑฺฉ ุฒุฑูุชู ู ูุญู ุบุฑูุณุชูู:**  \n",
    "  ุดูุงุณุง ุงุญุณุงุณุงุช ูพููุงู ุง ูุญู ุบุฑูุณุชูู ุฏุฑ ูพุงูโูุง ุจุง ุชูุฌู ุจู ุชุฑุชุจ ู ุฒููู ุฌููุงุช.\n",
    "- **ฺฉุงุฑุจุฑุฏ ุฏุฑ ุณูุงูุช ุฑูุงู ู ูุดุงูุฑู ููุดููุฏ:**  \n",
    "  ุฑุตุฏ ุญุงูุงุช ุฑูุญ ุจูุงุฑุงู ู ูพุดููุงุฏ ูุฏุงุฎูุงุช ูุจุชู ุจุฑ ุชุญูู ุงูฺฏููุง ุงุญุณุงุณ ุฏุฑ ุทูู ุฒูุงู.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฌ ุฎูุงุตู ุฏุฑ ฺฉ ุฌููู:\n",
    "\n",
    "> **LSTMูุง ุจุง ูุงุจูุช ุงุฏฺฏุฑ ุฑูุงุจุท ู ูุงุจุณุชฺฏโูุง ุจููุฏูุฏุช ุฏุฑ ุฏุงุฏูโูุง ุชุฑุชุจุ ฺฉ ุงุฒ ูุฏุฑุชููุฏุชุฑู ุงุจุฒุงุฑูุง ุจุฑุง ุชุญููุ ุฏุฑฺฉ ู ูพุดโุจู ุชุนุงููุงุช ุงุญุณุงุณ ุงูุณุงู ู ูุงุดู ุจู ุดูุงุฑ ูโุฑููุฏ.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_features = 10000  # ุชุนุฏุงุฏ ฺฉููุงุช ูพุฑฺฉุงุฑุจุฑุฏ\n",
    "maxlen = 200  # ุญุฏุงฺฉุซุฑ ุทูู ูุธุฑุงุช (ุชุนุฏุงุฏ ฺฉููุงุช)\n",
    "embedding_dim = 100  # ุงุจุนุงุฏ ุจุฑุฏุงุฑ ุฌุงุณุงุฒ\n",
    "\n",
    "# 1. ุจุงุฑฺฏุฐุงุฑ ุฏุชุงุณุช IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 2. ูพุดโูพุฑุฏุงุฒุด: ุงุณุชุงูุฏุงุฑุฏุณุงุฒ ุทูู ูุธุฑุงุช\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ูุฑูุช \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# 3. ุณุงุฎุช ูุฏู \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# ฺฉุงููพุงู ูุฏู\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู\n",
    "model.summary()\n",
    "\n",
    "# 4. ุขููุฒุด ูุฏู\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 5. ุงุฑุฒุงุจ ูุฏู\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'ุฏูุช ูุฏู ุฑู ุฏุงุฏูโูุง ุขุฒูุงุด: {test_accuracy:.4f}')\n",
    "\n",
    "# 6. ุฑุณู ูููุฏุงุฑ ุฏูุช ู ุฎุทุง\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# ูููุฏุงุฑ ุฏูุช\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='ุฏูุช ุขููุฒุด')\n",
    "plt.plot(history.history['val_accuracy'], label='ุฏูุช ุงุนุชุจุงุฑุณูุฌ')\n",
    "plt.title('ุฏูุช ูุฏู')\n",
    "plt.xlabel('ุฏูุฑู (Epoch)')\n",
    "plt.ylabel('ุฏูุช')\n",
    "plt.legend()\n",
    "\n",
    "# ูููุฏุงุฑ ุฎุทุง\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='ุฎุทุง ุขููุฒุด')\n",
    "plt.plot(history.history['val_loss'], label='ุฎุทุง ุงุนุชุจุงุฑุณูุฌ')\n",
    "plt.title('ุฎุทุง ูุฏู')\n",
    "plt.xlabel('ุฏูุฑู (Epoch)')\n",
    "plt.ylabel('ุฎุทุง')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3560eed",
   "metadata": {},
   "source": [
    "# ูุงุญุฏูุง ุจุงุฒฺฏุดุช ฺฏุชโุฏุงุฑ (Gated Recurrent Units - GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f98278",
   "metadata": {},
   "source": [
    "### ๐ฌ ฺฉุงุฑุจุฑุฏ GRU ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "\n",
    "---\n",
    "\n",
    "## โ๏ธ ูพุฑุฏุงุฒุด ูุชู ุชุฑุชุจ\n",
    "\n",
    "ุฏุฑ ุชุญูู ุงุญุณุงุณุงุชุ ูุชู ุจู ุนููุงู ฺฉ ุชูุงู ุงุฒ ฺฉููุงุช ุง ุชูฺฉูโูุง ูพุฑุฏุงุฒุด ูโุดูุฏ.  \n",
    "**GRU** (Gated Recurrent Unit) ุจุง ุชูุฌู ุจู **ุชุฑุชุจ ฺฉููุงุช**ุ ูโุชูุงูุฏ ูุนุงูุ ุฑูุงุจุท ู ูุงุจุณุชฺฏโูุง ูุนูุง ุจู ุขูโูุง ุฑุง ุจู ุฎูุจ ุงุฏ ุจฺฏุฑุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ูุซุงู ฺฉุงุฑุจุฑุฏ\n",
    "\n",
    "ุฏุฑ ุฌููู ุฒุฑ:\n",
    "> _ยซุงู ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏยป_\n",
    "\n",
    "GRU ูุงุฏุฑ ุงุณุช ุงุฑุชุจุงุท ุจู ุนุจุงุฑุงุช ยซุงุตูุงู ุฎูุจ ูุจูุฏยป ู ยซุฎุณุชูโฺฉููุฏูยป ุฑุง ุดูุงุณุง ฺฉูุฏ  \n",
    "ู **ุงุญุณุงุณ ููู** ูพุดุช ุงู ุฌููู ุฑุง ุจู ุฏุฑุณุช ุชุดุฎุต ุฏูุฏุ  \n",
    "ุญุช ุงฺฏุฑ ูุดุงููโูุง ูููโุจูุฏูุ ูพุฎุด ุง ุบุฑูุณุชูู ุจุงู ุดุฏู ุจุงุดูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ก ูฺฉุชู:\n",
    "\n",
    "GRU ุจุง ุณุงุฎุชุงุฑ ุณุงุฏูโุชุฑ ูุณุจุช ุจู LSTMุ ููฺูุงู ูุฏุฑุช ุงุฏฺฏุฑ ูุงุจุณุชฺฏโูุง ูุนูุง ุฏุฑ ูุชูโูุง ุจููุฏ ู ูพฺุฏู ุฑุง ุฏุงุฑุฏ ู ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช ฺฉ ุงุฒ ฺฏุฒููโูุง ูุญุจูุจ ุฏุฑ ุงุฏฺฏุฑ ุนูู ุงุณุช.\n",
    "\n",
    "---\n",
    "\n",
    "> **ุฎูุงุตู:**  \n",
    "> **GRU ุจุง ุฏุฑฺฉ ุชุฑุชุจ ู ูุนูุง ฺฉููุงุชุ ุชุญูู ุงุญุณุงุณุงุช ุฏููโุชุฑ ุฑุง ุฏุฑ ูุชูู ูุงุฑุณ ู ุงูฺฏูุณ ููฺฉู ูโฺฉูุฏ.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ูพุงุฑุงูุชุฑูุง\n",
    "max_words = 1000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช ุฏุฑ ูุงฺฺฏุงู\n",
    "max_len = 20      # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ูุชู\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ุขุฑุงู\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ุณุงุฎุช ูุฏู GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=max_len))  # ูุงู ุชุนุจูโุณุงุฒ\n",
    "model.add(GRU(64, return_sequences=False))  # ูุงู GRU ุจุง 64 ูุงุญุฏ\n",
    "model.add(Dense(1, activation='sigmoid'))   # ูุงู ุฎุฑูุฌ ุจุฑุง ุทุจููโุจูุฏ ุจุงูุฑ\n",
    "\n",
    "# ฺฉุงููพุงู ูุฏู\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู\n",
    "model.summary()\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ุชุณุช ูุฏู ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if prediction[0] > 0.5 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac0762",
   "metadata": {},
   "source": [
    "# ุชุฑุงูุณููุฑูุฑ (Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45b98e",
   "metadata": {},
   "source": [
    "### ๐ฌ ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจุง ูุฏูโูุง ุชุฑุงูุณููุฑูุฑ\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ ูุฏู ุชุญูู ุงุญุณุงุณุงุช ูุชู\n",
    "\n",
    "**ุชุญูู ุงุญุณุงุณุงุช ูุชู** ุจู ุดูุงุณุง ู ุทุจููโุจูุฏ ุงุญุณุงุณุงุช ุฒุฑ ุฏุฑ ูุชู ูโูพุฑุฏุงุฒุฏ:\n",
    "- **ูุซุจุช**\n",
    "- **ููู**\n",
    "- **ุฎูุซ**\n",
    "\n",
    "ุฏุฑ ูุชูู ูุงููุฏ:\n",
    "- ูุธุฑุงุช ฺฉุงุฑุจุฑุงู\n",
    "- ููุฏ ููู ุง ูุญุตูู\n",
    "- ูพุณุชโูุง ุดุจฺฉูโูุง ุงุฌุชูุงุน\n",
    "- ูพุงูโูุง ฺฉุงุฑุจุฑุงู\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ค ฺุฑุง ูุฏูโูุง ุชุฑุงูุณููุฑูุฑ (Transformers)ุ\n",
    "\n",
    "**ูุฏูโูุง ุชุฑุงูุณููุฑูุฑ** ุจู ุฏูู ุชูุงูุง ูููโุงูุนุงุฏูโุดุงู ุฏุฑ:\n",
    "- ุฏุฑฺฉ ุนูู ุฒููู ู ุฑูุงุจุท ูุนูุง ูพฺุฏู ุจู ฺฉููุงุช\n",
    "- ููู ูุงุจุณุชฺฏโูุง ุจููุฏูุฏุช ุฏุฑ ุฌููู ู ูพุงุฑุงฺฏุฑุงู\n",
    "\n",
    "ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช ูุชูุ ุจุณุงุฑ ูุคุซุฑ ู ุฏูู ูุณุชูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ูุชุฌู ููุง\n",
    "\n",
    "ุงู ุชุฑฺฉุจ (ุชุญูู ุงุญุณุงุณุงุช + ุชุฑุงูุณููุฑูุฑูุง)  \n",
    "ูุฑุขูุฏ ุดูุงุณุง ุงุญุณุงุณุงุช ุฑุง **ุณุฑุนโุชุฑุ ุฏููโุชุฑ ู ฺฉุงุฑุขูุฏุชุฑ** ุงุฒ ูุฏูโูุง ุณูุช ูโฺฉูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "> **ุฎูุงุตู:**  \n",
    "> **ูุฏูโูุง ุชุฑุงูุณููุฑูุฑุ ุงุจุฒุงุฑ ุจุณุงุฑ ูู ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจุง ุฏูุช ู ุณุฑุนุช ุจุงูุง ุฏุฑ ูุชูู ูพฺุฏู ู ุจููุฏ ูุณุชูุฏ.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ุจุงุฑฺฏุฐุงุฑ ุชูฺฉูุงุฒุฑ ู ูุฏู BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ุฏุงุฏูโูุง\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ุจุฑุง ุขููุฒุด\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# ุชูุธู ูุฏู ุจุฑุง ุขููุฒุด\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 ุฏูุฑู ุจุฑุง ูุซุงู\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# ุชุณุช ูุฏู ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_inputs = tokenizer(test_text, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "\n",
    "# ูพุดโุจู\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'])\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if predictions[0] == 1 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4cf31",
   "metadata": {},
   "source": [
    "# ุงุชูุงูฺฉูุฏุฑูุง (Autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926b26b",
   "metadata": {},
   "source": [
    "# ๐ ุชูุงูฺฉูุฏุฑูุง (Autoencoders)\n",
    "ุดุจฺฉูโูุง ุนุตุจ ุจุฑุง ุงุฏฺฏุฑ ูุดุฑุฏูโุณุงุฒ ุฏุงุฏูโูุง\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ฺุณุช ุชูุงูฺฉูุฏุฑูุง\n",
    "\n",
    "**ุชูุงูฺฉูุฏุฑูุง** ููุน ุดุจฺฉู ุนุตุจ ูุตููุน ูุณุชูุฏ ฺฉู ุจุฑุง **ุงุฏฺฏุฑ ููุงุด ูุดุฑุฏู ุฏุงุฏูโูุง** (Representation Learning) ุจู ุตูุฑุช **ุจุฏูู ูุธุงุฑุช** (Unsupervised) ุทุฑุงุญ ุดุฏูโุงูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## โ๏ธ ฺฺฏููู ฺฉุงุฑ ูโฺฉููุฏุ\n",
    "\n",
    "- ุฏุงุฏูโูุง ูุฑูุฏ (ูุซูุงู ูุชู ุง ุชุตูุฑ) ูุงุฑุฏ ุดุจฺฉู ูโุดููุฏ.\n",
    "- **ุจุฎุด ุฑูุฒฺฏุฐุงุฑ (Encoder)** ุฏุงุฏูโูุง ุฑุง ุจู ฺฉ ููุงุด ูุดุฑุฏู (ฺฉูุฏ) ุชุจุฏู ูโฺฉูุฏ.\n",
    "- **ุจุฎุด ุฑูุฒฺฏุดุง (Decoder)** ุชูุงุด ูโฺฉูุฏ ุฏุงุฏู ุงุตู ุฑุง ุงุฒ ุงู ฺฉุฏ ูุดุฑุฏู ุจุงุฒุณุงุฒ ฺฉูุฏ.\n",
    "- ุดุจฺฉูุ ุจู ุชุฏุฑุฌ ุงุฏ ูโฺฏุฑุฏ ูููโุชุฑู ูฺฺฏโูุง ุฏุงุฏู ุฑุง ุจู ุตูุฑุช ูุดุฑุฏู ุญูุธ ฺฉูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ ฺฉุงุฑุจุฑุฏ ุชูุงูฺฉูุฏุฑูุง ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "\n",
    "- **ฺฉุงูุด ุงุจุนุงุฏ ุฏุงุฏูโูุง ูุชู ุจุฒุฑฺฏ ู ูพฺุฏู**\n",
    "- **ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ููู ู ูุนูโุฏุงุฑ ุงุฒ ูุชู**\n",
    "- ุจูุจูุฏ ุนููฺฉุฑุฏ ูุฏูโูุง ุงุฏฺฏุฑ ูุงุดู ุฏุฑ **ุชุญูู ุงุญุณุงุณุงุช** ุจุง ุงุฑุงุฆู ููุงุด ูุดุฑุฏู ู ุจููู ุงุฒ ุฏุงุฏูโูุง\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ก ูุซุงู ุนูู (ููููู ฺฉุฏ)\n",
    "\n",
    "ุฏุฑ ุงุฏุงููุ ฺฉ ูุซุงู ุณุงุฏู ุงุฒ ูพุงุฏูโุณุงุฒ ุชูุงูฺฉูุฏุฑ ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุชู  \n",
    "ู ุงุณุชูุงุฏู ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช ุงุฑุงุฆู ุฎูุงูุฏ ุดุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "> **ุฎูุงุตู:**  \n",
    "> **ุชูุงูฺฉูุฏุฑูุง ุจุง ุงุฏฺฏุฑ ููุงุด ูุดุฑุฏู ุฏุงุฏูโูุงุ ุงุจุฒุงุฑ ูุฏุฑุชููุฏ ุจุฑุง ูพุดโูพุฑุฏุงุฒุดุ ฺฉุงูุด ุงุจุนุงุฏ ู ุงูุฒุงุด ุฏูุช ูุฏูโูุง ุชุญูู ุงุญุณุงุณุงุช ูุชู ูุณุชูุฏ.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ูุชู\n",
    "max_words = 1000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช\n",
    "max_len = 20      # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ุชุจุฏู ุฏุงุฏูโูุง ุจู ุขุฑุงู\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# ุณุงุฎุช ุงุชูุงูฺฉูุฏุฑ\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 50, input_length=max_len)(input_layer)\n",
    "encoded = LSTM(64, return_sequences=False)(embedding)  # ูุงู ุงูฺฉูุฏุฑ\n",
    "decoded = Dense(max_len * 50, activation='relu')(encoded)  # ูุงู ุฏฺฉูุฏุฑ\n",
    "decoded = Reshape((max_len, 50))(decoded)\n",
    "\n",
    "# ูุฏู ุงุชูุงูฺฉูุฏุฑ\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# ูุฏู ุทุจููโุจูุฏ (ุงุณุชูุงุฏู ุงุฒ ููุงุด ูุดุฑุฏู ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "classification_model = Model(input_layer, classification_output)\n",
    "classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู ุทุจููโุจูุฏ\n",
    "classification_model.summary()\n",
    "\n",
    "# ุขููุฒุด ุงุชูุงูฺฉูุฏุฑ\n",
    "autoencoder.fit(X, X.reshape(X.shape[0], max_len, 50), epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ุขููุฒุด ูุฏู ุทุจููโุจูุฏ\n",
    "classification_model.fit(X, y, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ุชุณุช ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = classification_model.predict(test_padded)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if prediction[0] > 0.5 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b5dc5",
   "metadata": {},
   "source": [
    "# ุดุจฺฉูโูุง ุจุงูุฑ ุนูู (Deep Belief Networks - DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05279512",
   "metadata": {},
   "source": [
    "### ๐ฌ ุชุญูู ุงุญุณุงุณุงุช ูุชู ู ููุด DBNูุง\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ ูุฏู ุชุญูู ุงุญุณุงุณุงุช ูุชู\n",
    "\n",
    "ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจู **ุดูุงุณุง ุงุญุณุงุณุงุช** ุฒุฑ ุฏุฑ ูุชู ูโูพุฑุฏุงุฒุฏ:\n",
    "- **ูุซุจุช**\n",
    "- **ููู**\n",
    "- **ุฎูุซ**\n",
    "\n",
    "ุฏุฑ ูุชูู ูุงููุฏ:\n",
    "- ูุธุฑุงุช ฺฉุงุฑุจุฑุงู\n",
    "- ููุฏูุง\n",
    "- ูพุณุชโูุง ุดุจฺฉูโูุง ุงุฌุชูุงุน\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง ููุด DBNูุง (Deep Belief Networks) ุฏุฑ ุงู ุญูุฒู\n",
    "\n",
    "DBNูุง ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจุฑุง ููุงุฑุฏ ุฒุฑ ุงุณุชูุงุฏู ูโุดููุฏ:\n",
    "- **ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุนูู ู ุบุฑุฎุท** ุงุฒ ุฏุงุฏูโูุง ูุชู  \n",
    "  (ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ุงูฺฏููุง ูพููุงู ู ุฑูุงุจุท ูพฺุฏู ุจู ูุงฺูโูุง ู ุนุจุงุฑุงุช)\n",
    "- **ฺฉุงูุด ุงุจุนุงุฏ ุฏุงุฏูโูุง** ู ุญุฐู ููุฒ ุจุฑุง ุจูุจูุฏ ุนููฺฉุฑุฏ ูุฏูโูุง ุทุจููโุจูุฏ ุงุญุณุงุณุงุช\n",
    "- **ุงุฏฺฏุฑ ุจุงุฒููุง ุจููู ุงุฒ ูุชู** ุฌูุช ุงูุชูุงู ุจู ูุฏูโูุง ุทุจููโุจูุฏ (ูุงููุฏ Softmax ุง SVM)\n",
    "- **ูพุดโูพุฑุฏุงุฒุด ู ูุดุฑุฏูโุณุงุฒ ุฏุงุฏูโูุง ุญุฌู** ุจุฑุง ุงูุฒุงุด ุณุฑุนุช ู ุฏูุช ุชุญูู ุงุญุณุงุณุงุช\n",
    "- **ุจูุจูุฏ ุนููฺฉุฑุฏ ุฏุฑ ุฏุงุฏูโูุง ูพฺุฏู ู ฺูุฏูุงู** ูุงููุฏ ุฌููุงุช ุทููุงูุ ุฒุจุงู ุบุฑูุณุชูู ุง ุทูุฒ\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ุฌูุนโุจูุฏ\n",
    "\n",
    "ุงู ุงุจุฒุงุฑูุง ฺฉูฺฉ ูโฺฉููุฏ ุชุง ุชุญูู ุงุญุณุงุณุงุช **ุฏููโุชุฑ** ู **ฺฉุงุฑุขูุฏุชุฑ** ุงูุฌุงู ุดูุฏุ  \n",
    "ุจูโูฺู ุฏุฑ ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุญุฌู ู ูุชูู ูพฺุฏู ู ฺูุฏูุนูุง.\n",
    "\n",
    "---\n",
    "\n",
    "> **ุฎูุงุตู:**  \n",
    "> **DBNูุง ุจุง ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ูฺฺฏโูุง ุนููุ ุชุญูู ุงุญุณุงุณุงุช ูุชู ุฑุง ุฏุฑ ูุชูู ุจุฒุฑฺฏ ู ูพฺุฏู ุจุณุงุฑ ุชููุช ูโฺฉููุฏ.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ูุชู ุจุง TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# ุณุงุฎุช ูุฏู DBN-ูุงููุฏ (ุดุจฺฉู ุนูู ุจุง ูุงูโูุง Dense)\n",
    "model = Sequential()\n",
    "# ูุงูโูุง ูุฎู ูุดุงุจู RBM ุจุฑุง ุงุฏฺฏุฑ ูฺฺฏโูุง\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# ูุงู ุฎุฑูุฌ ุจุฑุง ุทุจููโุจูุฏ\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ฺฉุงููพุงู ูุฏู\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู\n",
    "model.summary()\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.fit(X, y, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "# ุชุณุช ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_X = vectorizer.transform(test_text).toarray()\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if prediction[0] > 0.5 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c1c1a",
   "metadata": {},
   "source": [
    "### ๐ ุฌูุนโุจูุฏ\n",
    "\n",
    "ุฏุฑ ุงู ุจุฎุดุ ุจุง ุชูุงู ุงูฺฏูุฑุชูโูุง ู ูุฏูโูุง ฺฉู ุงุฒ ุญูุฒู **ุงุฏฺฏุฑ ุนูู (Deep Learning)** ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจู ฺฉุงุฑ ูโุฑููุฏุ ุขุดูุง ุดุฏู.\n",
    "\n",
    "ุงู ูุฏูโูุง ู ุงุจุฒุงุฑูุงุ ุจุง ุจูุฑูโฺฏุฑ ุงุฒ ุณุงุฎุชุงุฑูุง ูพฺุฏู ู ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ุงูฺฏููุง ูพููุงูุ  \n",
    "ูุฑุขูุฏ **ุดูุงุณุงุ ุฏุณุชูโุจูุฏ ู ุชุญูู ุงุญุณุงุณุงุช** ุฏุฑ ูุชูู ูุฎุชูู (ูุธุฑุงุช ฺฉุงุฑุจุฑุงูุ ุดุจฺฉูโูุง ุงุฌุชูุงุนุ ููุฏูุง ู ...) ุฑุง ุจุณุงุฑ ุณุฑุนโุชุฑุ ุฏููโุชุฑ ู ููุดููุฏุงููโุชุฑ ฺฉุฑุฏูโุงูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "> **ุงุฏฺฏุฑ ุนููุ ูุณุฑ ุชุญูู ุงุญุณุงุณุงุช ูุชู ุฑุง ูุชุญูู ู ุฒููู ุฑุง ุจุฑุง ุชูุณุนู ุณุณุชูโูุง ููุดููุฏ ู ููู ุนููโุชุฑ ุฒุจุงู ุงูุณุงู ูููุงุฑ ฺฉุฑุฏู ุงุณุช.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
