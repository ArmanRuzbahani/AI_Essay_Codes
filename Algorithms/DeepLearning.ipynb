{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77945b74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# سفر یادگیری: کشف معماری‌های یادگیری عمیق 🚀🧠\n",
    "\n",
    "**بیایید با هم سه معماری اساسی یادگیری عمیق را کشف کنیم:**\n",
    "\n",
    "## 1. <span style=\"color: #4d90fe;\">CNN (شبکه عصبی کانولوشنی)</span> 🖼️\n",
    "- **مغز بینایی ماشین** 👁️\n",
    "- از فیلترهای کانولوشنی برای تشخیص الگوهای محلی استفاده می‌کند\n",
    "- ایده‌آل برای پردازش تصاویر و ویدیو\n",
    "- مثال: تشخیص اشیا در عکس‌ها\n",
    "\n",
    "## 2. <span style=\"color: #34a853;\">RNN (شبکه عصبی بازگشتی)</span> 🔄\n",
    "- **حافظه کوتاه‌مدت برای داده‌های متوالی** 📜\n",
    "- می‌تواند اطلاعات را از مراحل قبلی به خاطر بسپارد\n",
    "- مناسب برای متن، گفتار و داده‌های زمانی\n",
    "- مثال: پیش‌بینی کلمه بعدی در جمله\n",
    "\n",
    "## 3. <span style=\"color: #ea4335;\">Transformer</span> ⚡\n",
    "- **انقلابی در پردازش زبان** 💬\n",
    "- از مکانیزم توجه (Attention) استفاده می‌کند\n",
    "- می‌تواند روابط بلندمدت را در داده‌ها تشخیص دهد\n",
    "- مثال: ترجمه ماشینی، مدل‌های گفتگو\n",
    "\n",
    "📌 **نکته طلایی:**  \n",
    "این معماری‌ها اغلب با هم ترکیب می‌شوند تا سیستم‌های هوشمند قدرتمندی بسازند!\n",
    "\n",
    "🎯 **هدف ما در این مسیر:**  \n",
    "یادگیری اصول هر معماری + درک کاربردهای عملی + پیاده‌سازی ساده\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664615",
   "metadata": {},
   "source": [
    "# یادگیری عمیق: موتور محرک هوش مصنوعی مدرن 🌟🧠\n",
    "\n",
    "**یادگیری عمیق**، پیشرفته‌ترین شاخه یادگیری ماشین است که با الهام از ساختار مغز انسان، انقلابی در پردازش داده‌ها ایجاد کرده است. این فناوری با معماری‌های هوشمندانه خود قادر به یادگیری سلسله‌مراتبی ویژگی‌ها از داده‌های خام است:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa30d",
   "metadata": {},
   "source": [
    "# CNN (شبکه عصبی کانولوشنی)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# کتابخانه‌های مورد نیاز\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# مرحله 1: آماده‌سازی داده‌های متنی\n",
    "# هدف: تبدیل متن به بردارهای عددی و آماده‌سازی برای ورودی شبکه\n",
    "def prepare_text_data(texts, labels, max_words=10000, max_len=100):\n",
    "    # توکن‌سازی: تبدیل کلمات به توکن‌های عددی\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # تبدیل متن به دنباله‌های عددی\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # پد کردن دنباله‌ها برای یکسان‌سازی طول\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    # تبدیل برچسب‌ها به آرایه\n",
    "    labels = np.array(labels)\n",
    "    return data, labels, tokenizer\n",
    "\n",
    "# مرحله 2: تعریف معماری شبکه کانولوشنی\n",
    "# هدف: ایجاد یک مدل CNN برای استخراج ویژگی‌های متنی و طبقه‌بندی احساسات\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = models.Sequential([\n",
    "        # لایه تعبیه: تبدیل توکن‌ها به بردارهای متراکم\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        # لایه کانولوشنی اول: استخراج ویژگی‌های محلی از متن\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        # لایه پولینگ: کاهش ابعاد و حفظ ویژگی‌های مهم\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # لایه کانولوشنی دوم: استخراج ویژگی‌های پیچیده‌تر\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # مسطح‌سازی: آماده‌سازی برای لایه‌های متراکم\n",
    "        layers.Flatten(),\n",
    "        # لایه کاملاً متصل: ترکیب ویژگی‌ها\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # لایه خروجی: طبقه‌بندی دودویی (مثبت/منفی)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# مرحله 3: آموزش و ارزیابی مدل\n",
    "# هدف: آموزش مدل و ارزیابی عملکرد آن در تحلیل احساسات\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # کامپایل مدل با بهینه‌ساز و تابع هزینه\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # آموزش مدل\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "    # ارزیابی مدل\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'دقت تست: {test_acc:.4f}')\n",
    "    return test_acc, test_loss\n",
    "\n",
    "# الگوریتم اصلی\n",
    "def main():\n",
    "    # نمونه داده‌ها\n",
    "    # فرض\n",
    "    texts = [\"فیلم عالی بود و خیلی لذت بردم\", \"این بدترین تجربه من بود\", ...]  # داده‌های متنی\n",
    "    labels = [1, 0, ...]  # برچسب‌ها\n",
    "    max_words = 10000  # حداکثر تعداد کلمات در واژگان\n",
    "    max_len = 100  # حداکثر طول دنباله\n",
    "\n",
    "    # آماده‌سازی داده‌ها\n",
    "    x_data, y_data, tokenizer = prepare_text_data(texts, labels, max_words, max_len)\n",
    "    # تقسیم داده‌ها به آموزشی و تست (فرض: 80% آموزشی، 20% تست)\n",
    "    train_size = int(0.8 * len(x_data))\n",
    "    x_train, y_train = x_data[:train_size], y_data[:train_size]\n",
    "    x_test, y_test = x_data[train_size:], y_data[train_size:]\n",
    "\n",
    "    # تعریف مدل\n",
    "    model = build_cnn_model(max_words, max_len)\n",
    "    # نمایش خلاصه مدل\n",
    "    model.summary()\n",
    "    # آموزش و ارزیابی\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# اجرای برنامه\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cf4ac",
   "metadata": {},
   "source": [
    "# شبکه‌های عصبی بازگشتی (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f57a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# داده\n",
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is bad\",\n",
    "    \"I hate this\",\n",
    "    \"This is great\",\n",
    "    \"I like it\",\n",
    "    \"It is terrible\"\n",
    "]\n",
    "labels = [1, 0, 0, 1, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# توکنایزر برای تبدیل کلمات به اعداد\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# تبدیل جملات به دنباله اعداد\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# ساخت مدل \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=padded.shape[1]),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# آموزش مدل\n",
    "model.fit(padded, labels, epochs=10)\n",
    "\n",
    "# جمله جدید کاربر\n",
    "new_sentence = [\"I love this product! It works really well\"]\n",
    "\n",
    "# پیش‌پردازش جمله جدید\n",
    "seq = tokenizer.texts_to_sequences(new_sentence)\n",
    "padded_seq = pad_sequences(seq, maxlen=padded.shape[1], padding='post')\n",
    "\n",
    "# پیش‌بینی احساس جمله\n",
    "prediction = model.predict(padded_seq)[0][0]\n",
    "\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {prediction:.3f}\")\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"Sentiment: Positive 😊\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative 😞\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8625a8",
   "metadata": {},
   "source": [
    "# شبکه‌های LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72f35",
   "metadata": {},
   "source": [
    "## کاربردهای LSTM در تعامل احساسی\n",
    "LSTM در حوزه تعامل احساسات انسان با هوش مصنوعی کاربردهای متنوعی دارد:\n",
    "- **تحلیل احساسات متنی:** تشخیص احساسات (شادی، غم، خشم) در متون کاربران.\n",
    "- **تشخیص گفتار احساسی:** تحلیل زیروبمی و آهنگ گفتار برای درک حالت احساسی.\n",
    "- **مدل‌سازی مکالمات:** حفظ تاریخچه مکالمه برای پاسخ‌های متناسب با احساسات.\n",
    "- **پیش‌بینی رفتار احساسی:** استفاده از داده‌های سری زمانی برای پیش‌بینی تغییرات احساسی."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_features = 10000  # تعداد کلمات پرکاربرد\n",
    "maxlen = 200  # حداکثر طول نظرات (تعداد کلمات)\n",
    "embedding_dim = 100  # ابعاد بردار جاسازی\n",
    "\n",
    "# 1. بارگذاری دیتاست IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 2. پیش‌پردازش: استانداردسازی طول نظرات\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# تبدیل برچسب‌ها به فرمت \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# 3. ساخت مدل \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# کامپایل مدل\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل\n",
    "model.summary()\n",
    "\n",
    "# 4. آموزش مدل\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 5. ارزیابی مدل\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'دقت مدل روی داده‌های آزمایشی: {test_accuracy:.4f}')\n",
    "\n",
    "# 6. رسم نمودار دقت و خطا\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# نمودار دقت\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='دقت آموزش')\n",
    "plt.plot(history.history['val_accuracy'], label='دقت اعتبارسنجی')\n",
    "plt.title('دقت مدل')\n",
    "plt.xlabel('دوره (Epoch)')\n",
    "plt.ylabel('دقت')\n",
    "plt.legend()\n",
    "\n",
    "# نمودار خطا\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='خطای آموزش')\n",
    "plt.plot(history.history['val_loss'], label='خطای اعتبارسنجی')\n",
    "plt.title('خطای مدل')\n",
    "plt.xlabel('دوره (Epoch)')\n",
    "plt.ylabel('خطا')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3560eed",
   "metadata": {},
   "source": [
    "# واحدهای بازگشتی گیت‌دار (Gated Recurrent Units - GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f98278",
   "metadata": {},
   "source": [
    "### کاربرد در تحلیل احساسات\n",
    "\n",
    "**پردازش متن ترتیبی:**  \n",
    "در تحلیل احساسات، متن به صورت یک توالی از کلمات یا توکن‌ها پردازش می‌شود.  \n",
    "**GRU** با توجه به ترتیب کلمات، معانی و روابط بین آن‌ها را یاد می‌گیرد.\n",
    "\n",
    "**مثال:**  \n",
    "برای مثال، در جمله زیر:  \n",
    "_\"این فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود\"_\n",
    "\n",
    "GRU می‌تواند وابستگی بین بخش‌های مختلف جمله مانند \"اصلاً خوب نبود\" و \"خسته‌کننده\" را درک کند تا احساس منفی را تشخیص دهد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# پارامترها\n",
    "max_words = 1000  # حداکثر تعداد کلمات در واژگان\n",
    "max_len = 20      # حداکثر طول دنباله\n",
    "\n",
    "# پیش‌پردازش متن\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# تبدیل برچسب‌ها به آرایه\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ساخت مدل GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=max_len))  # لایه تعبیه‌سازی\n",
    "model.add(GRU(64, return_sequences=False))  # لایه GRU با 64 واحد\n",
    "model.add(Dense(1, activation='sigmoid'))   # لایه خروجی برای طبقه‌بندی باینری\n",
    "\n",
    "# کامپایل مدل\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل\n",
    "model.summary()\n",
    "\n",
    "# آموزش مدل\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# تست مدل روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if prediction[0] > 0.5 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac0762",
   "metadata": {},
   "source": [
    "# ترانسفورمر (Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45b98e",
   "metadata": {},
   "source": [
    "### تحلیل احساسات متنی\n",
    "\n",
    "تحلیل احساسات متنی به **شناسایی احساسات** شامل:\n",
    "- **مثبت**\n",
    "- **منفی**\n",
    "- **خنثی**\n",
    "\n",
    "در متن‌هایی مانند:\n",
    "- نظرات کاربران\n",
    "- نقدها\n",
    "- پست‌های شبکه‌های اجتماعی\n",
    "\n",
    "می‌پردازد.\n",
    "\n",
    "---\n",
    "\n",
    "**مدل‌های ترانسفورمر** به دلیل توانایی بی‌نظیرشان در:\n",
    "- درک زمینه و زمینه‌های معنایی پیچیده\n",
    "- فهم روابط بلندمدت بین کلمات\n",
    "\n",
    "در این حوزه بسیار مؤثر هستند.\n",
    "\n",
    "---\n",
    "\n",
    "این ترکیب، تحلیل احساسات را سریع‌تر، دقیق‌تر و کارآمدتر می‌کند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# بارگذاری توکنایزر و مدل BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# پیش‌پردازش داده‌ها\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# آماده‌سازی داده‌ها برای آموزش\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# تنظیم مدل برای آموزش\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# آموزش مدل\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 دوره برای مثال\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# تست مدل روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_inputs = tokenizer(test_text, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "\n",
    "# پیش‌بینی\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'])\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if predictions[0] == 1 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4cf31",
   "metadata": {},
   "source": [
    "# اتوانکودرها (Autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926b26b",
   "metadata": {},
   "source": [
    "\n",
    "توانکودرها  نوعی از شبکه‌های عصبی مصنوعی هستند که برای **یادگیری نمایش فشرده داده‌ها** به صورت **بدون نظارت**  طراحی شده‌اند.\n",
    "\n",
    "---\n",
    "\n",
    "## توضیحات\n",
    "\n",
    "در این بخش، به موارد زیر پرداخته می‌شود:\n",
    "- **چیستی توانکودرها**  \n",
    "- **چگونگی عملکرد آن‌ها**  \n",
    "- **کاربردشان در تحلیل احساسات متنی**\n",
    "\n",
    "همچنین، یک **مثال ساده** از پیاده‌سازی توانکودر برای تحلیل احساسات متنی ارائه می‌گردد.\n",
    "\n",
    "---\n",
    "\n",
    "## کاربرد در تحلیل احساسات\n",
    "\n",
    "توانکودرها می‌توانند برای کاهش ابعاد و استخراج ویژگی‌های مهم از متن‌های بزرگ و پیچیده استفاده شوند، که این کار به بهبود فرآیند تحلیل احساسات کمک می‌کند.\n",
    "\n",
    "---\n",
    "\n",
    "**در ادامه، مثال عملی و کد نمونه‌ای برای استفاده از توانکودر در تحلیل احساسات متن آورده شده است.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# پیش‌پردازش متن\n",
    "max_words = 1000  # حداکثر تعداد کلمات\n",
    "max_len = 20      # حداکثر طول دنباله\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# تبدیل داده‌ها به آرایه\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# ساخت اتوانکودر\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 50, input_length=max_len)(input_layer)\n",
    "encoded = LSTM(64, return_sequences=False)(embedding)  # لایه انکودر\n",
    "decoded = Dense(max_len * 50, activation='relu')(encoded)  # لایه دیکودر\n",
    "decoded = Reshape((max_len, 50))(decoded)\n",
    "\n",
    "# مدل اتوانکودر\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# مدل طبقه‌بندی (استفاده از نمایش فشرده برای تحلیل احساسات)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "classification_model = Model(input_layer, classification_output)\n",
    "classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل طبقه‌بندی\n",
    "classification_model.summary()\n",
    "\n",
    "# آموزش اتوانکودر\n",
    "autoencoder.fit(X, X.reshape(X.shape[0], max_len, 50), epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# آموزش مدل طبقه‌بندی\n",
    "classification_model.fit(X, y, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# تست روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = classification_model.predict(test_padded)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if prediction[0] > 0.5 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b5dc5",
   "metadata": {},
   "source": [
    "# شبکه‌های باور عمیق (Deep Belief Networks - DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05279512",
   "metadata": {},
   "source": [
    "### تحلیل احساسات متنی و نقش DBN‌ها\n",
    "\n",
    "تحلیل احساسات متنی به **شناسایی احساسات** شامل موارد زیر می‌پردازد:\n",
    "- **مثبت**\n",
    "- **منفی**\n",
    "- **خنثی**\n",
    "\n",
    "در متن‌هایی مانند:\n",
    "- نظرات کاربران\n",
    "- نقدها\n",
    "- پست‌های شبکه‌های اجتماعی\n",
    "\n",
    "---\n",
    "\n",
    "## نقش DBN‌ها در این حوزه\n",
    "\n",
    "DBN‌ها (Deep Belief Networks) در این حوزه به روش‌های زیر استفاده می‌شوند:\n",
    "- **[در این قسمت، لیستی از روش‌ها یا کاربردهای خاص که DBN‌ها در تحلیل احساسات مورد استفاده قرار می‌گیرند، آورده می‌شود.]**\n",
    "\n",
    "---\n",
    "\n",
    "این ابزارها کمک می‌کنند تا تحلیل احساسات دقیق‌تر و کارآمدتر انجام شود، به ویژه در پردازش داده‌های حجیم و پیچیده."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# پیش‌پردازش متن با TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# ساخت مدل DBN-مانند (شبکه عمیق با لایه‌های Dense)\n",
    "model = Sequential()\n",
    "# لایه‌های مخفی مشابه RBM برای یادگیری ویژگی‌ها\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# لایه خروجی برای طبقه‌بندی\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# کامپایل مدل\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل\n",
    "model.summary()\n",
    "\n",
    "# آموزش مدل\n",
    "model.fit(X, y, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "# تست روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_X = vectorizer.transform(test_text).toarray()\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if prediction[0] > 0.5 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c1c1a",
   "metadata": {},
   "source": [
    "### جمع‌بندی\n",
    "\n",
    "در این قسمت، با تمام الگوریتم‌هایی که از بخش **یادگیری عمیق ** برای تحلیل احساسات متنی استفاده می‌شوند، آشنا شدیم."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
