{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77945b74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ุณูุฑ ุงุฏฺฏุฑ: ฺฉุดู ูุนูุงุฑโูุง ุงุฏฺฏุฑ ุนูู ๐๐ง\n",
    "\n",
    "**ุจุงุฏ ุจุง ูู ุณู ูุนูุงุฑ ุงุณุงุณ ุงุฏฺฏุฑ ุนูู ุฑุง ฺฉุดู ฺฉูู:**\n",
    "\n",
    "## 1. <span style=\"color: #4d90fe;\">CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู)</span> ๐ผ๏ธ\n",
    "- **ูุบุฒ ุจูุง ูุงุดู** ๐๏ธ\n",
    "- ุงุฒ ููุชุฑูุง ฺฉุงููููุดู ุจุฑุง ุชุดุฎุต ุงูฺฏููุง ูุญู ุงุณุชูุงุฏู ูโฺฉูุฏ\n",
    "- ุงุฏูโุขู ุจุฑุง ูพุฑุฏุงุฒุด ุชุตุงูุฑ ู ูุฏู\n",
    "- ูุซุงู: ุชุดุฎุต ุงุดุง ุฏุฑ ุนฺฉุณโูุง\n",
    "\n",
    "## 2. <span style=\"color: #34a853;\">RNN (ุดุจฺฉู ุนุตุจ ุจุงุฒฺฏุดุช)</span> ๐\n",
    "- **ุญุงูุธู ฺฉูุชุงูโูุฏุช ุจุฑุง ุฏุงุฏูโูุง ูุชูุงู** ๐\n",
    "- ูโุชูุงูุฏ ุงุทูุงุนุงุช ุฑุง ุงุฒ ูุฑุงุญู ูุจู ุจู ุฎุงุทุฑ ุจุณูพุงุฑุฏ\n",
    "- ููุงุณุจ ุจุฑุง ูุชูุ ฺฏูุชุงุฑ ู ุฏุงุฏูโูุง ุฒูุงู\n",
    "- ูุซุงู: ูพุดโุจู ฺฉููู ุจุนุฏ ุฏุฑ ุฌููู\n",
    "\n",
    "## 3. <span style=\"color: #ea4335;\">Transformer</span> โก\n",
    "- **ุงูููุงุจ ุฏุฑ ูพุฑุฏุงุฒุด ุฒุจุงู** ๐ฌ\n",
    "- ุงุฒ ูฺฉุงูุฒู ุชูุฌู (Attention) ุงุณุชูุงุฏู ูโฺฉูุฏ\n",
    "- ูโุชูุงูุฏ ุฑูุงุจุท ุจููุฏูุฏุช ุฑุง ุฏุฑ ุฏุงุฏูโูุง ุชุดุฎุต ุฏูุฏ\n",
    "- ูุซุงู: ุชุฑุฌูู ูุงุดูุ ูุฏูโูุง ฺฏูุชฺฏู\n",
    "\n",
    "๐ **ูฺฉุชู ุทูุง:**  \n",
    "ุงู ูุนูุงุฑโูุง ุงุบูุจ ุจุง ูู ุชุฑฺฉุจ ูโุดููุฏ ุชุง ุณุณุชูโูุง ููุดููุฏ ูุฏุฑุชููุฏ ุจุณุงุฒูุฏ!\n",
    "\n",
    "๐ฏ **ูุฏู ูุง ุฏุฑ ุงู ูุณุฑ:**  \n",
    "ุงุฏฺฏุฑ ุงุตูู ูุฑ ูุนูุงุฑ + ุฏุฑฺฉ ฺฉุงุฑุจุฑุฏูุง ุนูู + ูพุงุฏูโุณุงุฒ ุณุงุฏู\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664615",
   "metadata": {},
   "source": [
    "# ุงุฏฺฏุฑ ุนูู: ููุชูุฑ ูุญุฑฺฉ ููุด ูุตููุน ูุฏุฑู ๐๐ง\n",
    "\n",
    "**ุงุฏฺฏุฑ ุนูู**ุ ูพุดุฑูุชูโุชุฑู ุดุงุฎู ุงุฏฺฏุฑ ูุงุดู ุงุณุช ฺฉู ุจุง ุงููุงู ุงุฒ ุณุงุฎุชุงุฑ ูุบุฒ ุงูุณุงูุ ุงูููุงุจ ุฏุฑ ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุงุฌุงุฏ ฺฉุฑุฏู ุงุณุช. ุงู ููุงูุฑ ุจุง ูุนูุงุฑโูุง ููุดููุฏุงูู ุฎูุฏ ูุงุฏุฑ ุจู ุงุฏฺฏุฑ ุณูุณููโูุฑุงุชุจ ูฺฺฏโูุง ุงุฒ ุฏุงุฏูโูุง ุฎุงู ุงุณุช:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa30d",
   "metadata": {},
   "source": [
    "# CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ฺฉุชุงุจุฎุงููโูุง ููุฑุฏ ูุงุฒ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ูุฑุญูู 1: ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ูุชู\n",
    "# ูุฏู: ุชุจุฏู ูุชู ุจู ุจุฑุฏุงุฑูุง ุนุฏุฏ ู ุขูุงุฏูโุณุงุฒ ุจุฑุง ูุฑูุฏ ุดุจฺฉู\n",
    "def prepare_text_data(texts, labels, max_words=10000, max_len=100):\n",
    "    # ุชูฺฉูโุณุงุฒ: ุชุจุฏู ฺฉููุงุช ุจู ุชูฺฉูโูุง ุนุฏุฏ\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # ุชุจุฏู ูุชู ุจู ุฏูุจุงููโูุง ุนุฏุฏ\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # ูพุฏ ฺฉุฑุฏู ุฏูุจุงููโูุง ุจุฑุง ฺฉุณุงูโุณุงุฒ ุทูู\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    # ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ุขุฑุงู\n",
    "    labels = np.array(labels)\n",
    "    return data, labels, tokenizer\n",
    "\n",
    "# ูุฑุญูู 2: ุชุนุฑู ูุนูุงุฑ ุดุจฺฉู ฺฉุงููููุดู\n",
    "# ูุฏู: ุงุฌุงุฏ ฺฉ ูุฏู CNN ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุชู ู ุทุจููโุจูุฏ ุงุญุณุงุณุงุช\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = models.Sequential([\n",
    "        # ูุงู ุชุนุจู: ุชุจุฏู ุชูฺฉูโูุง ุจู ุจุฑุฏุงุฑูุง ูุชุฑุงฺฉู\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        # ูุงู ฺฉุงููููุดู ุงูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุญู ุงุฒ ูุชู\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        # ูุงู ูพูููฺฏ: ฺฉุงูุด ุงุจุนุงุฏ ู ุญูุธ ูฺฺฏโูุง ููู\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # ูุงู ฺฉุงููููุดู ุฏูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูพฺุฏูโุชุฑ\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # ูุณุทุญโุณุงุฒ: ุขูุงุฏูโุณุงุฒ ุจุฑุง ูุงูโูุง ูุชุฑุงฺฉู\n",
    "        layers.Flatten(),\n",
    "        # ูุงู ฺฉุงููุงู ูุชุตู: ุชุฑฺฉุจ ูฺฺฏโูุง\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # ูุงู ุฎุฑูุฌ: ุทุจููโุจูุฏ ุฏูุฏู (ูุซุจุช/ููู)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ูุฑุญูู 3: ุขููุฒุด ู ุงุฑุฒุงุจ ูุฏู\n",
    "# ูุฏู: ุขููุฒุด ูุฏู ู ุงุฑุฒุงุจ ุนููฺฉุฑุฏ ุขู ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # ฺฉุงููพุงู ูุฏู ุจุง ุจูููโุณุงุฒ ู ุชุงุจุน ูุฒูู\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # ุขููุฒุด ูุฏู\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "    # ุงุฑุฒุงุจ ูุฏู\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'ุฏูุช ุชุณุช: {test_acc:.4f}')\n",
    "    return test_acc, test_loss\n",
    "\n",
    "# ุงูฺฏูุฑุชู ุงุตู\n",
    "def main():\n",
    "    # ููููู ุฏุงุฏูโูุง\n",
    "    # ูุฑุถ\n",
    "    texts = [\"ููู ุนุงู ุจูุฏ ู ุฎู ูุฐุช ุจุฑุฏู\", \"ุงู ุจุฏุชุฑู ุชุฌุฑุจู ูู ุจูุฏ\", ...]  # ุฏุงุฏูโูุง ูุชู\n",
    "    labels = [1, 0, ...]  # ุจุฑฺุณุจโูุง\n",
    "    max_words = 10000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช ุฏุฑ ูุงฺฺฏุงู\n",
    "    max_len = 100  # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "\n",
    "    # ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง\n",
    "    x_data, y_data, tokenizer = prepare_text_data(texts, labels, max_words, max_len)\n",
    "    # ุชูุณู ุฏุงุฏูโูุง ุจู ุขููุฒุด ู ุชุณุช (ูุฑุถ: 80% ุขููุฒุดุ 20% ุชุณุช)\n",
    "    train_size = int(0.8 * len(x_data))\n",
    "    x_train, y_train = x_data[:train_size], y_data[:train_size]\n",
    "    x_test, y_test = x_data[train_size:], y_data[train_size:]\n",
    "\n",
    "    # ุชุนุฑู ูุฏู\n",
    "    model = build_cnn_model(max_words, max_len)\n",
    "    # ููุงุด ุฎูุงุตู ูุฏู\n",
    "    model.summary()\n",
    "    # ุขููุฒุด ู ุงุฑุฒุงุจ\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# ุงุฌุฑุง ุจุฑูุงูู\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cf4ac",
   "metadata": {},
   "source": [
    "# ุดุจฺฉูโูุง ุนุตุจ ุจุงุฒฺฏุดุช (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f57a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# ุฏุงุฏู\n",
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is bad\",\n",
    "    \"I hate this\",\n",
    "    \"This is great\",\n",
    "    \"I like it\",\n",
    "    \"It is terrible\"\n",
    "]\n",
    "labels = [1, 0, 0, 1, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# ุชูฺฉูุงุฒุฑ ุจุฑุง ุชุจุฏู ฺฉููุงุช ุจู ุงุนุฏุงุฏ\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# ุชุจุฏู ุฌููุงุช ุจู ุฏูุจุงูู ุงุนุฏุงุฏ\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# ุณุงุฎุช ูุฏู \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=padded.shape[1]),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.fit(padded, labels, epochs=10)\n",
    "\n",
    "# ุฌููู ุฌุฏุฏ ฺฉุงุฑุจุฑ\n",
    "new_sentence = [\"I love this product! It works really well\"]\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ุฌููู ุฌุฏุฏ\n",
    "seq = tokenizer.texts_to_sequences(new_sentence)\n",
    "padded_seq = pad_sequences(seq, maxlen=padded.shape[1], padding='post')\n",
    "\n",
    "# ูพุดโุจู ุงุญุณุงุณ ุฌููู\n",
    "prediction = model.predict(padded_seq)[0][0]\n",
    "\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {prediction:.3f}\")\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"Sentiment: Positive ๐\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative ๐\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8625a8",
   "metadata": {},
   "source": [
    "# ุดุจฺฉูโูุง LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72f35",
   "metadata": {},
   "source": [
    "## ฺฉุงุฑุจุฑุฏูุง LSTM ุฏุฑ ุชุนุงูู ุงุญุณุงุณ\n",
    "LSTM ุฏุฑ ุญูุฒู ุชุนุงูู ุงุญุณุงุณุงุช ุงูุณุงู ุจุง ููุด ูุตููุน ฺฉุงุฑุจุฑุฏูุง ูุชููุน ุฏุงุฑุฏ:\n",
    "- **ุชุญูู ุงุญุณุงุณุงุช ูุชู:** ุชุดุฎุต ุงุญุณุงุณุงุช (ุดุงุฏุ ุบูุ ุฎุดู) ุฏุฑ ูุชูู ฺฉุงุฑุจุฑุงู.\n",
    "- **ุชุดุฎุต ฺฏูุชุงุฑ ุงุญุณุงุณ:** ุชุญูู ุฒุฑูุจู ู ุขููฺฏ ฺฏูุชุงุฑ ุจุฑุง ุฏุฑฺฉ ุญุงูุช ุงุญุณุงุณ.\n",
    "- **ูุฏูโุณุงุฒ ูฺฉุงููุงุช:** ุญูุธ ุชุงุฑุฎฺู ูฺฉุงููู ุจุฑุง ูพุงุณุฎโูุง ูุชูุงุณุจ ุจุง ุงุญุณุงุณุงุช.\n",
    "- **ูพุดโุจู ุฑูุชุงุฑ ุงุญุณุงุณ:** ุงุณุชูุงุฏู ุงุฒ ุฏุงุฏูโูุง ุณุฑ ุฒูุงู ุจุฑุง ูพุดโุจู ุชุบุฑุงุช ุงุญุณุงุณ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_features = 10000  # ุชุนุฏุงุฏ ฺฉููุงุช ูพุฑฺฉุงุฑุจุฑุฏ\n",
    "maxlen = 200  # ุญุฏุงฺฉุซุฑ ุทูู ูุธุฑุงุช (ุชุนุฏุงุฏ ฺฉููุงุช)\n",
    "embedding_dim = 100  # ุงุจุนุงุฏ ุจุฑุฏุงุฑ ุฌุงุณุงุฒ\n",
    "\n",
    "# 1. ุจุงุฑฺฏุฐุงุฑ ุฏุชุงุณุช IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 2. ูพุดโูพุฑุฏุงุฒุด: ุงุณุชุงูุฏุงุฑุฏุณุงุฒ ุทูู ูุธุฑุงุช\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ูุฑูุช \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# 3. ุณุงุฎุช ูุฏู \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# ฺฉุงููพุงู ูุฏู\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู\n",
    "model.summary()\n",
    "\n",
    "# 4. ุขููุฒุด ูุฏู\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 5. ุงุฑุฒุงุจ ูุฏู\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'ุฏูุช ูุฏู ุฑู ุฏุงุฏูโูุง ุขุฒูุงุด: {test_accuracy:.4f}')\n",
    "\n",
    "# 6. ุฑุณู ูููุฏุงุฑ ุฏูุช ู ุฎุทุง\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# ูููุฏุงุฑ ุฏูุช\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='ุฏูุช ุขููุฒุด')\n",
    "plt.plot(history.history['val_accuracy'], label='ุฏูุช ุงุนุชุจุงุฑุณูุฌ')\n",
    "plt.title('ุฏูุช ูุฏู')\n",
    "plt.xlabel('ุฏูุฑู (Epoch)')\n",
    "plt.ylabel('ุฏูุช')\n",
    "plt.legend()\n",
    "\n",
    "# ูููุฏุงุฑ ุฎุทุง\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='ุฎุทุง ุขููุฒุด')\n",
    "plt.plot(history.history['val_loss'], label='ุฎุทุง ุงุนุชุจุงุฑุณูุฌ')\n",
    "plt.title('ุฎุทุง ูุฏู')\n",
    "plt.xlabel('ุฏูุฑู (Epoch)')\n",
    "plt.ylabel('ุฎุทุง')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3560eed",
   "metadata": {},
   "source": [
    "# ูุงุญุฏูุง ุจุงุฒฺฏุดุช ฺฏุชโุฏุงุฑ (Gated Recurrent Units - GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f98278",
   "metadata": {},
   "source": [
    "### ฺฉุงุฑุจุฑุฏ ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "\n",
    "**ูพุฑุฏุงุฒุด ูุชู ุชุฑุชุจ:**  \n",
    "ุฏุฑ ุชุญูู ุงุญุณุงุณุงุชุ ูุชู ุจู ุตูุฑุช ฺฉ ุชูุงู ุงุฒ ฺฉููุงุช ุง ุชูฺฉูโูุง ูพุฑุฏุงุฒุด ูโุดูุฏ.  \n",
    "**GRU** ุจุง ุชูุฌู ุจู ุชุฑุชุจ ฺฉููุงุชุ ูุนุงู ู ุฑูุงุจุท ุจู ุขูโูุง ุฑุง ุงุฏ ูโฺฏุฑุฏ.\n",
    "\n",
    "**ูุซุงู:**  \n",
    "ุจุฑุง ูุซุงูุ ุฏุฑ ุฌููู ุฒุฑ:  \n",
    "_\"ุงู ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ\"_\n",
    "\n",
    "GRU ูโุชูุงูุฏ ูุงุจุณุชฺฏ ุจู ุจุฎุดโูุง ูุฎุชูู ุฌููู ูุงููุฏ \"ุงุตูุงู ุฎูุจ ูุจูุฏ\" ู \"ุฎุณุชูโฺฉููุฏู\" ุฑุง ุฏุฑฺฉ ฺฉูุฏ ุชุง ุงุญุณุงุณ ููู ุฑุง ุชุดุฎุต ุฏูุฏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ูพุงุฑุงูุชุฑูุง\n",
    "max_words = 1000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช ุฏุฑ ูุงฺฺฏุงู\n",
    "max_len = 20      # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ูุชู\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ุขุฑุงู\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ุณุงุฎุช ูุฏู GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=max_len))  # ูุงู ุชุนุจูโุณุงุฒ\n",
    "model.add(GRU(64, return_sequences=False))  # ูุงู GRU ุจุง 64 ูุงุญุฏ\n",
    "model.add(Dense(1, activation='sigmoid'))   # ูุงู ุฎุฑูุฌ ุจุฑุง ุทุจููโุจูุฏ ุจุงูุฑ\n",
    "\n",
    "# ฺฉุงููพุงู ูุฏู\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู\n",
    "model.summary()\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ุชุณุช ูุฏู ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if prediction[0] > 0.5 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac0762",
   "metadata": {},
   "source": [
    "# ุชุฑุงูุณููุฑูุฑ (Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45b98e",
   "metadata": {},
   "source": [
    "### ุชุญูู ุงุญุณุงุณุงุช ูุชู\n",
    "\n",
    "ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจู **ุดูุงุณุง ุงุญุณุงุณุงุช** ุดุงูู:\n",
    "- **ูุซุจุช**\n",
    "- **ููู**\n",
    "- **ุฎูุซ**\n",
    "\n",
    "ุฏุฑ ูุชูโูุง ูุงููุฏ:\n",
    "- ูุธุฑุงุช ฺฉุงุฑุจุฑุงู\n",
    "- ููุฏูุง\n",
    "- ูพุณุชโูุง ุดุจฺฉูโูุง ุงุฌุชูุงุน\n",
    "\n",
    "ูโูพุฑุฏุงุฒุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "**ูุฏูโูุง ุชุฑุงูุณููุฑูุฑ** ุจู ุฏูู ุชูุงูุง ุจโูุธุฑุดุงู ุฏุฑ:\n",
    "- ุฏุฑฺฉ ุฒููู ู ุฒูููโูุง ูุนูุง ูพฺุฏู\n",
    "- ููู ุฑูุงุจุท ุจููุฏูุฏุช ุจู ฺฉููุงุช\n",
    "\n",
    "ุฏุฑ ุงู ุญูุฒู ุจุณุงุฑ ูุคุซุฑ ูุณุชูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "ุงู ุชุฑฺฉุจุ ุชุญูู ุงุญุณุงุณุงุช ุฑุง ุณุฑุนโุชุฑุ ุฏููโุชุฑ ู ฺฉุงุฑุขูุฏุชุฑ ูโฺฉูุฏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ุจุงุฑฺฏุฐุงุฑ ุชูฺฉูุงุฒุฑ ู ูุฏู BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ุฏุงุฏูโูุง\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ุจุฑุง ุขููุฒุด\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# ุชูุธู ูุฏู ุจุฑุง ุขููุฒุด\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 ุฏูุฑู ุจุฑุง ูุซุงู\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# ุชุณุช ูุฏู ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_inputs = tokenizer(test_text, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "\n",
    "# ูพุดโุจู\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'])\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if predictions[0] == 1 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4cf31",
   "metadata": {},
   "source": [
    "# ุงุชูุงูฺฉูุฏุฑูุง (Autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926b26b",
   "metadata": {},
   "source": [
    "\n",
    "ุชูุงูฺฉูุฏุฑูุง  ููุน ุงุฒ ุดุจฺฉูโูุง ุนุตุจ ูุตููุน ูุณุชูุฏ ฺฉู ุจุฑุง **ุงุฏฺฏุฑ ููุงุด ูุดุฑุฏู ุฏุงุฏูโูุง** ุจู ุตูุฑุช **ุจุฏูู ูุธุงุฑุช**  ุทุฑุงุญ ุดุฏูโุงูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ุชูุถุญุงุช\n",
    "\n",
    "ุฏุฑ ุงู ุจุฎุดุ ุจู ููุงุฑุฏ ุฒุฑ ูพุฑุฏุงุฎุชู ูโุดูุฏ:\n",
    "- **ฺุณุช ุชูุงูฺฉูุฏุฑูุง**  \n",
    "- **ฺฺฏููฺฏ ุนููฺฉุฑุฏ ุขูโูุง**  \n",
    "- **ฺฉุงุฑุจุฑุฏุดุงู ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช ูุชู**\n",
    "\n",
    "ููฺููุ ฺฉ **ูุซุงู ุณุงุฏู** ุงุฒ ูพุงุฏูโุณุงุฒ ุชูุงูฺฉูุฏุฑ ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช ูุชู ุงุฑุงุฆู ูโฺฏุฑุฏุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "## ฺฉุงุฑุจุฑุฏ ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "\n",
    "ุชูุงูฺฉูุฏุฑูุง ูโุชูุงููุฏ ุจุฑุง ฺฉุงูุด ุงุจุนุงุฏ ู ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ููู ุงุฒ ูุชูโูุง ุจุฒุฑฺฏ ู ูพฺุฏู ุงุณุชูุงุฏู ุดููุฏุ ฺฉู ุงู ฺฉุงุฑ ุจู ุจูุจูุฏ ูุฑุขูุฏ ุชุญูู ุงุญุณุงุณุงุช ฺฉูฺฉ ูโฺฉูุฏ.\n",
    "\n",
    "---\n",
    "\n",
    "**ุฏุฑ ุงุฏุงููุ ูุซุงู ุนูู ู ฺฉุฏ ูููููโุง ุจุฑุง ุงุณุชูุงุฏู ุงุฒ ุชูุงูฺฉูุฏุฑ ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช ูุชู ุขูุฑุฏู ุดุฏู ุงุณุช.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ูุชู\n",
    "max_words = 1000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช\n",
    "max_len = 20      # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ุชุจุฏู ุฏุงุฏูโูุง ุจู ุขุฑุงู\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# ุณุงุฎุช ุงุชูุงูฺฉูุฏุฑ\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 50, input_length=max_len)(input_layer)\n",
    "encoded = LSTM(64, return_sequences=False)(embedding)  # ูุงู ุงูฺฉูุฏุฑ\n",
    "decoded = Dense(max_len * 50, activation='relu')(encoded)  # ูุงู ุฏฺฉูุฏุฑ\n",
    "decoded = Reshape((max_len, 50))(decoded)\n",
    "\n",
    "# ูุฏู ุงุชูุงูฺฉูุฏุฑ\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# ูุฏู ุทุจููโุจูุฏ (ุงุณุชูุงุฏู ุงุฒ ููุงุด ูุดุฑุฏู ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "classification_model = Model(input_layer, classification_output)\n",
    "classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู ุทุจููโุจูุฏ\n",
    "classification_model.summary()\n",
    "\n",
    "# ุขููุฒุด ุงุชูุงูฺฉูุฏุฑ\n",
    "autoencoder.fit(X, X.reshape(X.shape[0], max_len, 50), epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ุขููุฒุด ูุฏู ุทุจููโุจูุฏ\n",
    "classification_model.fit(X, y, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ุชุณุช ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = classification_model.predict(test_padded)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if prediction[0] > 0.5 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b5dc5",
   "metadata": {},
   "source": [
    "# ุดุจฺฉูโูุง ุจุงูุฑ ุนูู (Deep Belief Networks - DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05279512",
   "metadata": {},
   "source": [
    "### ุชุญูู ุงุญุณุงุณุงุช ูุชู ู ููุด DBNโูุง\n",
    "\n",
    "ุชุญูู ุงุญุณุงุณุงุช ูุชู ุจู **ุดูุงุณุง ุงุญุณุงุณุงุช** ุดุงูู ููุงุฑุฏ ุฒุฑ ูโูพุฑุฏุงุฒุฏ:\n",
    "- **ูุซุจุช**\n",
    "- **ููู**\n",
    "- **ุฎูุซ**\n",
    "\n",
    "ุฏุฑ ูุชูโูุง ูุงููุฏ:\n",
    "- ูุธุฑุงุช ฺฉุงุฑุจุฑุงู\n",
    "- ููุฏูุง\n",
    "- ูพุณุชโูุง ุดุจฺฉูโูุง ุงุฌุชูุงุน\n",
    "\n",
    "---\n",
    "\n",
    "## ููุด DBNโูุง ุฏุฑ ุงู ุญูุฒู\n",
    "\n",
    "DBNโูุง (Deep Belief Networks) ุฏุฑ ุงู ุญูุฒู ุจู ุฑูุดโูุง ุฒุฑ ุงุณุชูุงุฏู ูโุดููุฏ:\n",
    "- **[ุฏุฑ ุงู ูุณูุชุ ูุณุช ุงุฒ ุฑูุดโูุง ุง ฺฉุงุฑุจุฑุฏูุง ุฎุงุต ฺฉู DBNโูุง ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช ููุฑุฏ ุงุณุชูุงุฏู ูุฑุงุฑ ูโฺฏุฑูุฏุ ุขูุฑุฏู ูโุดูุฏ.]**\n",
    "\n",
    "---\n",
    "\n",
    "ุงู ุงุจุฒุงุฑูุง ฺฉูฺฉ ูโฺฉููุฏ ุชุง ุชุญูู ุงุญุณุงุณุงุช ุฏููโุชุฑ ู ฺฉุงุฑุขูุฏุชุฑ ุงูุฌุงู ุดูุฏุ ุจู ูฺู ุฏุฑ ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุญุฌู ู ูพฺุฏู."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ุฏุงุฏูโูุง ููููู\n",
    "texts = [\n",
    "    \"ุงู ููู ูุงูุนุงู ุนุงู ุจูุฏุ ุฎู ูุฐุช ุจุฑุฏู!\",\n",
    "    \"ููู ุงุตูุงู ุฎูุจ ูุจูุฏุ ุฎู ุฎุณุชูโฺฉููุฏู ุจูุฏ.\",\n",
    "    \"ุฏุงุณุชุงู ููู ูููโุงูุนุงุฏู ุจูุฏ ู ุจุงุฒฺฏุฑุงู ุนุงู ุจูุฏูุฏ.\",\n",
    "    \"ฺฉ ุงุฒ ุจุฏุชุฑู ูููโูุง ฺฉู ุฏุฏูุ ุงูุชุถุงุญ ุจูุฏ.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: ูุซุจุชุ 0: ููู\n",
    "\n",
    "# ูพุดโูพุฑุฏุงุฒุด ูุชู ุจุง TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# ุณุงุฎุช ูุฏู DBN-ูุงููุฏ (ุดุจฺฉู ุนูู ุจุง ูุงูโูุง Dense)\n",
    "model = Sequential()\n",
    "# ูุงูโูุง ูุฎู ูุดุงุจู RBM ุจุฑุง ุงุฏฺฏุฑ ูฺฺฏโูุง\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# ูุงู ุฎุฑูุฌ ุจุฑุง ุทุจููโุจูุฏ\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ฺฉุงููพุงู ูุฏู\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ููุงุด ุณุงุฎุชุงุฑ ูุฏู\n",
    "model.summary()\n",
    "\n",
    "# ุขููุฒุด ูุฏู\n",
    "model.fit(X, y, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "# ุชุณุช ุฑู ฺฉ ููููู ุฌุฏุฏ\n",
    "test_text = [\"ุงู ููู ุฎู ุฎูุจ ุจูุฏ ู ูู ุนุงุดูุด ุดุฏู!\"]\n",
    "test_X = vectorizer.transform(test_text).toarray()\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "# ูุชุฌู\n",
    "print(\"ุงุญุณุงุณ ูพุดโุจูโุดุฏู:\", \"ูุซุจุช\" if prediction[0] > 0.5 else \"ููู\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c1c1a",
   "metadata": {},
   "source": [
    "### ุฌูุนโุจูุฏ\n",
    "\n",
    "ุฏุฑ ุงู ูุณูุชุ ุจุง ุชูุงู ุงูฺฏูุฑุชูโูุง ฺฉู ุงุฒ ุจุฎุด **ุงุฏฺฏุฑ ุนูู ** ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช ูุชู ุงุณุชูุงุฏู ูโุดููุฏุ ุขุดูุง ุดุฏู."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
