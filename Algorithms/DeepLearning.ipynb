{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77945b74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# سفر یادگیری: کشف معماری‌های یادگیری عمیق 🚀🧠\n",
    "\n",
    "**بیایید با هم سه معماری اساسی یادگیری عمیق را کشف کنیم:**\n",
    "\n",
    "## 1. <span style=\"color: #4d90fe;\">CNN (شبکه عصبی کانولوشنی)</span> 🖼️\n",
    "- **مغز بینایی ماشین** 👁️\n",
    "- از فیلترهای کانولوشنی برای تشخیص الگوهای محلی استفاده می‌کند\n",
    "- ایده‌آل برای پردازش تصاویر و ویدیو\n",
    "- مثال: تشخیص اشیا در عکس‌ها\n",
    "\n",
    "## 2. <span style=\"color: #34a853;\">RNN (شبکه عصبی بازگشتی)</span> 🔄\n",
    "- **حافظه کوتاه‌مدت برای داده‌های متوالی** 📜\n",
    "- می‌تواند اطلاعات را از مراحل قبلی به خاطر بسپارد\n",
    "- مناسب برای متن، گفتار و داده‌های زمانی\n",
    "- مثال: پیش‌بینی کلمه بعدی در جمله\n",
    "\n",
    "## 3. <span style=\"color: #ea4335;\">Transformer</span> ⚡\n",
    "- **انقلابی در پردازش زبان** 💬\n",
    "- از مکانیزم توجه (Attention) استفاده می‌کند\n",
    "- می‌تواند روابط بلندمدت را در داده‌ها تشخیص دهد\n",
    "- مثال: ترجمه ماشینی، مدل‌های گفتگو\n",
    "\n",
    "📌 **نکته طلایی:**  \n",
    "این معماری‌ها اغلب با هم ترکیب می‌شوند تا سیستم‌های هوشمند قدرتمندی بسازند!\n",
    "\n",
    "🎯 **هدف ما در این مسیر:**  \n",
    "یادگیری اصول هر معماری + درک کاربردهای عملی + پیاده‌سازی ساده\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664615",
   "metadata": {},
   "source": [
    "# یادگیری عمیق: موتور محرک هوش مصنوعی مدرن 🌟🧠\n",
    "\n",
    "**یادگیری عمیق**، پیشرفته‌ترین شاخه یادگیری ماشین است که با الهام از ساختار مغز انسان، انقلابی در پردازش داده‌ها ایجاد کرده است. این فناوری با معماری‌های هوشمندانه خود قادر به یادگیری سلسله‌مراتبی ویژگی‌ها از داده‌های خام است:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa30d",
   "metadata": {},
   "source": [
    "# 🧩 CNN (شبکه عصبی کانولوشنی)\n",
    "Convolutional Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ چگونه کار می‌کند؟\n",
    "\n",
    "**شبکه عصبی کانولوشنی (CNN)** یک مدل بسیار قدرتمند برای پردازش داده‌های تصویری و فضایی است.\n",
    "\n",
    "- هسته اصلی CNN، **لایه‌های کانولوشن (Convolutional Layers)** هستند که با فیلترهای کوچک (Kernel) از روی تصویر عبور می‌کنند و ویژگی‌های مهم را شناسایی می‌کنند (مثل لبه‌ها، بافت‌ها و اشیاء).\n",
    "- این ویژگی‌ها مرحله‌به‌مرحله ترکیب و انتزاع می‌شوند تا مدل بتواند حتی الگوهای پیچیده را شناسایی کند.\n",
    "- **لایه‌های Pooling** برای کاهش حجم داده و جلوگیری از یادگیری بیش‌ازحد (Overfitting) به کار می‌روند.\n",
    "- در انتها، **لایه‌های Fully Connected** تصمیم نهایی را درباره دسته‌بندی یا تشخیص می‌گیرند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 چرا CNN؟\n",
    "\n",
    "- **یادگیری خودکار ویژگی‌ها** بدون نیاز به استخراج دستی\n",
    "- **مقیاس‌پذیری و دقت بالا** برای داده‌های تصویری و فضایی\n",
    "- **مقاومت به نویز** و تغییرات جزئی در ورودی (مانند جابجایی یا چرخش)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 کاربردهای مهم CNN\n",
    "\n",
    "- **شناسایی و دسته‌بندی تصاویر** (مانند تشخیص گربه/سگ، چهره و ...)\n",
    "- **تشخیص اشیاء در تصاویر و ویدیوها**\n",
    "- **تحلیل تصاویر پزشکی** (مانند تشخیص بیماری از رادیولوژی یا MRI)\n",
    "- **پردازش تصویر و بینایی ماشین** (Computer Vision)\n",
    "- **تحلیل دست‌خط و اعداد نوشته‌شده**\n",
    "- **خودروهای خودران، رباتیک و سیستم‌های امنیتی**\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 خلاصه در یک جمله:\n",
    "\n",
    "> **CNNها با یادگیری خودکار ویژگی‌های تصویری از داده خام، یکی از قدرتمندترین الگوریتم‌ها برای تحلیل تصاویر و سیگنال‌های فضایی هستند.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# کتابخانه‌های مورد نیاز\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# مرحله 1: آماده‌سازی داده‌های متنی\n",
    "# هدف: تبدیل متن به بردارهای عددی و آماده‌سازی برای ورودی شبکه\n",
    "def prepare_text_data(texts, labels, max_words=10000, max_len=100):\n",
    "    # توکن‌سازی: تبدیل کلمات به توکن‌های عددی\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # تبدیل متن به دنباله‌های عددی\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # پد کردن دنباله‌ها برای یکسان‌سازی طول\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    # تبدیل برچسب‌ها به آرایه\n",
    "    labels = np.array(labels)\n",
    "    return data, labels, tokenizer\n",
    "\n",
    "# مرحله 2: تعریف معماری شبکه کانولوشنی\n",
    "# هدف: ایجاد یک مدل CNN برای استخراج ویژگی‌های متنی و طبقه‌بندی احساسات\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = models.Sequential([\n",
    "        # لایه تعبیه: تبدیل توکن‌ها به بردارهای متراکم\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        # لایه کانولوشنی اول: استخراج ویژگی‌های محلی از متن\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        # لایه پولینگ: کاهش ابعاد و حفظ ویژگی‌های مهم\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # لایه کانولوشنی دوم: استخراج ویژگی‌های پیچیده‌تر\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # مسطح‌سازی: آماده‌سازی برای لایه‌های متراکم\n",
    "        layers.Flatten(),\n",
    "        # لایه کاملاً متصل: ترکیب ویژگی‌ها\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # لایه خروجی: طبقه‌بندی دودویی (مثبت/منفی)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# مرحله 3: آموزش و ارزیابی مدل\n",
    "# هدف: آموزش مدل و ارزیابی عملکرد آن در تحلیل احساسات\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # کامپایل مدل با بهینه‌ساز و تابع هزینه\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # آموزش مدل\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "    # ارزیابی مدل\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'دقت تست: {test_acc:.4f}')\n",
    "    return test_acc, test_loss\n",
    "\n",
    "# الگوریتم اصلی\n",
    "def main():\n",
    "    # نمونه داده‌ها\n",
    "    # فرض\n",
    "    texts = [\"فیلم عالی بود و خیلی لذت بردم\", \"این بدترین تجربه من بود\", ...]  # داده‌های متنی\n",
    "    labels = [1, 0, ...]  # برچسب‌ها\n",
    "    max_words = 10000  # حداکثر تعداد کلمات در واژگان\n",
    "    max_len = 100  # حداکثر طول دنباله\n",
    "\n",
    "    # آماده‌سازی داده‌ها\n",
    "    x_data, y_data, tokenizer = prepare_text_data(texts, labels, max_words, max_len)\n",
    "    # تقسیم داده‌ها به آموزشی و تست (فرض: 80% آموزشی، 20% تست)\n",
    "    train_size = int(0.8 * len(x_data))\n",
    "    x_train, y_train = x_data[:train_size], y_data[:train_size]\n",
    "    x_test, y_test = x_data[train_size:], y_data[train_size:]\n",
    "\n",
    "    # تعریف مدل\n",
    "    model = build_cnn_model(max_words, max_len)\n",
    "    # نمایش خلاصه مدل\n",
    "    model.summary()\n",
    "    # آموزش و ارزیابی\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# اجرای برنامه\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cf4ac",
   "metadata": {},
   "source": [
    "# 🔄 شبکه‌های عصبی بازگشتی (RNN)\n",
    "Recurrent Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ چگونه کار می‌کند؟\n",
    "\n",
    "**شبکه‌های عصبی بازگشتی (RNN)** نوعی شبکه عصبی هستند که برای پردازش داده‌های ترتیبی (Sequence Data) مثل متن، گفتار یا سری زمانی طراحی شده‌اند.\n",
    "\n",
    "- در RNN، خروجی هر گام به عنوان ورودی به گام بعدی داده می‌شود؛  \n",
    "  یعنی حافظه‌ای از وضعیت‌های قبلی در مدل وجود دارد.\n",
    "- این ساختار اجازه می‌دهد تا مدل بتواند **وابستگی‌های زمانی** و **ارتباط بین داده‌ها** را در دنباله حفظ کند.\n",
    "\n",
    "> RNNها برخلاف شبکه‌های عصبی معمولی (Feedforward)، می‌توانند اطلاعات گذشته را در خود نگه دارند و با توجه به آنها تصمیم بگیرند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 چرا RNN؟\n",
    "\n",
    "- **درک ترتیب و زمینه:**  \n",
    "  برای تحلیل متن، صدا، موسیقی، داده‌های سنسور یا هر چیزی که ترتیب مهم باشد.\n",
    "- **پردازش داده‌هایی با طول متغیر:**  \n",
    "  مانند جملات کوتاه و بلند، یا زمان‌های مختلف در سری‌های زمانی.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌍 کاربردهای مهم\n",
    "\n",
    "- **مدل‌سازی زبان طبیعی (NLP):**  \n",
    "  تولید و ترجمه متن، خلاصه‌سازی خودکار و چت‌بات‌ها\n",
    "- **تحلیل احساسات و لحن در جملات:**  \n",
    "  درک معنای پنهان پشت متن و پیش‌بینی احساس یا قصد گوینده\n",
    "- **تشخیص گفتار و تبدیل صوت به متن**\n",
    "- **پیش‌بینی سری‌های زمانی:**  \n",
    "  مانند قیمت سهام، آب‌وهوا، یا داده‌های پزشکی\n",
    "- **تشخیص فعالیت و رفتار کاربر از داده‌های متوالی**\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 خلاصه در یک جمله:\n",
    "\n",
    "> **RNNها با داشتن حافظه داخلی، می‌توانند وابستگی‌های زمانی و معنایی را در داده‌های دنباله‌دار درک و مدل‌سازی کنند.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f57a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# داده\n",
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is bad\",\n",
    "    \"I hate this\",\n",
    "    \"This is great\",\n",
    "    \"I like it\",\n",
    "    \"It is terrible\"\n",
    "]\n",
    "labels = [1, 0, 0, 1, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# توکنایزر برای تبدیل کلمات به اعداد\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# تبدیل جملات به دنباله اعداد\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# ساخت مدل \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=padded.shape[1]),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# آموزش مدل\n",
    "model.fit(padded, labels, epochs=10)\n",
    "\n",
    "# جمله جدید کاربر\n",
    "new_sentence = [\"I love this product! It works really well\"]\n",
    "\n",
    "# پیش‌پردازش جمله جدید\n",
    "seq = tokenizer.texts_to_sequences(new_sentence)\n",
    "padded_seq = pad_sequences(seq, maxlen=padded.shape[1], padding='post')\n",
    "\n",
    "# پیش‌بینی احساس جمله\n",
    "prediction = model.predict(padded_seq)[0][0]\n",
    "\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {prediction:.3f}\")\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"Sentiment: Positive 😊\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative 😞\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8625a8",
   "metadata": {},
   "source": [
    "# شبکه‌های LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72f35",
   "metadata": {},
   "source": [
    "# 💛 LSTM (حافظه بلندمدت کوتاه - Long Short-Term Memory)\n",
    "کاربردهای LSTM در تعامل احساسی\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ چگونه کار می‌کند؟\n",
    "\n",
    "**LSTM** نوعی شبکه عصبی بازگشتی (RNN) پیشرفته است که برای یادگیری وابستگی‌های طولانی‌مدت در داده‌های ترتیبی (Sequence Data) طراحی شده و مشکل فراموشی حافظه کوتاه‌مدت را در RNNهای معمولی برطرف می‌کند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 کاربردهای مهم LSTM در تعامل احساسی\n",
    "\n",
    "- **تحلیل احساسات متنی:**  \n",
    "  شناسایی و دسته‌بندی احساسات (شادی، غم، خشم و...) در پیام‌ها، شبکه‌های اجتماعی یا نظرات کاربران.\n",
    "- **تشخیص گفتار احساسی:**  \n",
    "  تحلیل ویژگی‌هایی مثل زیروبمی، سرعت و ریتم گفتار برای تشخیص حالات احساسی گوینده.\n",
    "- **مدل‌سازی مکالمات احساسی:**  \n",
    "  حفظ تاریخچه و زمینه مکالمات برای ارائه پاسخ‌های هوشمند و همدلانه در چت‌بات‌ها و دستیارهای صوتی.\n",
    "- **پیش‌بینی رفتار یا تغییرات احساسی:**  \n",
    "  تحلیل داده‌های سری زمانی (مانند فعالیت کاربران یا حالات روزانه) برای پیش‌بینی نوسانات یا تغییر احساسات.\n",
    "- **درک زیرمتن و لحن غیرمستقیم:**  \n",
    "  شناسایی احساسات پنهان یا لحن غیرمستقیم در پیام‌ها با توجه به ترتیب و زمینه جملات.\n",
    "- **کاربرد در سلامت روان و مشاوره هوشمند:**  \n",
    "  رصد حالات روحی بیماران و پیشنهاد مداخلات مبتنی بر تحلیل الگوهای احساسی در طول زمان.\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 خلاصه در یک جمله:\n",
    "\n",
    "> **LSTMها با قابلیت یادگیری روابط و وابستگی‌های بلندمدت در داده‌های ترتیبی، یکی از قدرتمندترین ابزارها برای تحلیل، درک و پیش‌بینی تعاملات احساسی انسان و ماشین به شمار می‌روند.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_features = 10000  # تعداد کلمات پرکاربرد\n",
    "maxlen = 200  # حداکثر طول نظرات (تعداد کلمات)\n",
    "embedding_dim = 100  # ابعاد بردار جاسازی\n",
    "\n",
    "# 1. بارگذاری دیتاست IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 2. پیش‌پردازش: استانداردسازی طول نظرات\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# تبدیل برچسب‌ها به فرمت \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# 3. ساخت مدل \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# کامپایل مدل\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل\n",
    "model.summary()\n",
    "\n",
    "# 4. آموزش مدل\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 5. ارزیابی مدل\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'دقت مدل روی داده‌های آزمایشی: {test_accuracy:.4f}')\n",
    "\n",
    "# 6. رسم نمودار دقت و خطا\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# نمودار دقت\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='دقت آموزش')\n",
    "plt.plot(history.history['val_accuracy'], label='دقت اعتبارسنجی')\n",
    "plt.title('دقت مدل')\n",
    "plt.xlabel('دوره (Epoch)')\n",
    "plt.ylabel('دقت')\n",
    "plt.legend()\n",
    "\n",
    "# نمودار خطا\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='خطای آموزش')\n",
    "plt.plot(history.history['val_loss'], label='خطای اعتبارسنجی')\n",
    "plt.title('خطای مدل')\n",
    "plt.xlabel('دوره (Epoch)')\n",
    "plt.ylabel('خطا')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3560eed",
   "metadata": {},
   "source": [
    "# واحدهای بازگشتی گیت‌دار (Gated Recurrent Units - GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f98278",
   "metadata": {},
   "source": [
    "### 💬 کاربرد GRU در تحلیل احساسات\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ پردازش متن ترتیبی\n",
    "\n",
    "در تحلیل احساسات، متن به عنوان یک توالی از کلمات یا توکن‌ها پردازش می‌شود.  \n",
    "**GRU** (Gated Recurrent Unit) با توجه به **ترتیب کلمات**، می‌تواند معانی، روابط و وابستگی‌های معنایی بین آن‌ها را به خوبی یاد بگیرد.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 مثال کاربردی\n",
    "\n",
    "در جمله زیر:\n",
    "> _«این فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود»_\n",
    "\n",
    "GRU قادر است ارتباط بین عبارات «اصلاً خوب نبود» و «خسته‌کننده» را شناسایی کند  \n",
    "و **احساس منفی** پشت این جمله را به درستی تشخیص دهد؛  \n",
    "حتی اگر نشانه‌های منفی‌بودن، پخش یا غیرمستقیم بیان شده باشند.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 نکته:\n",
    "\n",
    "GRU با ساختار ساده‌تر نسبت به LSTM، همچنان قدرت یادگیری وابستگی‌های معنایی در متن‌های بلند و پیچیده را دارد و برای تحلیل احساسات یکی از گزینه‌های محبوب در یادگیری عمیق است.\n",
    "\n",
    "---\n",
    "\n",
    "> **خلاصه:**  \n",
    "> **GRU با درک ترتیب و معنای کلمات، تحلیل احساسات دقیق‌تری را در متون فارسی و انگلیسی ممکن می‌کند.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# پارامترها\n",
    "max_words = 1000  # حداکثر تعداد کلمات در واژگان\n",
    "max_len = 20      # حداکثر طول دنباله\n",
    "\n",
    "# پیش‌پردازش متن\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# تبدیل برچسب‌ها به آرایه\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ساخت مدل GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=max_len))  # لایه تعبیه‌سازی\n",
    "model.add(GRU(64, return_sequences=False))  # لایه GRU با 64 واحد\n",
    "model.add(Dense(1, activation='sigmoid'))   # لایه خروجی برای طبقه‌بندی باینری\n",
    "\n",
    "# کامپایل مدل\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل\n",
    "model.summary()\n",
    "\n",
    "# آموزش مدل\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# تست مدل روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if prediction[0] > 0.5 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac0762",
   "metadata": {},
   "source": [
    "# ترانسفورمر (Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45b98e",
   "metadata": {},
   "source": [
    "### 💬 تحلیل احساسات متنی با مدل‌های ترانسفورمر\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 هدف تحلیل احساسات متنی\n",
    "\n",
    "**تحلیل احساسات متنی** به شناسایی و طبقه‌بندی احساسات زیر در متن می‌پردازد:\n",
    "- **مثبت**\n",
    "- **منفی**\n",
    "- **خنثی**\n",
    "\n",
    "در متونی مانند:\n",
    "- نظرات کاربران\n",
    "- نقد فیلم یا محصول\n",
    "- پست‌های شبکه‌های اجتماعی\n",
    "- پیام‌های کاربران\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 چرا مدل‌های ترانسفورمر (Transformers)؟\n",
    "\n",
    "**مدل‌های ترانسفورمر** به دلیل توانایی فوق‌العاده‌شان در:\n",
    "- درک عمیق زمینه و روابط معنایی پیچیده بین کلمات\n",
    "- فهم وابستگی‌های بلندمدت در جمله و پاراگراف\n",
    "\n",
    "در تحلیل احساسات متنی، بسیار مؤثر و دقیق هستند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 نتیجه نهایی\n",
    "\n",
    "این ترکیب (تحلیل احساسات + ترانسفورمرها)  \n",
    "فرآیند شناسایی احساسات را **سریع‌تر، دقیق‌تر و کارآمدتر** از مدل‌های سنتی می‌کند.\n",
    "\n",
    "---\n",
    "\n",
    "> **خلاصه:**  \n",
    "> **مدل‌های ترانسفورمر، ابزاری بسیار قوی برای تحلیل احساسات متنی با دقت و سرعت بالا در متون پیچیده و بلند هستند.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# بارگذاری توکنایزر و مدل BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# پیش‌پردازش داده‌ها\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# آماده‌سازی داده‌ها برای آموزش\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# تنظیم مدل برای آموزش\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# آموزش مدل\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 دوره برای مثال\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# تست مدل روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_inputs = tokenizer(test_text, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "\n",
    "# پیش‌بینی\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'])\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if predictions[0] == 1 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4cf31",
   "metadata": {},
   "source": [
    "# اتوانکودرها (Autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926b26b",
   "metadata": {},
   "source": [
    "# 🌀 توانکودرها (Autoencoders)\n",
    "شبکه‌های عصبی برای یادگیری فشرده‌سازی داده‌ها\n",
    "\n",
    "---\n",
    "\n",
    "## 👀 چیستی توانکودرها\n",
    "\n",
    "**توانکودرها** نوعی شبکه عصبی مصنوعی هستند که برای **یادگیری نمایش فشرده داده‌ها** (Representation Learning) به صورت **بدون نظارت** (Unsupervised) طراحی شده‌اند.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ چگونه کار می‌کنند؟\n",
    "\n",
    "- داده‌های ورودی (مثلاً متن یا تصویر) وارد شبکه می‌شوند.\n",
    "- **بخش رمزگذار (Encoder)** داده‌ها را به یک نمایش فشرده (کُد) تبدیل می‌کند.\n",
    "- **بخش رمزگشا (Decoder)** تلاش می‌کند داده اصلی را از این کد فشرده بازسازی کند.\n",
    "- شبکه، به تدریج یاد می‌گیرد مهم‌ترین ویژگی‌های داده را به صورت فشرده حفظ کند.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 کاربرد توانکودرها در تحلیل احساسات\n",
    "\n",
    "- **کاهش ابعاد داده‌های متنی بزرگ و پیچیده**\n",
    "- **استخراج ویژگی‌های مهم و معنی‌دار از متن**\n",
    "- بهبود عملکرد مدل‌های یادگیری ماشین در **تحلیل احساسات** با ارائه نمایشی فشرده و بهینه از داده‌ها\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 مثال عملی (نمونه کد)\n",
    "\n",
    "در ادامه، یک مثال ساده از پیاده‌سازی توانکودر برای استخراج ویژگی‌های متنی  \n",
    "و استفاده در تحلیل احساسات ارائه خواهد شد.\n",
    "\n",
    "---\n",
    "\n",
    "> **خلاصه:**  \n",
    "> **توانکودرها با یادگیری نمایش فشرده داده‌ها، ابزاری قدرتمند برای پیش‌پردازش، کاهش ابعاد و افزایش دقت مدل‌های تحلیل احساسات متنی هستند.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# پیش‌پردازش متن\n",
    "max_words = 1000  # حداکثر تعداد کلمات\n",
    "max_len = 20      # حداکثر طول دنباله\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# تبدیل داده‌ها به آرایه\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# ساخت اتوانکودر\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 50, input_length=max_len)(input_layer)\n",
    "encoded = LSTM(64, return_sequences=False)(embedding)  # لایه انکودر\n",
    "decoded = Dense(max_len * 50, activation='relu')(encoded)  # لایه دیکودر\n",
    "decoded = Reshape((max_len, 50))(decoded)\n",
    "\n",
    "# مدل اتوانکودر\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# مدل طبقه‌بندی (استفاده از نمایش فشرده برای تحلیل احساسات)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "classification_model = Model(input_layer, classification_output)\n",
    "classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل طبقه‌بندی\n",
    "classification_model.summary()\n",
    "\n",
    "# آموزش اتوانکودر\n",
    "autoencoder.fit(X, X.reshape(X.shape[0], max_len, 50), epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# آموزش مدل طبقه‌بندی\n",
    "classification_model.fit(X, y, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# تست روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = classification_model.predict(test_padded)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if prediction[0] > 0.5 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b5dc5",
   "metadata": {},
   "source": [
    "# شبکه‌های باور عمیق (Deep Belief Networks - DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05279512",
   "metadata": {},
   "source": [
    "### 💬 تحلیل احساسات متنی و نقش DBNها\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 هدف تحلیل احساسات متنی\n",
    "\n",
    "تحلیل احساسات متنی به **شناسایی احساسات** زیر در متن می‌پردازد:\n",
    "- **مثبت**\n",
    "- **منفی**\n",
    "- **خنثی**\n",
    "\n",
    "در متونی مانند:\n",
    "- نظرات کاربران\n",
    "- نقدها\n",
    "- پست‌های شبکه‌های اجتماعی\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 نقش DBNها (Deep Belief Networks) در این حوزه\n",
    "\n",
    "DBNها در تحلیل احساسات متنی برای موارد زیر استفاده می‌شوند:\n",
    "- **استخراج ویژگی‌های عمیق و غیرخطی** از داده‌های متنی  \n",
    "  (یادگیری خودکار الگوهای پنهان و روابط پیچیده بین واژه‌ها و عبارات)\n",
    "- **کاهش ابعاد داده‌ها** و حذف نویز برای بهبود عملکرد مدل‌های طبقه‌بندی احساسات\n",
    "- **یادگیری بازنمایی بهینه از متن** جهت انتقال به مدل‌های طبقه‌بندی (مانند Softmax یا SVM)\n",
    "- **پیش‌پردازش و فشرده‌سازی داده‌های حجیم** برای افزایش سرعت و دقت تحلیل احساسات\n",
    "- **بهبود عملکرد در داده‌های پیچیده و چندلایه** مانند جملات طولانی، زبان غیرمستقیم یا طنز\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 جمع‌بندی\n",
    "\n",
    "این ابزارها کمک می‌کنند تا تحلیل احساسات **دقیق‌تر** و **کارآمدتر** انجام شود،  \n",
    "به‌ویژه در پردازش داده‌های حجیم و متون پیچیده و چندمعنایی.\n",
    "\n",
    "---\n",
    "\n",
    "> **خلاصه:**  \n",
    "> **DBNها با یادگیری خودکار ویژگی‌های عمیق، تحلیل احساسات متنی را در متون بزرگ و پیچیده بسیار تقویت می‌کنند.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# داده‌های نمونه\n",
    "texts = [\n",
    "    \"این فیلم واقعاً عالی بود، خیلی لذت بردم!\",\n",
    "    \"فیلم اصلاً خوب نبود، خیلی خسته‌کننده بود.\",\n",
    "    \"داستان فیلم فوق‌العاده بود و بازیگران عالی بودند.\",\n",
    "    \"یکی از بدترین فیلم‌هایی که دیدم، افتضاح بود.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: مثبت، 0: منفی\n",
    "\n",
    "# پیش‌پردازش متن با TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# ساخت مدل DBN-مانند (شبکه عمیق با لایه‌های Dense)\n",
    "model = Sequential()\n",
    "# لایه‌های مخفی مشابه RBM برای یادگیری ویژگی‌ها\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# لایه خروجی برای طبقه‌بندی\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# کامپایل مدل\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# نمایش ساختار مدل\n",
    "model.summary()\n",
    "\n",
    "# آموزش مدل\n",
    "model.fit(X, y, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "# تست روی یک نمونه جدید\n",
    "test_text = [\"این فیلم خیلی خوب بود و من عاشقش شدم!\"]\n",
    "test_X = vectorizer.transform(test_text).toarray()\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "# نتیجه\n",
    "print(\"احساس پیش‌بینی‌شده:\", \"مثبت\" if prediction[0] > 0.5 else \"منفی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c1c1a",
   "metadata": {},
   "source": [
    "### 📝 جمع‌بندی\n",
    "\n",
    "در این بخش، با تمام الگوریتم‌ها و مدل‌هایی که از حوزه **یادگیری عمیق (Deep Learning)** برای تحلیل احساسات متنی به کار می‌روند، آشنا شدیم.\n",
    "\n",
    "این مدل‌ها و ابزارها، با بهره‌گیری از ساختارهای پیچیده و یادگیری خودکار الگوهای پنهان،  \n",
    "فرآیند **شناسایی، دسته‌بندی و تحلیل احساسات** در متون مختلف (نظرات کاربران، شبکه‌های اجتماعی، نقدها و ...) را بسیار سریع‌تر، دقیق‌تر و هوشمندانه‌تر کرده‌اند.\n",
    "\n",
    "---\n",
    "\n",
    "> **یادگیری عمیق، مسیر تحلیل احساسات متنی را متحول و زمینه را برای توسعه سیستم‌های هوشمند و فهم عمیق‌تر زبان انسانی هموار کرده است.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
