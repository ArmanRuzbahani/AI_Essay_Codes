{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77945b74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Ø³ÙØ± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ: Ú©Ø´Ù Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ ğŸš€ğŸ§ \n",
    "\n",
    "**Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø¨Ø§ Ù‡Ù… Ø³Ù‡ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ø§Ø³ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ø±Ø§ Ú©Ø´Ù Ú©Ù†ÛŒÙ…:**\n",
    "\n",
    "## 1. <span style=\"color: #4d90fe;\">CNN (Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ)</span> ğŸ–¼ï¸\n",
    "- **Ù…ØºØ² Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ù…Ø§Ø´ÛŒÙ†** ğŸ‘ï¸\n",
    "- Ø§Ø² ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "- Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµØ§ÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯ÛŒÙˆ\n",
    "- Ù…Ø«Ø§Ù„: ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§ Ø¯Ø± Ø¹Ú©Ø³â€ŒÙ‡Ø§\n",
    "\n",
    "## 2. <span style=\"color: #34a853;\">RNN (Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ)</span> ğŸ”„\n",
    "- **Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ** ğŸ“œ\n",
    "- Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø§Ø² Ù…Ø±Ø§Ø­Ù„ Ù‚Ø¨Ù„ÛŒ Ø¨Ù‡ Ø®Ø§Ø·Ø± Ø¨Ø³Ù¾Ø§Ø±Ø¯\n",
    "- Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†ØŒ Ú¯ÙØªØ§Ø± Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ\n",
    "- Ù…Ø«Ø§Ù„: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¯Ø± Ø¬Ù…Ù„Ù‡\n",
    "\n",
    "## 3. <span style=\"color: #ea4335;\">Transformer</span> âš¡\n",
    "- **Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù†** ğŸ’¬\n",
    "- Ø§Ø² Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ (Attention) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "- Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø±Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯\n",
    "- Ù…Ø«Ø§Ù„: ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªÚ¯Ùˆ\n",
    "\n",
    "ğŸ“Œ **Ù†Ú©ØªÙ‡ Ø·Ù„Ø§ÛŒÛŒ:**  \n",
    "Ø§ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø§ØºÙ„Ø¨ Ø¨Ø§ Ù‡Ù… ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ÛŒ Ø¨Ø³Ø§Ø²Ù†Ø¯!\n",
    "\n",
    "ğŸ¯ **Ù‡Ø¯Ù Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³ÛŒØ±:**  \n",
    "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§ØµÙˆÙ„ Ù‡Ø± Ù…Ø¹Ù…Ø§Ø±ÛŒ + Ø¯Ø±Ú© Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ + Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664615",
   "metadata": {},
   "source": [
    "# ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚: Ù…ÙˆØªÙˆØ± Ù…Ø­Ø±Ú© Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø¯Ø±Ù† ğŸŒŸğŸ§ \n",
    "\n",
    "**ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚**ØŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±ÛŒÙ† Ø´Ø§Ø®Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ Ø§Ù„Ù‡Ø§Ù… Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ù…ØºØ² Ø§Ù†Ø³Ø§Ù†ØŒ Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† ÙÙ†Ø§ÙˆØ±ÛŒ Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø®ÙˆØ¯ Ù‚Ø§Ø¯Ø± Ø¨Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø³Ù„Ø³Ù„Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø³Øª:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa30d",
   "metadata": {},
   "source": [
    "# ğŸ§© CNN (Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ)\n",
    "Convolutional Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ\n",
    "\n",
    "**Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ (CNN)** ÛŒÚ© Ù…Ø¯Ù„ Ø¨Ø³ÛŒØ§Ø± Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ ÙØ¶Ø§ÛŒÛŒ Ø§Ø³Øª.\n",
    "\n",
    "- Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ CNNØŒ **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù† (Convolutional Layers)** Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¨Ø§ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ú©ÙˆÚ†Ú© (Kernel) Ø§Ø² Ø±ÙˆÛŒ ØªØµÙˆÛŒØ± Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ (Ù…Ø«Ù„ Ù„Ø¨Ù‡â€ŒÙ‡Ø§ØŒ Ø¨Ø§ÙØªâ€ŒÙ‡Ø§ Ùˆ Ø§Ø´ÛŒØ§Ø¡).\n",
    "- Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡ ØªØ±Ú©ÛŒØ¨ Ùˆ Ø§Ù†ØªØ²Ø§Ø¹ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ØªØ§ Ù…Ø¯Ù„ Ø¨ØªÙˆØ§Ù†Ø¯ Ø­ØªÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù†Ø¯.\n",
    "- **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Pooling** Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨ÛŒØ´â€ŒØ§Ø²Ø­Ø¯ (Overfitting) Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯.\n",
    "- Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ØŒ **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Fully Connected** ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÛŒØ§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Ú†Ø±Ø§ CNNØŸ\n",
    "\n",
    "- **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§** Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø³ØªÛŒ\n",
    "- **Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§** Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ ÙØ¶Ø§ÛŒÛŒ\n",
    "- **Ù…Ù‚Ø§ÙˆÙ…Øª Ø¨Ù‡ Ù†ÙˆÛŒØ²** Ùˆ ØªØºÛŒÛŒØ±Ø§Øª Ø¬Ø²Ø¦ÛŒ Ø¯Ø± ÙˆØ±ÙˆØ¯ÛŒ (Ù…Ø§Ù†Ù†Ø¯ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ ÛŒØ§ Ú†Ø±Ø®Ø´)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… CNN\n",
    "\n",
    "- **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ØªØµØ§ÙˆÛŒØ±** (Ù…Ø§Ù†Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ú¯Ø±Ø¨Ù‡/Ø³Ú¯ØŒ Ú†Ù‡Ø±Ù‡ Ùˆ ...)\n",
    "- **ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ØªØµØ§ÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§**\n",
    "- **ØªØ­Ù„ÛŒÙ„ ØªØµØ§ÙˆÛŒØ± Ù¾Ø²Ø´Ú©ÛŒ** (Ù…Ø§Ù†Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ø§Ø² Ø±Ø§Ø¯ÛŒÙˆÙ„ÙˆÚ˜ÛŒ ÛŒØ§ MRI)\n",
    "- **Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ù…Ø§Ø´ÛŒÙ†** (Computer Vision)\n",
    "- **ØªØ­Ù„ÛŒÙ„ Ø¯Ø³Øªâ€ŒØ®Ø· Ùˆ Ø§Ø¹Ø¯Ø§Ø¯ Ù†ÙˆØ´ØªÙ‡â€ŒØ´Ø¯Ù‡**\n",
    "- **Ø®ÙˆØ¯Ø±ÙˆÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ø±Ø§Ù†ØŒ Ø±Ø¨Ø§ØªÛŒÚ© Ùˆ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¬ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡:\n",
    "\n",
    "> **CNNÙ‡Ø§ Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…ØŒ ÛŒÚ©ÛŒ Ø§Ø² Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªØµØ§ÙˆÛŒØ± Ùˆ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¶Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 1: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ\n",
    "# Ù‡Ø¯Ù: ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø´Ø¨Ú©Ù‡\n",
    "def prepare_text_data(texts, labels, max_words=10000, max_len=100):\n",
    "    # ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ: ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Ù¾Ø¯ Ú©Ø±Ø¯Ù† Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ø·ÙˆÙ„\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡\n",
    "    labels = np.array(labels)\n",
    "    return data, labels, tokenizer\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 2: ØªØ¹Ø±ÛŒÙ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ\n",
    "# Ù‡Ø¯Ù: Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù…Ø¯Ù„ CNN Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = models.Sequential([\n",
    "        # Ù„Ø§ÛŒÙ‡ ØªØ¹Ø¨ÛŒÙ‡: ØªØ¨Ø¯ÛŒÙ„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªØ±Ø§Ú©Ù…\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        # Ù„Ø§ÛŒÙ‡ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ Ø§ÙˆÙ„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ø§Ø² Ù…ØªÙ†\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        # Ù„Ø§ÛŒÙ‡ Ù¾ÙˆÙ„ÛŒÙ†Ú¯: Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ Ø­ÙØ¸ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # Ù„Ø§ÛŒÙ‡ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†ÛŒ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # Ù…Ø³Ø·Ø­â€ŒØ³Ø§Ø²ÛŒ: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ±Ø§Ú©Ù…\n",
    "        layers.Flatten(),\n",
    "        # Ù„Ø§ÛŒÙ‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…ØªØµÙ„: ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ (Ù…Ø«Ø¨Øª/Ù…Ù†ÙÛŒ)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 3: Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„\n",
    "# Ù‡Ø¯Ù: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù† Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„ Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ùˆ ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'Ø¯Ù‚Øª ØªØ³Øª: {test_acc:.4f}')\n",
    "    return test_acc, test_loss\n",
    "\n",
    "# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø§ØµÙ„ÛŒ\n",
    "def main():\n",
    "    # Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "    # ÙØ±Ø¶\n",
    "    texts = [\"ÙÛŒÙ„Ù… Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù…\", \"Ø§ÛŒÙ† Ø¨Ø¯ØªØ±ÛŒÙ† ØªØ¬Ø±Ø¨Ù‡ Ù…Ù† Ø¨ÙˆØ¯\", ...]  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ\n",
    "    labels = [1, 0, ...]  # Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "    max_words = 10000  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
    "    max_len = 100  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡\n",
    "\n",
    "    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "    x_data, y_data, tokenizer = prepare_text_data(texts, labels, max_words, max_len)\n",
    "    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ ØªØ³Øª (ÙØ±Ø¶: 80% Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ 20% ØªØ³Øª)\n",
    "    train_size = int(0.8 * len(x_data))\n",
    "    x_train, y_train = x_data[:train_size], y_data[:train_size]\n",
    "    x_test, y_test = x_data[train_size:], y_data[train_size:]\n",
    "\n",
    "    # ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„\n",
    "    model = build_cnn_model(max_words, max_len)\n",
    "    # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„\n",
    "    model.summary()\n",
    "    # Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cf4ac",
   "metadata": {},
   "source": [
    "# ğŸ”„ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ (RNN)\n",
    "Recurrent Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ\n",
    "\n",
    "**Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ (RNN)** Ù†ÙˆØ¹ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±ØªÛŒØ¨ÛŒ (Sequence Data) Ù…Ø«Ù„ Ù…ØªÙ†ØŒ Ú¯ÙØªØ§Ø± ÛŒØ§ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n",
    "\n",
    "- Ø¯Ø± RNNØŒ Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø± Ú¯Ø§Ù… Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ú¯Ø§Ù… Ø¨Ø¹Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯Ø›  \n",
    "  ÛŒØ¹Ù†ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø¯Ø± Ù…Ø¯Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.\n",
    "- Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ§ Ù…Ø¯Ù„ Ø¨ØªÙˆØ§Ù†Ø¯ **ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ** Ùˆ **Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§** Ø±Ø§ Ø¯Ø± Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø­ÙØ¸ Ú©Ù†Ø¯.\n",
    "\n",
    "> RNNÙ‡Ø§ Ø¨Ø±Ø®Ù„Ø§Ù Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (Feedforward)ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø°Ø´ØªÙ‡ Ø±Ø§ Ø¯Ø± Ø®ÙˆØ¯ Ù†Ú¯Ù‡ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Ú†Ø±Ø§ RNNØŸ\n",
    "\n",
    "- **Ø¯Ø±Ú© ØªØ±ØªÛŒØ¨ Ùˆ Ø²Ù…ÛŒÙ†Ù‡:**  \n",
    "  Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†ØŒ ØµØ¯Ø§ØŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ø³ÙˆØ± ÛŒØ§ Ù‡Ø± Ú†ÛŒØ²ÛŒ Ú©Ù‡ ØªØ±ØªÛŒØ¨ Ù…Ù‡Ù… Ø¨Ø§Ø´Ø¯.\n",
    "- **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø·ÙˆÙ„ Ù…ØªØºÛŒØ±:**  \n",
    "  Ù…Ø§Ù†Ù†Ø¯ Ø¬Ù…Ù„Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ø¨Ù„Ù†Ø¯ØŒ ÛŒØ§ Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø± Ø³Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù…\n",
    "\n",
    "- **Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (NLP):**  \n",
    "  ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ†ØŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ùˆ Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§\n",
    "- **ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ Ù„Ø­Ù† Ø¯Ø± Ø¬Ù…Ù„Ø§Øª:**  \n",
    "  Ø¯Ø±Ú© Ù…Ø¹Ù†Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ù¾Ø´Øª Ù…ØªÙ† Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³ ÛŒØ§ Ù‚ØµØ¯ Ú¯ÙˆÛŒÙ†Ø¯Ù‡\n",
    "- **ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ùˆ ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ†**\n",
    "- **Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ:**  \n",
    "  Ù…Ø§Ù†Ù†Ø¯ Ù‚ÛŒÙ…Øª Ø³Ù‡Ø§Ù…ØŒ Ø¢Ø¨â€ŒÙˆÙ‡ÙˆØ§ØŒ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø²Ø´Ú©ÛŒ\n",
    "- **ØªØ´Ø®ÛŒØµ ÙØ¹Ø§Ù„ÛŒØª Ùˆ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¬ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡:\n",
    "\n",
    "> **RNNÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø´ØªÙ† Ø­Ø§ÙØ¸Ù‡ Ø¯Ø§Ø®Ù„ÛŒØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ¯Ø§Ø± Ø¯Ø±Ú© Ùˆ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f57a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Ø¯Ø§Ø¯Ù‡\n",
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is bad\",\n",
    "    \"I hate this\",\n",
    "    \"This is great\",\n",
    "    \"I like it\",\n",
    "    \"It is terrible\"\n",
    "]\n",
    "labels = [1, 0, 0, 1, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=padded.shape[1]),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "model.fit(padded, labels, epochs=10)\n",
    "\n",
    "# Ø¬Ù…Ù„Ù‡ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±\n",
    "new_sentence = [\"I love this product! It works really well\"]\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "seq = tokenizer.texts_to_sequences(new_sentence)\n",
    "padded_seq = pad_sequences(seq, maxlen=padded.shape[1], padding='post')\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³ Ø¬Ù…Ù„Ù‡\n",
    "prediction = model.predict(padded_seq)[0][0]\n",
    "\n",
    "print(f\"Sentiment score (0=negative, 1=positive): {prediction:.3f}\")\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"Sentiment: Positive ğŸ˜Š\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative ğŸ˜\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8625a8",
   "metadata": {},
   "source": [
    "# Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72f35",
   "metadata": {},
   "source": [
    "# ğŸ’› LSTM (Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ú©ÙˆØªØ§Ù‡ - Long Short-Term Memory)\n",
    "Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ LSTM Ø¯Ø± ØªØ¹Ø§Ù…Ù„ Ø§Ø­Ø³Ø§Ø³ÛŒ\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ\n",
    "\n",
    "**LSTM** Ù†ÙˆØ¹ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ (RNN) Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒÙ…Ø¯Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±ØªÛŒØ¨ÛŒ (Sequence Data) Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ùˆ Ù…Ø´Ú©Ù„ ÙØ±Ø§Ù…ÙˆØ´ÛŒ Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ø±Ø§ Ø¯Ø± RNNÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨Ø±Ø·Ø±Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… LSTM Ø¯Ø± ØªØ¹Ø§Ù…Ù„ Ø§Ø­Ø³Ø§Ø³ÛŒ\n",
    "\n",
    "- **ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ:**  \n",
    "  Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Ø´Ø§Ø¯ÛŒØŒ ØºÙ…ØŒ Ø®Ø´Ù… Ùˆ...) Ø¯Ø± Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ØŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ÛŒØ§ Ù†Ø¸Ø±Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†.\n",
    "- **ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø§Ø­Ø³Ø§Ø³ÛŒ:**  \n",
    "  ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ Ø²ÛŒØ±ÙˆØ¨Ù…ÛŒØŒ Ø³Ø±Ø¹Øª Ùˆ Ø±ÛŒØªÙ… Ú¯ÙØªØ§Ø± Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ Ú¯ÙˆÛŒÙ†Ø¯Ù‡.\n",
    "- **Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ:**  \n",
    "  Ø­ÙØ¸ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ùˆ Ø²Ù…ÛŒÙ†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ù‡Ù…Ø¯Ù„Ø§Ù†Ù‡ Ø¯Ø± Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§ Ùˆ Ø¯Ø³ØªÛŒØ§Ø±Ù‡Ø§ÛŒ ØµÙˆØªÛŒ.\n",
    "- **Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙØªØ§Ø± ÛŒØ§ ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ:**  \n",
    "  ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ (Ù…Ø§Ù†Ù†Ø¯ ÙØ¹Ø§Ù„ÛŒØª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÛŒØ§ Ø­Ø§Ù„Ø§Øª Ø±ÙˆØ²Ø§Ù†Ù‡) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†ÙˆØ³Ø§Ù†Ø§Øª ÛŒØ§ ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³Ø§Øª.\n",
    "- **Ø¯Ø±Ú© Ø²ÛŒØ±Ù…ØªÙ† Ùˆ Ù„Ø­Ù† ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ…:**  \n",
    "  Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾Ù†Ù‡Ø§Ù† ÛŒØ§ Ù„Ø­Ù† ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ùˆ Ø²Ù…ÛŒÙ†Ù‡ Ø¬Ù…Ù„Ø§Øª.\n",
    "- **Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø± Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù† Ùˆ Ù…Ø´Ø§ÙˆØ±Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯:**  \n",
    "  Ø±ØµØ¯ Ø­Ø§Ù„Ø§Øª Ø±ÙˆØ­ÛŒ Ø¨ÛŒÙ…Ø§Ø±Ø§Ù† Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ø¯Ø§Ø®Ù„Ø§Øª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³ÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù†.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¬ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡:\n",
    "\n",
    "> **LSTMÙ‡Ø§ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ§Ø¨Ø· Ùˆ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ±ØªÛŒØ¨ÛŒØŒ ÛŒÚ©ÛŒ Ø§Ø² Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ØªØ±ÛŒÙ† Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ØŒ Ø¯Ø±Ú© Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ø­Ø³Ø§Ø³ÛŒ Ø§Ù†Ø³Ø§Ù† Ùˆ Ù…Ø§Ø´ÛŒÙ† Ø¨Ù‡ Ø´Ù…Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_features = 10000  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯\n",
    "maxlen = 200  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù†Ø¸Ø±Ø§Øª (ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª)\n",
    "embedding_dim = 100  # Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø¯Ø§Ø± Ø¬Ø§Ø³Ø§Ø²ÛŒ\n",
    "\n",
    "# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 2. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´: Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø·ÙˆÙ„ Ù†Ø¸Ø±Ø§Øª\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# 3. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„\n",
    "model.summary()\n",
    "\n",
    "# 4. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 5. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ: {test_accuracy:.4f}')\n",
    "\n",
    "# 6. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª Ùˆ Ø®Ø·Ø§\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Ø¯Ù‚Øª Ø¢Ù…ÙˆØ²Ø´')\n",
    "plt.plot(history.history['val_accuracy'], label='Ø¯Ù‚Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ')\n",
    "plt.title('Ø¯Ù‚Øª Ù…Ø¯Ù„')\n",
    "plt.xlabel('Ø¯ÙˆØ±Ù‡ (Epoch)')\n",
    "plt.ylabel('Ø¯Ù‚Øª')\n",
    "plt.legend()\n",
    "\n",
    "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·Ø§\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Ø®Ø·Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´')\n",
    "plt.plot(history.history['val_loss'], label='Ø®Ø·Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ')\n",
    "plt.title('Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„')\n",
    "plt.xlabel('Ø¯ÙˆØ±Ù‡ (Epoch)')\n",
    "plt.ylabel('Ø®Ø·Ø§')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3560eed",
   "metadata": {},
   "source": [
    "# ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ú¯ÛŒØªâ€ŒØ¯Ø§Ø± (Gated Recurrent Units - GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f98278",
   "metadata": {},
   "source": [
    "### ğŸ’¬ Ú©Ø§Ø±Ø¨Ø±Ø¯ GRU Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ØªØ±ØªÛŒØ¨ÛŒ\n",
    "\n",
    "Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§ØªØŒ Ù…ØªÙ† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ØªÙˆØ§Ù„ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª ÛŒØ§ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯.  \n",
    "**GRU** (Gated Recurrent Unit) Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ **ØªØ±ØªÛŒØ¨ Ú©Ù„Ù…Ø§Øª**ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ø¹Ø§Ù†ÛŒØŒ Ø±ÙˆØ§Ø¨Ø· Ùˆ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Ù…Ø«Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ\n",
    "\n",
    "Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ±:\n",
    "> _Â«Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø§ØµÙ„Ø§Ù‹ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ø®Ø³ØªÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯Â»_\n",
    "\n",
    "GRU Ù‚Ø§Ø¯Ø± Ø§Ø³Øª Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† Ø¹Ø¨Ø§Ø±Ø§Øª Â«Ø§ØµÙ„Ø§Ù‹ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯Â» Ùˆ Â«Ø®Ø³ØªÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡Â» Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù†Ø¯  \n",
    "Ùˆ **Ø§Ø­Ø³Ø§Ø³ Ù…Ù†ÙÛŒ** Ù¾Ø´Øª Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯Ø›  \n",
    "Ø­ØªÛŒ Ø§Ú¯Ø± Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†ÙÛŒâ€ŒØ¨ÙˆØ¯Ù†ØŒ Ù¾Ø®Ø´ ÛŒØ§ ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Ù†Ú©ØªÙ‡:\n",
    "\n",
    "GRU Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ù†Ø³Ø¨Øª Ø¨Ù‡ LSTMØŒ Ù‡Ù…Ú†Ù†Ø§Ù† Ù‚Ø¯Ø±Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ù†Ø¯ Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒÚ©ÛŒ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ø§Ø³Øª.\n",
    "\n",
    "---\n",
    "\n",
    "> **Ø®Ù„Ø§ØµÙ‡:**  \n",
    "> **GRU Ø¨Ø§ Ø¯Ø±Ú© ØªØ±ØªÛŒØ¨ Ùˆ Ù…Ø¹Ù†Ø§ÛŒ Ú©Ù„Ù…Ø§ØªØŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒ Ø±Ø§ Ø¯Ø± Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù…Ù…Ú©Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
    "texts = [\n",
    "    \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù…!\",\n",
    "    \"ÙÛŒÙ„Ù… Ø§ØµÙ„Ø§Ù‹ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ø®Ø³ØªÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯.\",\n",
    "    \"Ø¯Ø§Ø³ØªØ§Ù† ÙÛŒÙ„Ù… ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§Ø²ÛŒÚ¯Ø±Ø§Ù† Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù†Ø¯.\",\n",
    "    \"ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø¯ØªØ±ÛŒÙ† ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÛŒØ¯Ù…ØŒ Ø§ÙØªØ¶Ø§Ø­ Ø¨ÙˆØ¯.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: Ù…Ø«Ø¨ØªØŒ 0: Ù…Ù†ÙÛŒ\n",
    "\n",
    "# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§\n",
    "max_words = 1000  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
    "max_len = 20      # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=max_len))  # Ù„Ø§ÛŒÙ‡ ØªØ¹Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ\n",
    "model.add(GRU(64, return_sequences=False))  # Ù„Ø§ÛŒÙ‡ GRU Ø¨Ø§ 64 ÙˆØ§Ø­Ø¯\n",
    "model.add(Dense(1, activation='sigmoid'))   # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ\n",
    "\n",
    "# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„\n",
    "model.summary()\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "test_text = [\"Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ùˆ Ù…Ù† Ø¹Ø§Ø´Ù‚Ø´ Ø´Ø¯Ù…!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# Ù†ØªÛŒØ¬Ù‡\n",
    "print(\"Ø§Ø­Ø³Ø§Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡:\", \"Ù…Ø«Ø¨Øª\" if prediction[0] > 0.5 else \"Ù…Ù†ÙÛŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac0762",
   "metadata": {},
   "source": [
    "# ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø± (Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45b98e",
   "metadata": {},
   "source": [
    "### ğŸ’¬ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø±\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ù‡Ø¯Ù ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ\n",
    "\n",
    "**ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ** Ø¨Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø²ÛŒØ± Ø¯Ø± Ù…ØªÙ† Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯:\n",
    "- **Ù…Ø«Ø¨Øª**\n",
    "- **Ù…Ù†ÙÛŒ**\n",
    "- **Ø®Ù†Ø«ÛŒ**\n",
    "\n",
    "Ø¯Ø± Ù…ØªÙˆÙ†ÛŒ Ù…Ø§Ù†Ù†Ø¯:\n",
    "- Ù†Ø¸Ø±Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "- Ù†Ù‚Ø¯ ÙÛŒÙ„Ù… ÛŒØ§ Ù…Ø­ØµÙˆÙ„\n",
    "- Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ\n",
    "- Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤– Ú†Ø±Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø± (Transformers)ØŸ\n",
    "\n",
    "**Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø±** Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ØªÙˆØ§Ù†Ø§ÛŒÛŒ ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡â€ŒØ´Ø§Ù† Ø¯Ø±:\n",
    "- Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚ Ø²Ù…ÛŒÙ†Ù‡ Ùˆ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª\n",
    "- ÙÙ‡Ù… ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ùˆ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù\n",
    "\n",
    "Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒØŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¤Ø«Ø± Ùˆ Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÙ†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "\n",
    "Ø§ÛŒÙ† ØªØ±Ú©ÛŒØ¨ (ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª + ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø±Ù‡Ø§)  \n",
    "ÙØ±Ø¢ÛŒÙ†Ø¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø±Ø§ **Ø³Ø±ÛŒØ¹â€ŒØªØ±ØŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ùˆ Ú©Ø§Ø±Ø¢Ù…Ø¯ØªØ±** Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "> **Ø®Ù„Ø§ØµÙ‡:**  \n",
    "> **Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø±ØŒ Ø§Ø¨Ø²Ø§Ø±ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø¨Ø§ Ø¯Ù‚Øª Ùˆ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ Ø¯Ø± Ù…ØªÙˆÙ† Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ Ø¨Ù„Ù†Ø¯ Ù‡Ø³ØªÙ†Ø¯.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
    "texts = [\n",
    "    \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù…!\",\n",
    "    \"ÙÛŒÙ„Ù… Ø§ØµÙ„Ø§Ù‹ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ø®Ø³ØªÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯.\",\n",
    "    \"Ø¯Ø§Ø³ØªØ§Ù† ÙÛŒÙ„Ù… ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§Ø²ÛŒÚ¯Ø±Ø§Ù† Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù†Ø¯.\",\n",
    "    \"ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø¯ØªØ±ÛŒÙ† ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÛŒØ¯Ù…ØŒ Ø§ÙØªØ¶Ø§Ø­ Ø¨ÙˆØ¯.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: Ù…Ø«Ø¨ØªØŒ 0: Ù…Ù†ÙÛŒ\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 Ø¯ÙˆØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "test_text = [\"Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ùˆ Ù…Ù† Ø¹Ø§Ø´Ù‚Ø´ Ø´Ø¯Ù…!\"]\n",
    "test_inputs = tokenizer(test_text, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'])\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# Ù†ØªÛŒØ¬Ù‡\n",
    "print(\"Ø§Ø­Ø³Ø§Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡:\", \"Ù…Ø«Ø¨Øª\" if predictions[0] == 1 else \"Ù…Ù†ÙÛŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4cf31",
   "metadata": {},
   "source": [
    "# Ø§ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±Ù‡Ø§ (Autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926b26b",
   "metadata": {},
   "source": [
    "# ğŸŒ€ ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±Ù‡Ø§ (Autoencoders)\n",
    "Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘€ Ú†ÛŒØ³ØªÛŒ ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±Ù‡Ø§\n",
    "\n",
    "**ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±Ù‡Ø§** Ù†ÙˆØ¹ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙØ´Ø±Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§** (Representation Learning) Ø¨Ù‡ ØµÙˆØ±Øª **Ø¨Ø¯ÙˆÙ† Ù†Ø¸Ø§Ø±Øª** (Unsupervised) Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŸ\n",
    "\n",
    "- Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ (Ù…Ø«Ù„Ø§Ù‹ Ù…ØªÙ† ÛŒØ§ ØªØµÙˆÛŒØ±) ÙˆØ§Ø±Ø¯ Ø´Ø¨Ú©Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
    "- **Ø¨Ø®Ø´ Ø±Ù…Ø²Ú¯Ø°Ø§Ø± (Encoder)** Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù†Ù…Ø§ÛŒØ´ ÙØ´Ø±Ø¯Ù‡ (Ú©ÙØ¯) ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "- **Ø¨Ø®Ø´ Ø±Ù…Ø²Ú¯Ø´Ø§ (Decoder)** ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ú©Ø¯ ÙØ´Ø±Ø¯Ù‡ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ù†Ø¯.\n",
    "- Ø´Ø¨Ú©Ù‡ØŒ Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÙØ´Ø±Ø¯Ù‡ Ø­ÙØ¸ Ú©Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ú©Ø§Ø±Ø¨Ø±Ø¯ ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±Ù‡Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "\n",
    "- **Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø²Ø±Ú¯ Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ù‡**\n",
    "- **Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ùˆ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø§Ø² Ù…ØªÙ†**\n",
    "- Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¯Ø± **ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª** Ø¨Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ù…Ø§ÛŒØ´ÛŒ ÙØ´Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ (Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯)\n",
    "\n",
    "Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ ÛŒÚ© Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡ Ø§Ø² Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ  \n",
    "Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "> **Ø®Ù„Ø§ØµÙ‡:**  \n",
    "> **ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±Ù‡Ø§ Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙØ´Ø±Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ø¨Ø²Ø§Ø±ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ù‡Ø³ØªÙ†Ø¯.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
    "texts = [\n",
    "    \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù…!\",\n",
    "    \"ÙÛŒÙ„Ù… Ø§ØµÙ„Ø§Ù‹ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ø®Ø³ØªÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯.\",\n",
    "    \"Ø¯Ø§Ø³ØªØ§Ù† ÙÛŒÙ„Ù… ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§Ø²ÛŒÚ¯Ø±Ø§Ù† Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù†Ø¯.\",\n",
    "    \"ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø¯ØªØ±ÛŒÙ† ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÛŒØ¯Ù…ØŒ Ø§ÙØªØ¶Ø§Ø­ Ø¨ÙˆØ¯.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: Ù…Ø«Ø¨ØªØŒ 0: Ù…Ù†ÙÛŒ\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
    "max_words = 1000  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª\n",
    "max_len = 20      # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ø§ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding = Embedding(max_words, 50, input_length=max_len)(input_layer)\n",
    "encoded = LSTM(64, return_sequences=False)(embedding)  # Ù„Ø§ÛŒÙ‡ Ø§Ù†Ú©ÙˆØ¯Ø±\n",
    "decoded = Dense(max_len * 50, activation='relu')(encoded)  # Ù„Ø§ÛŒÙ‡ Ø¯ÛŒÚ©ÙˆØ¯Ø±\n",
    "decoded = Reshape((max_len, 50))(decoded)\n",
    "\n",
    "# Ù…Ø¯Ù„ Ø§ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒØ´ ÙØ´Ø±Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "classification_model = Model(input_layer, classification_output)\n",
    "classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "classification_model.summary()\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ø§ØªÙˆØ§Ù†Ú©ÙˆØ¯Ø±\n",
    "autoencoder.fit(X, X.reshape(X.shape[0], max_len, 50), epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "classification_model.fit(X, y, epochs=5, batch_size=2, verbose=1)\n",
    "\n",
    "# ØªØ³Øª Ø±ÙˆÛŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "test_text = [\"Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ùˆ Ù…Ù† Ø¹Ø§Ø´Ù‚Ø´ Ø´Ø¯Ù…!\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_len)\n",
    "prediction = classification_model.predict(test_padded)\n",
    "\n",
    "# Ù†ØªÛŒØ¬Ù‡\n",
    "print(\"Ø§Ø­Ø³Ø§Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡:\", \"Ù…Ø«Ø¨Øª\" if prediction[0] > 0.5 else \"Ù…Ù†ÙÛŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b5dc5",
   "metadata": {},
   "source": [
    "# Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÙˆØ± Ø¹Ù…ÛŒÙ‚ (Deep Belief Networks - DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05279512",
   "metadata": {},
   "source": [
    "### ğŸ’¬ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ùˆ Ù†Ù‚Ø´ DBNÙ‡Ø§\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ù‡Ø¯Ù ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ\n",
    "\n",
    "ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø¨Ù‡ **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª** Ø²ÛŒØ± Ø¯Ø± Ù…ØªÙ† Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯:\n",
    "- **Ù…Ø«Ø¨Øª**\n",
    "- **Ù…Ù†ÙÛŒ**\n",
    "- **Ø®Ù†Ø«ÛŒ**\n",
    "\n",
    "Ø¯Ø± Ù…ØªÙˆÙ†ÛŒ Ù…Ø§Ù†Ù†Ø¯:\n",
    "- Ù†Ø¸Ø±Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "- Ù†Ù‚Ø¯Ù‡Ø§\n",
    "- Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Ù†Ù‚Ø´ DBNÙ‡Ø§ (Deep Belief Networks) Ø¯Ø± Ø§ÛŒÙ† Ø­ÙˆØ²Ù‡\n",
    "\n",
    "DBNÙ‡Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:\n",
    "- **Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚ Ùˆ ØºÛŒØ±Ø®Ø·ÛŒ** Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ  \n",
    "  (ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ùˆ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨ÛŒÙ† ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¹Ø¨Ø§Ø±Ø§Øª)\n",
    "- **Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§** Ùˆ Ø­Ø°Ù Ù†ÙˆÛŒØ² Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "- **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§Ø²Ù†Ù…Ø§ÛŒÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø² Ù…ØªÙ†** Ø¬Ù‡Øª Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Ù…Ø§Ù†Ù†Ø¯ Softmax ÛŒØ§ SVM)\n",
    "- **Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ…** Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ùˆ Ø¯Ù‚Øª ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª\n",
    "- **Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡** Ù…Ø§Ù†Ù†Ø¯ Ø¬Ù…Ù„Ø§Øª Ø·ÙˆÙ„Ø§Ù†ÛŒØŒ Ø²Ø¨Ø§Ù† ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ… ÛŒØ§ Ø·Ù†Ø²\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "\n",
    "Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª **Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±** Ùˆ **Ú©Ø§Ø±Ø¢Ù…Ø¯ØªØ±** Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŒ  \n",
    "Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ… Ùˆ Ù…ØªÙˆÙ† Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ Ú†Ù†Ø¯Ù…Ø¹Ù†Ø§ÛŒÛŒ.\n",
    "\n",
    "---\n",
    "\n",
    "> **Ø®Ù„Ø§ØµÙ‡:**  \n",
    "> **DBNÙ‡Ø§ Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚ØŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø±Ø§ Ø¯Ø± Ù…ØªÙˆÙ† Ø¨Ø²Ø±Ú¯ Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø³ÛŒØ§Ø± ØªÙ‚ÙˆÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
    "texts = [\n",
    "    \"Ø§ÛŒÙ† ÙÛŒÙ„Ù… ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù…!\",\n",
    "    \"ÙÛŒÙ„Ù… Ø§ØµÙ„Ø§Ù‹ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ø®ÛŒÙ„ÛŒ Ø®Ø³ØªÙ‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯.\",\n",
    "    \"Ø¯Ø§Ø³ØªØ§Ù† ÙÛŒÙ„Ù… ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø§Ø²ÛŒÚ¯Ø±Ø§Ù† Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù†Ø¯.\",\n",
    "    \"ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø¯ØªØ±ÛŒÙ† ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÛŒØ¯Ù…ØŒ Ø§ÙØªØ¶Ø§Ø­ Ø¨ÙˆØ¯.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: Ù…Ø«Ø¨ØªØŒ 0: Ù…Ù†ÙÛŒ\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ DBN-Ù…Ø§Ù†Ù†Ø¯ (Ø´Ø¨Ú©Ù‡ Ø¹Ù…ÛŒÙ‚ Ø¨Ø§ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Dense)\n",
    "model = Sequential()\n",
    "# Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ÙÛŒ Ù…Ø´Ø§Ø¨Ù‡ RBM Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„\n",
    "model.summary()\n",
    "\n",
    "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "model.fit(X, y, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "# ØªØ³Øª Ø±ÙˆÛŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "test_text = [\"Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ùˆ Ù…Ù† Ø¹Ø§Ø´Ù‚Ø´ Ø´Ø¯Ù…!\"]\n",
    "test_X = vectorizer.transform(test_text).toarray()\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "# Ù†ØªÛŒØ¬Ù‡\n",
    "print(\"Ø§Ø­Ø³Ø§Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡:\", \"Ù…Ø«Ø¨Øª\" if prediction[0] > 0.5 else \"Ù…Ù†ÙÛŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c1c1a",
   "metadata": {},
   "source": [
    "### ğŸ“ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ø¨Ø§ ØªÙ…Ø§Ù… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø² Ø­ÙˆØ²Ù‡ **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ (Deep Learning)** Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯ØŒ Ø¢Ø´Ù†Ø§ Ø´Ø¯ÛŒÙ….\n",
    "\n",
    "Ø§ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ØŒ Ø¨Ø§ Ø¨Ù‡Ø±Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†ØŒ  \n",
    "ÙØ±Ø¢ÛŒÙ†Ø¯ **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒØŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª** Ø¯Ø± Ù…ØªÙˆÙ† Ù…Ø®ØªÙ„Ù (Ù†Ø¸Ø±Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†ØŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒØŒ Ù†Ù‚Ø¯Ù‡Ø§ Ùˆ ...) Ø±Ø§ Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±ØŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡â€ŒØªØ± Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "> **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ØŒ Ù…Ø³ÛŒØ± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ†ÛŒ Ø±Ø§ Ù…ØªØ­ÙˆÙ„ Ùˆ Ø²Ù…ÛŒÙ†Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ ÙÙ‡Ù… Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ø²Ø¨Ø§Ù† Ø§Ù†Ø³Ø§Ù†ÛŒ Ù‡Ù…ÙˆØ§Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f442fcb",
   "metadata": {},
   "source": [
    "## âš¡ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø®Ø§Ø±Ø¯Ø§Ø± (Spiking Neural Networks - SNN)\n",
    "\n",
    "**Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø®Ø§Ø±Ø¯Ø§Ø±** ÛŒØ§ **Spiking Neural Networks (SNN)** Ù†Ø³Ù„ Ø³ÙˆÙ… Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø³Ø¹ÛŒ Ø¯Ø§Ø±Ù†Ø¯ **Ù†Ø­ÙˆÙ‡â€ŒÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…ØºØ² Ùˆ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒÚ©ÛŒ** Ø±Ø§ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯.\n",
    "\n",
    "Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ© (Ù…Ø§Ù†Ù†Ø¯ MLP ÛŒØ§ CNN)ØŒ SNNÙ‡Ø§ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ØŒ Ø§Ø² **Ù¾Ø§Ù„Ø³â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ (spikes)** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Ø§Ù„Ù‡Ø§Ù…â€ŒÚ¯Ø±ÙØªÙ‡ Ø§Ø² Ù…ØºØ² ÙˆØ§Ù‚Ø¹ÛŒ\n",
    "\n",
    "Ø¯Ø± Ù…ØºØ² Ø§Ù†Ø³Ø§Ù†ØŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ù†Ù‡ Ø¨Ø§ Ø§Ø¹Ø¯Ø§Ø¯ Ø«Ø§Ø¨ØªØŒ Ø¨Ù„Ú©Ù‡ Ø¨Ø§ **Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù¾Ø§Ù„Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ù„Ú©ØªØ±ÛŒÚ©ÛŒ** Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¯Ø§Ø± Ù‡Ù… Ø§Ø² Ù‡Ù…ÛŒÙ† Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯:\n",
    "- Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† ÙÙ‚Ø· Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ ØªØ­Ø±ÛŒÚ© Ø¢Ù† Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡â€ŒØ§ÛŒ ÙØ±Ø§ØªØ± Ø±ÙˆØ¯ØŒ ÛŒÚ© **Ù¾Ø§Ù„Ø³ (spike)** Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "- Ø§ÛŒÙ† ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø³ÛŒØ³ØªÙ… **Ø±ÙˆÛŒØ¯Ø§Ø¯-Ù…Ø­ÙˆØ± (event-driven)**ØŒ **Ø²Ù…Ø§Ù†â€ŒÙ…Ù†Ø¯ (temporal)** Ùˆ **Ú©Ù…â€ŒÙ…ØµØ±Ù (energy-efficient)** Ø¨Ø§Ø´Ø¯.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ SNN:\n",
    "\n",
    "| ÙˆÛŒÚ˜Ú¯ÛŒ                 | ØªÙˆØ¶ÛŒØ­                                           |\n",
    "|------------------------|--------------------------------------------------|\n",
    "| ğŸ”‹ Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ Ù¾Ø§ÛŒÛŒÙ†   | Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ ÙÙ‚Ø· Ù‡Ù†Ú¯Ø§Ù… Ø§Ø³Ù¾Ø§ÛŒÚ© Ú©Ø±Ø¯Ù† ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯    |\n",
    "| â±ï¸ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ù…Ø§Ù†â€ŒÙ…Ø­ÙˆØ±    | Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯             |\n",
    "| â›“ï¸ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø²Ù…Ø§Ù†ÛŒ       | ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ ØªÙˆØ§Ù„ÛŒ Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ù¾Ø§Ù„Ø³â€ŒÙ‡Ø§          |\n",
    "| ğŸ§¬ Ø²ÛŒØ³Øªâ€ŒØ§Ù„Ù‡Ø§Ù…â€ŒÚ¯Ø±ÙØªÙ‡     | Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø±ÙØªØ§Ø± Ù…ØºØ² Ùˆ Ø³ÛŒØ³ØªÙ… Ø¹ØµØ¨ÛŒ       |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ† Ø¯Ø± SNN:\n",
    "\n",
    "- **Leaky Integrate-and-Fire (LIF):** Ø±Ø§ÛŒØ¬â€ŒØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ù†ÙˆØ±ÙˆÙ†\n",
    "- **Hodgkinâ€“Huxley Model:** Ø¯Ù‚ÛŒÙ‚ ÙˆÙ„ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ø³Ù†Ú¯ÛŒÙ†\n",
    "- **Izhikevich Model:** ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† Ø¯Ù‚Øª Ø²ÛŒØ³ØªÛŒ Ùˆ Ú©Ø§Ø±Ø§ÛŒÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ©\n",
    "\n",
    "| ÙˆÛŒÚ˜Ú¯ÛŒ           | Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ© (ANN) | Ø´Ø¨Ú©Ù‡ Ø®Ø§Ø±Ø¯Ø§Ø± (SNN)     |\n",
    "|------------------|--------------------------|-------------------------|\n",
    "| Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ       | Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡           | Ù¾Ø§Ù„Ø³â€ŒÙ‡Ø§ÛŒ Ú¯Ø³Ø³ØªÙ‡ (spike) |\n",
    "| Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ù…Ø§Ù†      | Ù†Ø¯Ø§Ø±Ø¯ (instantaneous)   | Ø¯Ø§Ø±Ø¯ (temporal)         |\n",
    "| Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ       | Ù†Ø³Ø¨ØªØ§Ù‹ Ø¨Ø§Ù„Ø§              | Ø¨Ø³ÛŒØ§Ø± Ú©Ù…                |\n",
    "| Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø²ÛŒØ³ØªÛŒ | Ù†Ù‡                 | Ø¨Ù„Ù‡ (Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ù†ÙˆØ±ÙˆÙ…ÙˆØ±ÙÛŒÚ©) |\n",
    "| Ø¯Ù‚Øª Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒÚ©ÛŒ    | Ù¾Ø§ÛŒÛŒÙ†                    | Ø¨Ø§Ù„Ø§                    |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¬ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ SNN:\n",
    "\n",
    "- ğŸ‘ï¸ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ú¯Ø±Ù‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ…ÙˆØ±ÙÛŒÚ© (Ù…Ø«Ù„ Ø¯ÙˆØ±Ø¨ÛŒÙ†â€ŒÙ‡Ø§ÛŒ DVS)\n",
    "- ğŸ¤– Ø±Ø¨Ø§ØªÛŒÚ© Ú©Ù…â€ŒÙ…ØµØ±Ù Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ù„Ø­Ø¸Ù‡\n",
    "- ğŸ§  Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØºØ² Ø¨Ø±Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ø¹ØµØ§Ø¨\n",
    "- ğŸ›°ï¸ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± ÙØ¶Ø§Ù¾ÛŒÙ…Ø§Ù‡Ø§ Ùˆ Ù†Ø§Ù†ÙˆÙ…Ø§Ø´ÛŒÙ†â€ŒÙ‡Ø§\n",
    "- ğŸ›¡ï¸ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù†Ø¸Ø§Ù…ÛŒ Ø¯Ø± Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ ÙÙˆØ±ÛŒ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ:\n",
    "\n",
    "- **Brian2:** Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ SNN\n",
    "- **NEST:** Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø² Ø¹Ù„Ù…ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ†ÛŒ Ø²ÛŒØ³ØªÛŒ\n",
    "- **BindsNET:** ØªÙˆØ³Ø¹Ù‡ SNNÙ‡Ø§ Ø¨Ø§ Ù¾Ø§ÛŒØªÙˆÙ† Ùˆ PyTorch\n",
    "- **Nengo:** Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ…ÙˆØ±ÙÛŒÚ©\n",
    "- **Loihi:** ØªØ±Ø§Ø´Ù‡â€ŒÛŒ Ù†ÙˆØ±ÙˆÙ…ÙˆØ±ÙÛŒÚ© Ø´Ø±Ú©Øª Intel\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸš€ SNNÙ‡Ø§ Ù…Ø³ÛŒØ± Ø¢ÛŒÙ†Ø¯Ù‡â€ŒÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø±Ø§ Ø¨Ù‡â€ŒØ³ÙˆÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ù…â€ŒØ²Ù…Ø§Ù†ØŒ Ú©Ù…â€ŒÙ…ØµØ±Ù Ùˆ Ø´Ø¨ÛŒÙ‡ Ù…ØºØ² Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0421388",
   "metadata": {},
   "source": [
    "# Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ÛŒÚ© Ù†ÙˆØ±ÙˆÙ† LIF Ú©Ù‡ Ø¨Ø§ ØªØ­Ø±ÛŒÚ© Ù¾ÛŒÙˆØ³ØªÙ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2383b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ§Ø­Ø¯ Ø²Ù…Ø§Ù†\n",
    "defaultclock.dt = 0.1*ms\n",
    "\n",
    "# Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ù†ÙˆØ±ÙˆÙ† LIF (ØªØ¬Ù…Ø¹ Ùˆ Ù†Ø´Øª)\n",
    "eqs = '''\n",
    "dv/dt = (I - v) / (10*ms) : volt\n",
    "I : volt\n",
    "'''\n",
    "\n",
    "# ØªØ¹Ø±ÛŒÙ Ù†ÙˆØ±ÙˆÙ†\n",
    "G = NeuronGroup(1, eqs, threshold='v > -50*mV', reset='v = -70*mV', method='exact')\n",
    "G.v = -70*mV     # Ù…Ù‚Ø¯Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ ØºØ´Ø§Ø¡\n",
    "G.I = -55*mV     # Ø¬Ø±ÛŒØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø«Ø§Ø¨Øª\n",
    "\n",
    "# Ø¶Ø¨Ø· Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§\n",
    "mon = StateMonitor(G, 'v', record=0)\n",
    "spikemon = SpikeMonitor(G)\n",
    "\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ\n",
    "run(100*ms)\n",
    "\n",
    "# Ø±Ø³Ù… Ø®Ø±ÙˆØ¬ÛŒ\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(mon.t/ms, mon.v[0]/mV, label='Ù¾ØªØ§Ù†Ø³ÛŒÙ„ ØºØ´Ø§Ø¡ Ù†ÙˆØ±ÙˆÙ†')\n",
    "plt.axhline(y=-50, color='r', linestyle='--', label='Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ø³Ù¾Ø§ÛŒÚ©')\n",
    "plt.xlabel('Ø²Ù…Ø§Ù† (ms)')\n",
    "plt.ylabel('Ù¾ØªØ§Ù†Ø³ÛŒÙ„ (mV)')\n",
    "plt.title('Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ±ÙˆÙ† LIF Ø¯Ø± Brian2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ø§Ø³Ù¾Ø§ÛŒÚ©â€ŒÙ‡Ø§\n",
    "print(\"â±ï¸ Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§ÛŒÚ©:\", spikemon.t/ms, \"Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
