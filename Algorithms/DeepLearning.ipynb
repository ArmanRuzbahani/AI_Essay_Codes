{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77945b74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ุณูุฑ ุงุฏฺฏุฑ: ฺฉุดู ูุนูุงุฑโูุง ุงุฏฺฏุฑ ุนูู ๐๐ง\n",
    "\n",
    "**ุจุงุฏ ุจุง ูู ุณู ูุนูุงุฑ ุงุณุงุณ ุงุฏฺฏุฑ ุนูู ุฑุง ฺฉุดู ฺฉูู:**\n",
    "\n",
    "## 1. <span style=\"color: #4d90fe;\">CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู)</span> ๐ผ๏ธ\n",
    "- **ูุบุฒ ุจูุง ูุงุดู** ๐๏ธ\n",
    "- ุงุฒ ููุชุฑูุง ฺฉุงููููุดู ุจุฑุง ุชุดุฎุต ุงูฺฏููุง ูุญู ุงุณุชูุงุฏู ูโฺฉูุฏ\n",
    "- ุงุฏูโุขู ุจุฑุง ูพุฑุฏุงุฒุด ุชุตุงูุฑ ู ูุฏู\n",
    "- ูุซุงู: ุชุดุฎุต ุงุดุง ุฏุฑ ุนฺฉุณโูุง\n",
    "\n",
    "## 2. <span style=\"color: #34a853;\">RNN (ุดุจฺฉู ุนุตุจ ุจุงุฒฺฏุดุช)</span> ๐\n",
    "- **ุญุงูุธู ฺฉูุชุงูโูุฏุช ุจุฑุง ุฏุงุฏูโูุง ูุชูุงู** ๐\n",
    "- ูโุชูุงูุฏ ุงุทูุงุนุงุช ุฑุง ุงุฒ ูุฑุงุญู ูุจู ุจู ุฎุงุทุฑ ุจุณูพุงุฑุฏ\n",
    "- ููุงุณุจ ุจุฑุง ูุชูุ ฺฏูุชุงุฑ ู ุฏุงุฏูโูุง ุฒูุงู\n",
    "- ูุซุงู: ูพุดโุจู ฺฉููู ุจุนุฏ ุฏุฑ ุฌููู\n",
    "\n",
    "## 3. <span style=\"color: #ea4335;\">Transformer</span> โก\n",
    "- **ุงูููุงุจ ุฏุฑ ูพุฑุฏุงุฒุด ุฒุจุงู** ๐ฌ\n",
    "- ุงุฒ ูฺฉุงูุฒู ุชูุฌู (Attention) ุงุณุชูุงุฏู ูโฺฉูุฏ\n",
    "- ูโุชูุงูุฏ ุฑูุงุจุท ุจููุฏูุฏุช ุฑุง ุฏุฑ ุฏุงุฏูโูุง ุชุดุฎุต ุฏูุฏ\n",
    "- ูุซุงู: ุชุฑุฌูู ูุงุดูุ ูุฏูโูุง ฺฏูุชฺฏู\n",
    "\n",
    "๐ **ูฺฉุชู ุทูุง:**  \n",
    "ุงู ูุนูุงุฑโูุง ุงุบูุจ ุจุง ูู ุชุฑฺฉุจ ูโุดููุฏ ุชุง ุณุณุชูโูุง ููุดููุฏ ูุฏุฑุชููุฏ ุจุณุงุฒูุฏ!\n",
    "\n",
    "๐ฏ **ูุฏู ูุง ุฏุฑ ุงู ูุณุฑ:**  \n",
    "ุงุฏฺฏุฑ ุงุตูู ูุฑ ูุนูุงุฑ + ุฏุฑฺฉ ฺฉุงุฑุจุฑุฏูุง ุนูู + ูพุงุฏูโุณุงุฒ ุณุงุฏู\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664615",
   "metadata": {},
   "source": [
    "# ุงุฏฺฏุฑ ุนูู: ููุชูุฑ ูุญุฑฺฉ ููุด ูุตููุน ูุฏุฑู ๐๐ง\n",
    "\n",
    "**ุงุฏฺฏุฑ ุนูู**ุ ูพุดุฑูุชูโุชุฑู ุดุงุฎู ุงุฏฺฏุฑ ูุงุดู ุงุณุช ฺฉู ุจุง ุงููุงู ุงุฒ ุณุงุฎุชุงุฑ ูุบุฒ ุงูุณุงูุ ุงูููุงุจ ุฏุฑ ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ุงุฌุงุฏ ฺฉุฑุฏู ุงุณุช. ุงู ููุงูุฑ ุจุง ูุนูุงุฑโูุง ููุดููุฏุงูู ุฎูุฏ ูุงุฏุฑ ุจู ุงุฏฺฏุฑ ุณูุณููโูุฑุงุชุจ ูฺฺฏโูุง ุงุฒ ุฏุงุฏูโูุง ุฎุงู ุงุณุช:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa30d",
   "metadata": {},
   "source": [
    "# CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ฺฉุชุงุจุฎุงููโูุง ููุฑุฏ ูุงุฒ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ูุฑุญูู 1: ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ูุชู\n",
    "# ูุฏู: ุชุจุฏู ูุชู ุจู ุจุฑุฏุงุฑูุง ุนุฏุฏ ู ุขูุงุฏูโุณุงุฒ ุจุฑุง ูุฑูุฏ ุดุจฺฉู\n",
    "def prepare_text_data(texts, labels, max_words=10000, max_len=100):\n",
    "    # ุชูฺฉูโุณุงุฒ: ุชุจุฏู ฺฉููุงุช ุจู ุชูฺฉูโูุง ุนุฏุฏ\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # ุชุจุฏู ูุชู ุจู ุฏูุจุงููโูุง ุนุฏุฏ\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # ูพุฏ ฺฉุฑุฏู ุฏูุจุงููโูุง ุจุฑุง ฺฉุณุงูโุณุงุฒ ุทูู\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    # ุชุจุฏู ุจุฑฺุณุจโูุง ุจู ุขุฑุงู\n",
    "    labels = np.array(labels)\n",
    "    return data, labels, tokenizer\n",
    "\n",
    "# ูุฑุญูู 2: ุชุนุฑู ูุนูุงุฑ ุดุจฺฉู ฺฉุงููููุดู\n",
    "# ูุฏู: ุงุฌุงุฏ ฺฉ ูุฏู CNN ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุชู ู ุทุจููโุจูุฏ ุงุญุณุงุณุงุช\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = models.Sequential([\n",
    "        # ูุงู ุชุนุจู: ุชุจุฏู ุชูฺฉูโูุง ุจู ุจุฑุฏุงุฑูุง ูุชุฑุงฺฉู\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        # ูุงู ฺฉุงููููุดู ุงูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูุญู ุงุฒ ูุชู\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        # ูุงู ูพูููฺฏ: ฺฉุงูุด ุงุจุนุงุฏ ู ุญูุธ ูฺฺฏโูุง ููู\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # ูุงู ฺฉุงููููุดู ุฏูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ูพฺุฏูโุชุฑ\n",
    "        layers.Conv1D(128, 5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # ูุณุทุญโุณุงุฒ: ุขูุงุฏูโุณุงุฒ ุจุฑุง ูุงูโูุง ูุชุฑุงฺฉู\n",
    "        layers.Flatten(),\n",
    "        # ูุงู ฺฉุงููุงู ูุชุตู: ุชุฑฺฉุจ ูฺฺฏโูุง\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        # ูุงู ุฎุฑูุฌ: ุทุจููโุจูุฏ ุฏูุฏู (ูุซุจุช/ููู)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ูุฑุญูู 3: ุขููุฒุด ู ุงุฑุฒุงุจ ูุฏู\n",
    "# ูุฏู: ุขููุฒุด ูุฏู ู ุงุฑุฒุงุจ ุนููฺฉุฑุฏ ุขู ุฏุฑ ุชุญูู ุงุญุณุงุณุงุช\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # ฺฉุงููพุงู ูุฏู ุจุง ุจูููโุณุงุฒ ู ุชุงุจุน ูุฒูู\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # ุขููุฒุด ูุฏู\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "    # ุงุฑุฒุงุจ ูุฏู\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f'ุฏูุช ุชุณุช: {test_acc:.4f}')\n",
    "    return test_acc, test_loss\n",
    "\n",
    "# ุงูฺฏูุฑุชู ุงุตู\n",
    "def main():\n",
    "    # ููููู ุฏุงุฏูโูุง (ุฌุงฺฏุฒู ุจุง ุฏุชุงุณุช ูุงูุน ุดูุง)\n",
    "    # ูุฑุถ\n",
    "    texts = [\"ููู ุนุงู ุจูุฏ ู ุฎู ูุฐุช ุจุฑุฏู\", \"ุงู ุจุฏุชุฑู ุชุฌุฑุจู ูู ุจูุฏ\", ...]  # ุฏุงุฏูโูุง ูุชู\n",
    "    labels = [1, 0, ...]  # ุจุฑฺุณุจโูุง\n",
    "    max_words = 10000  # ุญุฏุงฺฉุซุฑ ุชุนุฏุงุฏ ฺฉููุงุช ุฏุฑ ูุงฺฺฏุงู\n",
    "    max_len = 100  # ุญุฏุงฺฉุซุฑ ุทูู ุฏูุจุงูู\n",
    "\n",
    "    # ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง\n",
    "    x_data, y_data, tokenizer = prepare_text_data(texts, labels, max_words, max_len)\n",
    "    # ุชูุณู ุฏุงุฏูโูุง ุจู ุขููุฒุด ู ุชุณุช (ูุฑุถ: 80% ุขููุฒุดุ 20% ุชุณุช)\n",
    "    train_size = int(0.8 * len(x_data))\n",
    "    x_train, y_train = x_data[:train_size], y_data[:train_size]\n",
    "    x_test, y_test = x_data[train_size:], y_data[train_size:]\n",
    "\n",
    "    # ุชุนุฑู ูุฏู\n",
    "    model = build_cnn_model(max_words, max_len)\n",
    "    # ููุงุด ุฎูุงุตู ูุฏู\n",
    "    model.summary()\n",
    "    # ุขููุฒุด ู ุงุฑุฒุงุจ\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# ุงุฌุฑุง ุจุฑูุงูู\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
